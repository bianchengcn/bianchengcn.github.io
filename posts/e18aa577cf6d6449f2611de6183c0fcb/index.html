<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>图机器学习（Graph Machine Learning）- 第三章 无监督图学习3 （Unsupervised Graph Learning）- 图神经网络 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="图机器学习（Graph Machine Learning）- 第三章 无监督图学习3 （Unsupervised Graph Learning）- 图神经网络" />
<meta property="og:description" content="第三章 无监督图学习3 - 图神经网络 Graph neural networks 文章目录 第三章 无监督图学习3 - 图神经网络 Graph neural networks前言3.4 图神经网络 Graph neural networks3.4.1 GNNs的其他形式3.4.2 谱卷积 Spectral graph convolution 3.4.3 空间图卷积 Spectral graph convolution3.4.4 图卷积实战 总结 前言 Graph neural network，GNN 是处理图形结构数据的深度学习方法。这类方法也被称为几何深度学习 geometric deep learning ，并在各种应用中引起了广泛的兴趣，包括社会网络分析和计算机图形学。
根据第二章图机器学习中定义的分类，编码器部分以图结构和节点特征作为输入。这些算法可以在有或没有监督的情况下进行训练。在本章中，我们将专注于无监督训练，而监督设置将在第四章，监督图学习中探讨。
3.4 图神经网络 Graph neural networks 如果您熟悉卷积神经网络(CNN)的概念，您可能已经知道它们能够在处理常规欧几里德空间(如文本(一维)、图像(二维)和视频(三维))时获得令人印象深刻的结果。经典的CNN由层序列组成，每一层提取多尺度的局部空间特征。这些特征被更深层的层利用来构造更复杂和高度表达的表示。
近年来，人们发现诸如多层和局部性等概念对于处理图结构数据也很有用。然而，图是在一个非欧几里德空间上定义的，要为图找到一个CNN的泛化并不简单，如图3.20所示:
GNN的最初的构想是由Scarselli等人在2009年提出的。它依赖于一个事实，即每个节点都可以用它的特征和它的邻域来描述。来自邻域(表示图域中的局部性概念)的信息可以聚合并用于计算更复杂和高级的特征。让我们更详细地了解它是如何实现的。
一开始，每个节点 v i v_i vi​都与一个状态相关联。让我们从一个随机嵌入 h i t ℎ_i^t hit​开始(为了简单起见，忽略节点属性)。在算法的每次迭代中，节点使用一个简单的神经网络层积累来自其邻居的输入:
h i t = ∑ v j ∈ N ( v i ) σ ( W h j t − 1 &#43; b ) h_i^t = \sum_{v_j \in N(v_i)} \sigma(Wh_j^{t-1}&#43;b) hit​=vj​∈N(vi​)∑​σ(Whjt−1​&#43;b)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/e18aa577cf6d6449f2611de6183c0fcb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-14T17:44:49+08:00" />
<meta property="article:modified_time" content="2022-04-14T17:44:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">图机器学习（Graph Machine Learning）- 第三章 无监督图学习3 （Unsupervised Graph Learning）- 图神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_3___Graph_neural_networks_0"></a>第三章 无监督图学习3 - 图神经网络 Graph neural networks</h2> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_3___Graph_neural_networks_0" rel="nofollow">第三章 无监督图学习3 - 图神经网络 Graph neural networks</a></li><li><a href="#_6" rel="nofollow">前言</a></li><li><ul><li><a href="#34__Graph_neural_networks_11" rel="nofollow">3.4 图神经网络 Graph neural networks</a></li><li><ul><li><a href="#341_GNNs_28" rel="nofollow">3.4.1 GNNs的其他形式</a></li><li><a href="#342__Spectral_graph_convolution_46" rel="nofollow">3.4.2 谱卷积 Spectral graph convolution</a></li></ul> 
   </li><li><a href="#343___Spectral_graph_convolution_169" rel="nofollow">3.4.3 空间图卷积 Spectral graph convolution</a></li><li><a href="#344__185" rel="nofollow">3.4.4 图卷积实战</a></li></ul> 
  </li><li><a href="#_317" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_6"></a>前言</h2> 
<p><strong>Graph neural network，GNN</strong> 是处理图形结构数据的深度学习方法。这类方法也被称为<strong>几何深度学习 geometric deep learning</strong> ，并在各种应用中引起了广泛的兴趣，包括社会网络分析和计算机图形学。</p> 
<p>根据第二章图机器学习中定义的分类，编码器部分以图结构和节点特征作为输入。这些算法可以在有或没有监督的情况下进行训练。在本章中，我们将专注于无监督训练，而监督设置将在第四章，监督图学习中探讨。</p> 
<h3><a id="34__Graph_neural_networks_11"></a>3.4 图神经网络 Graph neural networks</h3> 
<p>如果您熟悉卷积神经网络(CNN)的概念，您可能已经知道它们能够在处理常规欧几里德空间(如文本(一维)、图像(二维)和视频(三维))时获得令人印象深刻的结果。经典的CNN由层序列组成，每一层提取多尺度的局部空间特征。这些特征被更深层的层利用来构造更复杂和高度表达的表示。</p> 
<p>近年来，人们发现诸如多层和局部性等概念对于处理图结构数据也很有用。然而，图是在一个<em>非欧几里德空间</em>上定义的，要为图找到一个CNN的泛化并不简单，如图3.20所示:</p> 
<p><img src="https://images2.imgbox.com/0c/48/Kp2UEpxm_o.png" alt="在这里插入图片描述"></p> 
<p>GNN的最初的构想是由Scarselli等人在2009年提出的。它依赖于一个事实，即每个节点都可以用它的特征和它的邻域来描述。来自邻域(表示图域中的局部性概念)的信息可以聚合并用于计算更复杂和高级的特征。让我们更详细地了解它是如何实现的。</p> 
<p>一开始，每个节点<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          v 
         
        
          i 
         
        
       
      
        v_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>都与一个状态相关联。让我们从一个随机嵌入<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          h 
         
        
          i 
         
        
          t 
         
        
       
      
        ℎ_i^t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.05222em; vertical-align: -0.258664em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.793556em;"><span class="" style="top: -2.44134em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.258664em;"><span class=""></span></span></span></span></span></span></span></span></span></span>开始(为了简单起见，忽略节点属性)。在算法的每次迭代中，节点使用一个简单的神经网络层积累来自其邻居的输入:</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           h 
          
         
           i 
          
         
           t 
          
         
        
          = 
         
         
         
           ∑ 
          
          
           
           
             v 
            
           
             j 
            
           
          
            ∈ 
           
          
            N 
           
          
            ( 
           
           
           
             v 
            
           
             i 
            
           
          
            ) 
           
          
         
        
          σ 
         
        
          ( 
         
        
          W 
         
         
         
           h 
          
         
           j 
          
          
          
            t 
           
          
            − 
           
          
            1 
           
          
         
        
          + 
         
        
          b 
         
        
          ) 
         
        
       
         h_i^t = \sum_{v_j \in N(v_i)} \sigma(Wh_j^{t-1}+b) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.09056em; vertical-align: -0.247em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.843556em;"><span class="" style="top: -2.453em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.58833em; vertical-align: -1.53832em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.05001em;"><span class="" style="top: -1.80899em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328086em;"><span class="" style="top: -2.357em; margin-left: -0.03588em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.281886em;"><span class=""></span></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328086em;"><span class="" style="top: -2.357em; margin-left: -0.03588em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.53832em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.864108em;"><span class="" style="top: -2.43301em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.403103em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         W 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           d 
          
         
           × 
          
         
           d 
          
         
        
       
      
        W \in \mathbb{R}^{d \times d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         b 
        
       
         ∈ 
        
        
        
          R 
         
        
          d 
         
        
       
      
        b \in \mathbb{R}^{d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.73354em; vertical-align: -0.0391em;"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>是可训练参数(其中d为嵌入的维数)，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span>是非线性函数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         t 
        
       
      
        t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"></span><span class="mord mathdefault">t</span></span></span></span></span>表示算法的第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         t 
        
       
      
        t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"></span><span class="mord mathdefault">t</span></span></span></span></span>次迭代。这个方程可以递归地应用，直到达到一个特定的目标。请注意，在每次迭代中，之前的状态(在之前的迭代中计算的状态)被用来计算新的状态已经在<em>循环神经网络recurrent neural networks</em> 中得到了应用。</p> 
<h4><a id="341_GNNs_28"></a>3.4.1 GNNs的其他形式</h4> 
<p>从GNN最初的构想开始，近年来已经进行了一些尝试，以重新解决从图数据中学习的问题。特别是，前面描述的GNN的变体已经被提出，目的是提高其表示学习能力。其中一些专门设计用于处理特定类型的图(有向、无向、加权、未加权、静态、动态等等)。</p> 
<p>此外，还对传播步提出了一些改进(卷积、门机制、注意机制和跳跃连接等)，目的是在不同级别上改善表示。此外，还提出了不同的训练方法来改善学习。</p> 
<p>在处理无监督表示学习时，最常见的方法之一是使用编码器来嵌入图(编码器被表示为GNN变体之一)，然后使用一个简单的解码器重构邻接矩阵。<br> 损失函数通常表示为原始邻接矩阵与重构邻接矩阵之间的相似度。形式上，它可以定义为:</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          Z 
         
        
          = 
         
        
          G 
         
        
          N 
         
        
          N 
         
        
          ( 
         
        
          X 
         
        
          , 
         
        
          A 
         
        
          ) 
         
        
          , 
         
        
          s.t. 
         
         
         
           A 
          
         
           ^ 
          
         
        
          = 
         
        
          Z 
         
         
         
           Z 
          
         
           T 
          
         
        
       
         Z = GNN(X, A), \text{s.t.} \hat{A} = ZZ^T 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">Z</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.19677em; vertical-align: -0.25em;"></span><span class="mord mathdefault">G</span><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">A</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord text"><span class="mord">s.t.</span></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.94677em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">A</span></span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.11111em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.891331em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">Z</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07153em;">Z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         A 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           N 
          
         
           × 
          
         
           N 
          
         
        
       
      
        A \in \mathbb{R}^{N \times N} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.841331em; vertical-align: 0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span></span></span></span></span></span></span></span></span></span>为邻接矩阵表示，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           N 
          
         
           × 
          
         
           d 
          
         
        
       
      
        X \in \mathbb{R}^{N \times d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>为节点属性矩阵。</p> 
<p>这种方法的另一种常见变体，特别是在处理图分类/表示学习时，是针对<em>目标距离</em> 进行训练。其思想是同时嵌入两对图，获得一个组合表示。然后对模型进行训练，使其表示与距离匹配。在使用节点相似度函数处理节点分类/表示学习时，也可以采用类似的策略。</p> 
<p>基于<strong>图卷积神经网络(Graph Convolutional Neural Network, GCN)</strong> 的编码器是用于无监督学习的最广泛的GNN变体之一。GCN是受CNN背后许多基本思想启发的GNN模型。滤波器参数通常在图中的所有位置上共享，几个层被连接起来形成一个深层网络。</p> 
<p>图数据的卷积操作本质上有两种类型，即<strong>谱spectral</strong> 方法和<strong>非谱(空间)non-spectral (spatial)</strong> 方法。顾名思义，第一个定义了谱域的卷积(即，将图分解为简单元素的组合)。空间卷积将卷积定义为聚集来自邻居的特征信息。</p> 
<h4><a id="342__Spectral_graph_convolution_46"></a>3.4.2 谱卷积 Spectral graph convolution</h4> 
<p>谱方法与谱图理论相关，研究与图相关的特征多项式、特征值和矩阵的特征向量有关的图的特征。卷积运算定义为信号(节点特征)与核的乘积。更详细地说，它定义在通过确定图拉普拉斯矩阵的特征分解来实现的傅里叶域上(把图拉普拉斯矩阵想象成一个以特殊方式归一化的邻接矩阵)。</p> 
<p>虽然这种谱卷积的定义有很强的数学基础，但该操作的计算代价昂贵。基于这个原因，已经的一些工作试图以一种有效的方法来近似它。例如，Defferrard等人的ChebNet是关于谱图卷积的第一个开创性工作之一。这里，通过使用K阶切比雪夫多项式(一种用于有效逼近函数的特殊多项式)的概念来逼近操作。</p> 
<p>这里，K是一个非常有用的参数，因为它决定了滤波器的位置。直观地说，对于K=1，只有节点特征被输入到网络中。当K=2时，将two-hop邻节点(邻节点的邻节点)的平均作为输入，以此类推。</p> 
<p>设<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           N 
          
         
           × 
          
         
           d 
          
         
        
       
      
        X \in \mathbb{R}^{N \times d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>为节点特征矩阵。在经典的神经网络处理中，该信号将由以下形式的层组成:</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           H 
          
         
           l 
          
         
        
          = 
         
        
          σ 
         
        
          ( 
         
        
          X 
         
        
          W 
         
        
          ) 
         
        
       
         H^l = \sigma (XW) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.899108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mclose">)</span></span></span></span></span></span></p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         W 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           N 
          
         
           × 
          
         
           N 
          
         
        
       
      
        W \in \mathbb{R}^{N \times N} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.841331em; vertical-align: 0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span></span></span></span></span></span></span></span></span></span>是层权重值，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span> 表示非线性激活函数。这种操作的缺点是，它独立处理每个节点的信号，而不考虑节点之间的连接。为了克服这个限制，可以做一个简单(但有效)的修改，如下所示:</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           H 
          
         
           l 
          
         
        
          = 
         
        
          σ 
         
        
          ( 
         
        
          A 
         
        
          X 
         
        
          W 
         
        
          ) 
         
        
       
         H^l = \sigma (AXW) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.899108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mclose">)</span></span></span></span></span></span></p> 
<p>通过引入邻接矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         A 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           N 
          
         
           × 
          
         
           N 
          
         
        
       
      
        A \in \mathbb{R}^{N \times N} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72243em; vertical-align: -0.0391em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.841331em; vertical-align: 0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span></span></span></span></span></span></span></span></span></span>, 每个节点与其对应的邻节点之间增加了一个新的线性组合。这样，信息只依赖于邻域，但参数将应用于所有节点。</p> 
<p>值得注意的是，该操作可以按顺序重复多次，从而创建一个深度网络。每一层中节点描述符<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
      
        X 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span></span></span></span></span>将被前一层的输出<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          H 
         
         
         
           l 
          
         
           − 
          
         
           1 
          
         
        
       
      
        H^{l-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>替换。</p> 
<p>然而，前面提出的公式有一些局限性，不能直接应用。第一个限制是，通过乘以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         A 
        
       
      
        A 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">A</span></span></span></span></span>，我们考虑的是节点的所有邻居，而不是节点本身。这个问题可以很容易地通过在图中添加自循环来克服，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          A 
         
        
          ^ 
         
        
       
         = 
        
       
         A 
        
       
         + 
        
       
         I 
        
       
      
        \hat{A} = A+I 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.94677em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.94677em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">A</span></span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.11111em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.07847em;">I</span></span></span></span></span>。</p> 
<p>第二个限制与邻接矩阵本身有关。由于它通常是不归一化的，我们会观察到high-degree节点的特征表示值大，而low-degree节点的特征表示值小。这将导致在训练过程中会出现一些问题，因为优化算法往往对特征的大小敏感。解决方案是对A进行归一化。</p> 
<p>例如，在Kipf和Welling, 2017(著名的GCN模型之一)中，通过将<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         A 
        
       
      
        A 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault">A</span></span></span></span></span>乘以<em>对角线节点度矩阵diagonal node degree matrix <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          D 
         
        
       
         D 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span></span></span></span></span></em> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          D 
         
         
         
           − 
          
         
           1 
          
         
        
       
         A 
        
       
      
        D^{-1}A 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">A</span></span></span></span></span>来进行归一化，使得所有行的和为1。更具体地说，使用对称标准化<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          D 
         
         
         
           − 
          
         
           1 
          
         
           / 
          
         
           2 
          
         
        
       
         A 
        
        
        
          D 
         
         
         
           − 
          
         
           1 
          
         
           / 
          
         
           2 
          
         
        
       
      
        D^{-1/2}AD^{-1/2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span><span class="mord mtight">/</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">A</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span><span class="mord mtight">/</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>，使得提出的传播规则为:</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           H 
          
         
           l 
          
         
        
          = 
         
        
          σ 
         
        
          ( 
         
         
          
          
            D 
           
          
            ^ 
           
          
          
          
            − 
           
          
            1 
           
          
            / 
           
          
            2 
           
          
         
        
          A 
         
         
          
          
            D 
           
          
            ^ 
           
          
          
          
            − 
           
          
            1 
           
          
            / 
           
          
            2 
           
          
         
        
          X 
         
        
          W 
         
        
          ) 
         
        
       
         H^l = \sigma (\hat{D}^{-1/2}A\hat{D}^{-1/2}XW) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.899108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.19677em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.94677em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span></span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span><span class="mord mtight">/</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">A</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.94677em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span></span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span><span class="mord mtight">/</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="mclose">)</span></span></span></span></span></span><br> 其中 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          D 
         
        
          ^ 
         
        
       
      
        \hat{D} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.94677em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.94677em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">D</span></span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">^</span></span></span></span></span></span></span></span></span></span>是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          A 
         
        
          ^ 
         
        
       
      
        \hat{A} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.94677em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.94677em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">A</span></span></span><span class="" style="top: -3.25233em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.11111em;">^</span></span></span></span></span></span></span></span></span></span>的对角度矩阵。</p> 
<p>在下面的例子中，我们将创建一个Kipf和Welling中定义的GCN，我们将应用这个传播规则来嵌入一个著名的网络:Zachary’s karate club graph。</p> 
<pre><code class="prism language-c"># 利用networkx加载哑铃图
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">from</span> <span class="token expression">networkx import karate_club_graph<span class="token punctuation">,</span> to_numpy_matrix</span></span>
import numpy as np
import networkx as nx
from scipy<span class="token punctuation">.</span>linalg import sqrtm
import matplotlib<span class="token punctuation">.</span>pyplot as plt

G <span class="token operator">=</span> nx<span class="token punctuation">.</span><span class="token function">barbell_graph</span><span class="token punctuation">(</span>m1<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> m2<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>

#为了实现GraphConv传播规则，我们需要一个邻接矩阵A来表示G<span class="token punctuation">.</span>由于该网络不具有节点特征，我们将使用单位矩阵作为节点描述符
order <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">arange</span><span class="token punctuation">(</span>G<span class="token punctuation">.</span><span class="token function">number_of_nodes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
A <span class="token operator">=</span> nx<span class="token punctuation">.</span><span class="token function">to_numpy_matrix</span><span class="token punctuation">(</span>G<span class="token punctuation">,</span> nodelist<span class="token operator">=</span>order<span class="token punctuation">)</span>
I <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">eye</span><span class="token punctuation">(</span>G<span class="token punctuation">.</span><span class="token function">number_of_nodes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-c">np<span class="token punctuation">.</span>random<span class="token punctuation">.</span><span class="token function">seed</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span>

A_hat <span class="token operator">=</span> A <span class="token operator">+</span> np<span class="token punctuation">.</span><span class="token function">eye</span><span class="token punctuation">(</span>G<span class="token punctuation">.</span><span class="token function">number_of_nodes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> # add self<span class="token operator">-</span>connections

D_hat <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token function">sum</span><span class="token punctuation">(</span>A_hat<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> # diagonal node degree matrix<span class="token operator">:</span>
D_hat <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token function">diag</span><span class="token punctuation">(</span>D_hat<span class="token punctuation">)</span><span class="token punctuation">)</span>
D_hat <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span><span class="token function">inv</span><span class="token punctuation">(</span><span class="token function">sqrtm</span><span class="token punctuation">(</span>D_hat<span class="token punctuation">)</span><span class="token punctuation">)</span>

A_hat <span class="token operator">=</span> D_hat @ A_hat @ D_hat

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">Layer weights W will be initialized using Glorot uniform initialization</span></span>
<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">other</span> <span class="token expression">initialization methods can be also used， e<span class="token punctuation">.</span>g<span class="token punctuation">.</span><span class="token punctuation">,</span> Gaussian or uniform</span></span>
def <span class="token function">glorot_init</span><span class="token punctuation">(</span>nin<span class="token punctuation">,</span> nout<span class="token punctuation">)</span><span class="token operator">:</span>
  sd <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">sqrt</span><span class="token punctuation">(</span><span class="token number">6.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>nin <span class="token operator">+</span> nout<span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span><span class="token function">uniform</span><span class="token punctuation">(</span><span class="token operator">-</span>sd<span class="token punctuation">,</span> sd<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>nin<span class="token punctuation">,</span> nout<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">GCN  propagation rule</span></span>
class <span class="token function">GCNLayer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span>
  def <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_inputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">)</span><span class="token operator">:</span>
      self<span class="token punctuation">.</span>n_inputs <span class="token operator">=</span> n_inputs
      self<span class="token punctuation">.</span>n_outputs <span class="token operator">=</span> n_outputs
      self<span class="token punctuation">.</span>W <span class="token operator">=</span> <span class="token function">glorot_init</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_outputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_inputs<span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>activation <span class="token operator">=</span> np<span class="token punctuation">.</span>tanh
      
  def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token operator">:</span>
      self<span class="token punctuation">.</span>_X <span class="token operator">=</span> <span class="token punctuation">(</span>A @ X<span class="token punctuation">)</span><span class="token punctuation">.</span>T # <span class="token punctuation">(</span>N<span class="token punctuation">,</span>N<span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span>n_outputs<span class="token punctuation">)</span> <span class="token operator">==</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>n_outputs<span class="token punctuation">,</span>N<span class="token punctuation">)</span>
      H <span class="token operator">=</span> self<span class="token punctuation">.</span>W @ self<span class="token punctuation">.</span>_X # <span class="token punctuation">(</span>N<span class="token punctuation">,</span> D<span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>D<span class="token punctuation">,</span> n_outputs<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> n_outputs<span class="token punctuation">)</span>
      H <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token function">activation</span><span class="token punctuation">(</span>H<span class="token punctuation">)</span>
      <span class="token keyword">return</span> H<span class="token punctuation">.</span>T # <span class="token punctuation">(</span>n_outputs<span class="token punctuation">,</span> N<span class="token punctuation">)</span>


<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">create</span> <span class="token expression">network </span></span>
gcn1 <span class="token operator">=</span> <span class="token function">GCNLayer</span><span class="token punctuation">(</span>G<span class="token punctuation">.</span><span class="token function">number_of_nodes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
gcn2 <span class="token operator">=</span> <span class="token function">GCNLayer</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
gcn3 <span class="token operator">=</span> <span class="token function">GCNLayer</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> # 输出层输出数目为<span class="token number">2</span>，表示输出的嵌入为<span class="token number">2</span>维嵌入，方便可视化

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">compute</span> <span class="token expression">the forward pass</span></span>
H1 <span class="token operator">=</span> gcn1<span class="token punctuation">.</span><span class="token function">forward</span><span class="token punctuation">(</span>A_hat<span class="token punctuation">,</span> I<span class="token punctuation">)</span>
H2 <span class="token operator">=</span> gcn2<span class="token punctuation">.</span><span class="token function">forward</span><span class="token punctuation">(</span>A_hat<span class="token punctuation">,</span> H1<span class="token punctuation">)</span>
H3 <span class="token operator">=</span> gcn3<span class="token punctuation">.</span><span class="token function">forward</span><span class="token punctuation">(</span>A_hat<span class="token punctuation">,</span> H2<span class="token punctuation">)</span> #H3包含使用GCN传播规则计算的嵌入。

embeddings <span class="token operator">=</span> H3
</code></pre> 
<pre><code class="prism language-c">def <span class="token function">draw_graph</span><span class="token punctuation">(</span>G<span class="token punctuation">,</span> filename<span class="token operator">=</span>None<span class="token punctuation">,</span> node_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token operator">:</span>
  pos_nodes <span class="token operator">=</span> nx<span class="token punctuation">.</span><span class="token function">spring_layout</span><span class="token punctuation">(</span>G<span class="token punctuation">)</span>
  nx<span class="token punctuation">.</span><span class="token function">draw</span><span class="token punctuation">(</span>G<span class="token punctuation">,</span> pos_nodes<span class="token punctuation">,</span> with_labels<span class="token operator">=</span>False<span class="token punctuation">,</span> node_size<span class="token operator">=</span>node_size<span class="token punctuation">,</span> edge_color<span class="token operator">=</span><span class="token string">'gray'</span><span class="token punctuation">)</span>
  
  pos_attrs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
  <span class="token keyword">for</span> node<span class="token punctuation">,</span> coords in pos_nodes<span class="token punctuation">.</span><span class="token function">items</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span>
    pos_attrs<span class="token punctuation">[</span>node<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>coords<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> coords<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">0.08</span><span class="token punctuation">)</span>

  plt<span class="token punctuation">.</span><span class="token function">axis</span><span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>
  axis <span class="token operator">=</span> plt<span class="token punctuation">.</span><span class="token function">gca</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  axis<span class="token punctuation">.</span><span class="token function">set_xlim</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.2</span><span class="token operator">*</span>x <span class="token keyword">for</span> x in axis<span class="token punctuation">.</span><span class="token function">get_xlim</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  axis<span class="token punctuation">.</span><span class="token function">set_ylim</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.2</span><span class="token operator">*</span>y <span class="token keyword">for</span> y in axis<span class="token punctuation">.</span><span class="token function">get_ylim</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

embeddings <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
<span class="token function">draw_graph</span><span class="token punctuation">(</span>G<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/f5/8a/WHh6kBsT_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-c">plt<span class="token punctuation">.</span><span class="token function">scatter</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embeddings<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">savefig</span><span class="token punctuation">(</span><span class="token string">'embedding_gcn.png'</span><span class="token punctuation">,</span>dpi<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/b3/81/dRzdfsWR_o.png" alt="在这里插入图片描述"><br> GCN嵌入的结果可以很明显的观察到两个相当分离的部分。考虑到我们还没有训练这个网络，这是一个很好的结果!</p> 
<p>谱图卷积方法在许多领域都取得了显著的成果。然而，它们也存在一些缺点。例如，考虑一个具有数十亿个节点的非常大的图，谱方法要求同时处理图，从计算的角度来看这是不现实的。</p> 
<p>此外，谱卷积通常假设一个固定的图，导致在新的不同的图上泛化能力较差。为了克服这些问题，空间图卷积是另一种有趣的尝试。</p> 
<h3><a id="343___Spectral_graph_convolution_169"></a>3.4.3 空间图卷积 Spectral graph convolution</h3> 
<p>空间图卷积网络通过聚合来自空间近邻的信息直接对图执行操作。空间卷积有很多优点:权值可以很容易地在图的不同位置共享，从而在不同的图上具有很好的泛化能力。此外，计算可以通过考虑节点的子集而不是整个图来完成，这可能会提高计算效率。</p> 
<p>GraphSAGE是实现空间卷积的算法之一。其主要特点之一是能够在各种类型的网络上扩展。</p> 
<p>GraphSAGE由三个步骤组成:</p> 
<ol><li> <p><strong>邻域采样 Neighborhood sampling</strong> :对于图中的每个节点，第一步是找到它的k邻域，其中k是由用户定义的，用来决定需要考虑多少hops(邻域的邻域)。</p> </li><li> <p><strong>聚合 Aggregation</strong> :对于每个节点，描述各自邻域的节点特征。可以执行各种类型的聚合，包括平均聚合、池化聚合(例如，根据特定的标准采用最佳特性)，或者更复杂的操作，例如使用循环单位(如LSTM)。</p> </li><li> <p><strong>预测 Prediction</strong>:每个节点都配备了一个简单的神经网络，该网络学习如何基于来自邻域的聚合特征进行预测。</p> </li></ol> 
<p>GraphSAGE经常用于监督学习。然而，通过采用诸如使用相似函数作为目标距离的策略，它也可以有效地进行学习嵌入，而无需明确地监督任务。</p> 
<h3><a id="344__185"></a>3.4.4 图卷积实战</h3> 
<p>在实际应用中，GNN已经在许多机器学习和深度学习框架中实现，包括TensorFlow、Keras和PyTorch。在下一个例子中，我们将使用用于图形机器学习的Python库<code>StellarGraph</code>。</p> 
<p>在下面的例子中，我们将学习在没有目标变量的情况下，以无监督的方式嵌入向量。该方法受到Bai et al. 2019的启发，基于同时嵌入成对图。该嵌入用来匹配图之间的ground-truth距离:</p> 
<ol><li>安装StellarGraph, 加载需要的Python模块。</li></ol> 
<pre><code class="prism language-c"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">install</span> <span class="token expression">StellarGraph</span></span>
<span class="token operator">!</span>pip install <span class="token operator">-</span>q stellargraph<span class="token punctuation">[</span>demos<span class="token punctuation">]</span><span class="token operator">==</span><span class="token number">1.2</span><span class="token number">.1</span>
</code></pre> 
<pre><code class="prism language-c"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">load</span> <span class="token expression">the required Python modules</span></span>
import pandas as pd
import numpy as np
import networkx as nx
import os

import stellargraph as sg
from stellargraph<span class="token punctuation">.</span>mapper import FullBatchNodeGenerator
from stellargraph<span class="token punctuation">.</span>layer import GCN

import tensorflow as tf
from tensorflow<span class="token punctuation">.</span>keras import layers<span class="token punctuation">,</span> optimizers<span class="token punctuation">,</span> losses<span class="token punctuation">,</span> metrics<span class="token punctuation">,</span> Model
from sklearn import preprocessing<span class="token punctuation">,</span> model_selection
from IPython<span class="token punctuation">.</span>display import display<span class="token punctuation">,</span> HTML
import matplotlib<span class="token punctuation">.</span>pyplot as plt
<span class="token operator">%</span>matplotlib <span class="token keyword">inline</span>

</code></pre> 
<ol start="2"><li>加载数据集。我们将在这个例子中使用<code>PROTEINS</code>数据集，该数据集可从StellarGraph中获取，由1114个图组成，平均每个图有39个节点，73条边。每个节点由四个属性描述，属于两个类中的一个。</li></ol> 
<pre><code class="prism language-c">dataset <span class="token operator">=</span> sg<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span><span class="token function">PROTEINS</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token function">display</span><span class="token punctuation">(</span><span class="token function">HTML</span><span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>description<span class="token punctuation">)</span><span class="token punctuation">)</span>
graphs<span class="token punctuation">,</span> graph_labels <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>Each graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes.
</code></pre> 
<pre><code class="prism language-c"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">let</span><span class="token expression">'s print some info to better understand the dataset</span></span>
<span class="token function">print</span><span class="token punctuation">(</span>graphs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
graph_labels<span class="token punctuation">.</span><span class="token function">value_counts</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">to_frame</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>StellarGraph: Undirected multigraph
 Nodes: 42, Edges: 162

 Node types:
  default: [42]
    Features: float32 vector, length 4
    Edge types: default-default-&gt;default

 Edge types:
    default-default-&gt;default: [162]
        Weights: all 1 (default)
        Features: none
label
1	663
2	450
</code></pre> 
<ol start="3"><li>创建模型。它将由两个输出尺寸为64和32的图卷积层组成，然后分别接入ReLU激活层。输出为两个嵌入的欧几里德距离:</li></ol> 
<pre><code class="prism language-c"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">TODO</span></span>
generator <span class="token operator">=</span> sg<span class="token punctuation">.</span>mapper<span class="token punctuation">.</span><span class="token function">PaddedGraphGenerator</span><span class="token punctuation">(</span>graphs<span class="token punctuation">)</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">define</span> <span class="token macro-name">a</span> <span class="token expression">GCN model containing <span class="token number">2</span> layers of size <span class="token number">64</span> and <span class="token number">32</span><span class="token punctuation">,</span> respectively<span class="token punctuation">.</span> </span></span>
<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">ReLU activation function is used to add non<span class="token operator">-</span>linearity between layers</span></span>
gc_model <span class="token operator">=</span> sg<span class="token punctuation">.</span>layer<span class="token punctuation">.</span><span class="token function">GCNSupervisedGraphClassification</span><span class="token punctuation">(</span>
    <span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">"relu"</span><span class="token punctuation">,</span> <span class="token string">"relu"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> generator<span class="token punctuation">,</span> pool_all_layers<span class="token operator">=</span>True
<span class="token punctuation">)</span>

inp1<span class="token punctuation">,</span> out1 <span class="token operator">=</span> gc_model<span class="token punctuation">.</span><span class="token function">in_out_tensors</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
inp2<span class="token punctuation">,</span> out2 <span class="token operator">=</span> gc_model<span class="token punctuation">.</span><span class="token function">in_out_tensors</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

vec_distance <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token function">norm</span><span class="token punctuation">(</span>out1 <span class="token operator">-</span> out2<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

pair_model <span class="token operator">=</span> <span class="token function">Model</span><span class="token punctuation">(</span>inp1 <span class="token operator">+</span> inp2<span class="token punctuation">,</span> vec_distance<span class="token punctuation">)</span>
embedding_model <span class="token operator">=</span> <span class="token function">Model</span><span class="token punctuation">(</span>inp1<span class="token punctuation">,</span> out1<span class="token punctuation">)</span>
</code></pre> 
<ol start="4"><li>准备训练。对于每一对输入图，我们将分配一个相似度分数。在这种情况下可以使用任何图形相似性的概念，包括图形编辑距离。为了简单起见，我们将使用图的拉普拉斯谱距离:</li></ol> 
<pre><code class="prism language-c">def <span class="token function">graph_distance</span><span class="token punctuation">(</span>graph1<span class="token punctuation">,</span> graph2<span class="token punctuation">)</span><span class="token operator">:</span>
    spec1 <span class="token operator">=</span> nx<span class="token punctuation">.</span><span class="token function">laplacian_spectrum</span><span class="token punctuation">(</span>graph1<span class="token punctuation">.</span><span class="token function">to_networkx</span><span class="token punctuation">(</span>feature_attr<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">)</span>
    spec2 <span class="token operator">=</span> nx<span class="token punctuation">.</span><span class="token function">laplacian_spectrum</span><span class="token punctuation">(</span>graph2<span class="token punctuation">.</span><span class="token function">to_networkx</span><span class="token punctuation">(</span>feature_attr<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">)</span>
    k <span class="token operator">=</span> <span class="token function">min</span><span class="token punctuation">(</span><span class="token function">len</span><span class="token punctuation">(</span>spec1<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">len</span><span class="token punctuation">(</span>spec2<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span><span class="token function">norm</span><span class="token punctuation">(</span>spec1<span class="token punctuation">[</span><span class="token operator">:</span>k<span class="token punctuation">]</span> <span class="token operator">-</span> spec2<span class="token punctuation">[</span><span class="token operator">:</span>k<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-c">graph_idx <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span><span class="token function">RandomState</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">randint</span><span class="token punctuation">(</span><span class="token function">len</span><span class="token punctuation">(</span>graphs<span class="token punctuation">)</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
targets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token function">graph_distance</span><span class="token punctuation">(</span>graphs<span class="token punctuation">[</span>left<span class="token punctuation">]</span><span class="token punctuation">,</span> graphs<span class="token punctuation">[</span>right<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> left<span class="token punctuation">,</span> right in graph_idx<span class="token punctuation">]</span>
train_gen <span class="token operator">=</span> generator<span class="token punctuation">.</span><span class="token function">flow</span><span class="token punctuation">(</span>graph_idx<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> targets<span class="token operator">=</span>targets<span class="token punctuation">)</span>
</code></pre> 
<ol start="5"><li>编译和训练模型。我们将使用自适应矩估计优化器(Adam)，学习速率参数设置为1e-2。我们将使用的损失函数定义为预测和ground-truth距离之间的最小平方误差。模型将500次迭代训练</li></ol> 
<pre><code class="prism language-c">pair_model<span class="token punctuation">.</span><span class="token function">compile</span><span class="token punctuation">(</span>optimizers<span class="token punctuation">.</span><span class="token function">Adam</span><span class="token punctuation">(</span><span class="token number">1e-2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">"mse"</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-c">history <span class="token operator">=</span> pair_model<span class="token punctuation">.</span><span class="token function">fit</span><span class="token punctuation">(</span>train_gen<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
sg<span class="token punctuation">.</span>utils<span class="token punctuation">.</span><span class="token function">plot_history</span><span class="token punctuation">(</span>history<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/58/4f/79EPIABE_o.png" alt="在这里插入图片描述"><br> 6. 检查和可视化结果。<br> 在训练之后可以检查和可视化学习到的表示。由于输出是32维的，我们需要一种方法来定性地评估嵌入，例如，通过在二维空间中绘制它们。我们将使用T-SNE。把嵌入图画出来。在图中，每个点(嵌入图)都根据对应的标签(蓝色=0，红色=1)着色。结果如图所示。</p> 
<p>以上只是学习图形嵌入的一种可能的方法。可以尝试更先进的解决方案，以更好地适应感兴趣的问题。</p> 
<pre><code class="prism language-c">embeddings <span class="token operator">=</span> embedding_model<span class="token punctuation">.</span><span class="token function">predict</span><span class="token punctuation">(</span>generator<span class="token punctuation">.</span><span class="token function">flow</span><span class="token punctuation">(</span>graphs<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-c">from sklearn<span class="token punctuation">.</span>manifold import TSNE

tsne <span class="token operator">=</span> <span class="token function">TSNE</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
two_d <span class="token operator">=</span> tsne<span class="token punctuation">.</span><span class="token function">fit_transform</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-c">plt<span class="token punctuation">.</span><span class="token function">scatter</span><span class="token punctuation">(</span>two_d<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> two_d<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span>graph_labels<span class="token punctuation">.</span>cat<span class="token punctuation">.</span>codes<span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"jet"</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">savefig</span><span class="token punctuation">(</span><span class="token string">'embedding_TSNE.png'</span><span class="token punctuation">,</span>dpi<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/7b/b7/O6rFGWUY_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_317"></a>总结</h2> 
<p>在本章中，我们学习了如何将无监督机器学习有效地应用于图来解决实际问题，如节点和图表示学习。</p> 
<p>我们首先分析了浅嵌入方法，这是一组能够学习并只返回学习到的输入数据的嵌入值的算法。</p> 
<p>然后，我们学习了如何使用自动编码器算法，通过在低维空间中保存重要信息来编码输入。通过学习允许我们重构成对节点/图相似性的嵌入，我们还了解了这种思想如何适用于图。</p> 
<p>最后，我们介绍了GNN背后的主要概念。并介绍如何将卷积等概念应用到图中。</p> 
<p>在下一章中，我们将在有监督的环境中修改这些概念。监督学习会提供一个目标标签，目的是学习输入和输出之间的映射。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2c18445bbc0a72e6ff5dbd3c335823ed/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">win10添加开机自启软件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7be5d5dcf2d99de3312986710a1c2e6f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">intellij idea关闭代码检查提高性能解决卡顿</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>