<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Video SR-1 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Video SR-1" />
<meta property="og:description" content="一、Video Super-Resolution With Convolutional Neural Networks （VSRNet-2016） 滑动窗口&#43;LR图像运动补偿&#43;特征拼接 1、主要工作 提出基于CNN的视频超分算法；提出三种结构，用来融合相邻的多帧信息；提出预训练方法，利用图像数据对VSR模型预训练；利用一种自适应运动补偿方案来处理视频中的快速运动物体和运动模糊； 2、三种结构 3、运动补偿（Motion Compensation） 运动补偿（MC）的作用其实就是多个相邻帧的对齐。
基于运动补偿算法（Druleas algorithm），作者提出自适应运动补偿（AMC），用来解决大运动，严重运动模糊的情况。
从公式可以看出，当运动较大时，即e（i,j）较大时，r（i,j）较小，r小，则t帧权重大，反之，参考帧权重增大。所以，当运动大时，目标帧权重大，当运动小时，参考帧权重增大。
自适应运动补偿基于运动补偿算法改进得来，根据不匹配误差e（i，j）的大小，确定当前帧与相邻帧的权重。作者实验中表明AMC在大运动视频中优于MC算法。
二、Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation（VESPCN-CVPR2018） 滑动窗口&#43;LR运动估计（由粗到细网络）&#43;特征拼接3D卷积。SR网络与运动估计网络分开训练。 1、主要工作 提出了一种基于亚像素卷积和时空网络的视频SR实时算法（VESPCN），提高了精度和时间一致性。提出了一种基于多尺度空间变换网络的密集帧间运动补偿方法。 2、网络结构 3、Spatial transformer motion compensation 网络结构
分两步预测光流：先粗略预测光流δc，然后精细预测光流δf，最后将两者相加达到总的光流，利用光流和t&#43;1帧进行Warp操作，得到修正后的t&#43;1帧
class MotionCompensator(nn.Module): def __init__(self, args): self.device = &#39;cuda&#39; if args.cpu: self.device = &#39;cpu&#39; super(MotionCompensator, self).__init__() print(&#34;Creating Motion compensator&#34;) def _gconv(in_channels, out_channels, kernel_size=3, groups=1, stride=1, bias=True): return nn.Conv2d(in_channels*groups, out_channels*groups, kernel_size, groups=groups, stride=stride, padding=(kernel_size // 2), bias=bias) # Coarse flow coarse_flow = [_gconv(2, 24, kernel_size=5, groups=args." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/07f2e98bacba6d0d82f9bf0bc7667e31/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-01T16:10:31+08:00" />
<meta property="article:modified_time" content="2021-06-01T16:10:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Video SR-1</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="Video_SuperResolution_With_Convolutional_Neural_Networks_VSRNet2016_0"></a>一、Video Super-Resolution With Convolutional Neural Networks （VSRNet-2016）</h3> 
<ul><li>滑动窗口+LR图像运动补偿+特征拼接</li></ul> 
<h5><a id="1_3"></a>1、主要工作</h5> 
<ul><li>提出基于CNN的视频超分算法；</li><li>提出三种结构，用来<strong>融合相邻的多帧信息</strong>；</li><li>提出预训练方法，利用图像数据对VSR模型预训练；</li><li>利用一种<strong>自适应运动补偿</strong>方案来处理视频中的快速运动物体和运动模糊；</li></ul> 
<h5><a id="2_8"></a>2、三种结构</h5> 
<p><img src="https://images2.imgbox.com/49/a4/JIYcYPHz_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="3Motion_Compensation_10"></a>3、运动补偿（Motion Compensation）</h5> 
<p>运动补偿（MC）的作用其实就是多个相邻帧的对齐。<br> 基于运动补偿算法（Druleas algorithm），作者提出<strong>自适应运动补偿（AMC）</strong>，用来解决大运动，严重运动模糊的情况。<br> 从公式可以看出，当运动较大时，即e（i,j）较大时，r（i,j）较小，r小，则t帧权重大，反之，参考帧权重增大。所以，当运动大时，目标帧权重大，当运动小时，参考帧权重增大。<br> <img src="https://images2.imgbox.com/06/11/2UNyuLO6_o.png" alt="在这里插入图片描述"><br> 自适应运动补偿基于运动补偿算法改进得来，根据不匹配误差e（i，j）的大小，确定当前帧与相邻帧的权重。作者实验中表明AMC在大运动视频中优于MC算法。</p> 
<h3><a id="RealTime_Video_SuperResolution_with_SpatioTemporal_Networks_and_Motion_CompensationVESPCNCVPR2018_16"></a>二、Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation（VESPCN-CVPR2018）</h3> 
<ul><li>滑动窗口+LR运动估计（由粗到细网络）+特征拼接3D卷积。</li><li>SR网络与运动估计网络分开训练。</li></ul> 
<h5><a id="1_19"></a>1、主要工作</h5> 
<ul><li>提出了一种基于<strong>亚像素卷积</strong>和时空网络的视频SR实时算法（<strong>VESPCN</strong>），提高了精度和时间一致性。</li><li>提出了一种基于<strong>多尺度空间变换网络的密集帧间运动补偿方法</strong>。</li></ul> 
<h5><a id="2_22"></a>2、网络结构</h5> 
<p><img src="https://images2.imgbox.com/55/1f/aPlCPd2o_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="3Spatial_transformer_motion_compensation_25"></a>3、Spatial transformer motion compensation</h5> 
<ul><li><strong>网络结构</strong><br> 分两步预测光流：先粗略预测光流δc，然后精细预测光流δf，最后将两者相加达到总的光流，利用光流和t+1帧进行Warp操作，得到修正后的t+1帧<br> <img src="https://images2.imgbox.com/45/cc/kDV9Qu7k_o.png" alt="在这里插入图片描述"></li></ul> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MotionCompensator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> <span class="token string">'cuda'</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>cpu<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>device <span class="token operator">=</span> <span class="token string">'cpu'</span> 
        <span class="token builtin">super</span><span class="token punctuation">(</span>MotionCompensator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Creating Motion compensator"</span><span class="token punctuation">)</span>

        <span class="token keyword">def</span> <span class="token function">_gconv</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">*</span>groups<span class="token punctuation">,</span> out_channels<span class="token operator">*</span>groups<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> groups<span class="token operator">=</span>groups<span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span>
                             padding<span class="token operator">=</span><span class="token punctuation">(</span>kernel_size <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>

        <span class="token comment"># Coarse flow</span>
        coarse_flow <span class="token operator">=</span> <span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        coarse_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        coarse_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        coarse_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        coarse_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        coarse_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>PixelShuffle<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>C_flow <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>coarse_flow<span class="token punctuation">)</span>

        <span class="token comment"># Fine flow</span>
        fine_flow <span class="token operator">=</span> <span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            fine_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        fine_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>_gconv<span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>args<span class="token punctuation">.</span>n_colors<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        fine_flow<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>PixelShuffle<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>F_flow <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>fine_flow<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> frame_1<span class="token punctuation">,</span> frame_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Create identity flow</span>
        x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> frame_1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> frame_1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        xv<span class="token punctuation">,</span> yv <span class="token operator">=</span> np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
        id_flow <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>np<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>xv<span class="token punctuation">,</span> yv<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>identity_flow <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>id_flow<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

        <span class="token comment"># Coarse flow</span>
        coarse_in <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>frame_1<span class="token punctuation">,</span> frame_2<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        coarse_out <span class="token operator">=</span> self<span class="token punctuation">.</span>C_flow<span class="token punctuation">(</span>coarse_in<span class="token punctuation">)</span>
        coarse_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/=</span> frame_1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>
        coarse_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/=</span> frame_2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
        frame_2_compensated_coarse <span class="token operator">=</span> self<span class="token punctuation">.</span>warp<span class="token punctuation">(</span>frame_2<span class="token punctuation">,</span> coarse_out<span class="token punctuation">)</span>
        
        <span class="token comment"># Fine flow</span>
        fine_in <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>frame_1<span class="token punctuation">,</span> frame_2<span class="token punctuation">,</span> frame_2_compensated_coarse<span class="token punctuation">,</span> coarse_out<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        fine_out <span class="token operator">=</span> self<span class="token punctuation">.</span>F_flow<span class="token punctuation">(</span>fine_in<span class="token punctuation">)</span>
        fine_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/=</span> frame_1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>
        fine_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/=</span> frame_2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
        flow <span class="token operator">=</span> <span class="token punctuation">(</span>coarse_out <span class="token operator">+</span> fine_out<span class="token punctuation">)</span>

        frame_2_compensated <span class="token operator">=</span> self<span class="token punctuation">.</span>warp<span class="token punctuation">(</span>frame_2<span class="token punctuation">,</span> flow<span class="token punctuation">)</span>

        <span class="token keyword">return</span> frame_2_compensated<span class="token punctuation">,</span> flow

    <span class="token keyword">def</span> <span class="token function">warp</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> flow<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># https://discuss.pytorch.org/t/solved-how-to-do-the-interpolating-of-optical-flow/5019</span>
        <span class="token comment"># permute flow N C H W -&gt; N H W C</span>
        img_compensated <span class="token operator">=</span> F<span class="token punctuation">.</span>grid_sample<span class="token punctuation">(</span>img<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token operator">-</span>flow<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">+</span>self<span class="token punctuation">.</span>identity_flow<span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding_mode<span class="token operator">=</span><span class="token string">'border'</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> img_compensated
</code></pre> 
<h5><a id="_96"></a>光流估计损失函数</h5> 
<p>MSE损失保证t+1帧与t帧尽可能接近。<br> 在经典的光流方法中，通常会约束光流在空间中的平滑度，所以利用Huber损失，最小化对光流图梯度。<br> <img src="https://images2.imgbox.com/c8/71/5wzHpjqT_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Approx_Huber_Loss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Approx_Huber_Loss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span> <span class="token keyword">if</span> args<span class="token punctuation">.</span>cpu <span class="token keyword">else</span> <span class="token string">'cuda'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sobel_filter_X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sobel_filter_Y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sobel_filter_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sobel_filter_X<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sobel_filter_Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sobel_filter_Y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.01</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> flow<span class="token punctuation">)</span><span class="token punctuation">:</span>
        flow_X <span class="token operator">=</span> flow<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>
        flow_Y <span class="token operator">=</span> flow<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        grad_X <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>flow_X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>sobel_filter_X<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        grad_Y <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>flow_Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>sobel_filter_Y<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        huber <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>epsilon <span class="token operator">+</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>grad_X<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">+</span>grad_Y<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> huber
</code></pre> 
<h3><a id="Detailrevealing_Deep_Video_SuperresolutionICCV2017_121"></a>三、Detail-revealing Deep Video Super-resolution（ICCV2017）</h3> 
<ul><li>滑动窗口+LR图像运动补偿（采用VESPCN中的光流估计方法，然后SPMC层利用光流直接对参考帧上采样重建）+LSTM多帧特征融合；</li><li>先单独训练ME模块，然后联合训练。</li></ul> 
<h5><a id="1_126"></a>1、主要工作</h5> 
<p>提出了<strong>亚像素运动补偿</strong>（Sub-pixel Motion Compensation, SPMC）层，可以<strong>同时完成超分辨和运动补偿</strong>。</p> 
<h5><a id="2_128"></a>2、网络流程图</h5> 
<ul><li>利用运动估计（ME）方法预测光流；</li><li>利用光流和LR参考帧，经过SPMC层，直接重建出参考帧的HR图像；</li><li>利用U-Net的encoder部分提取特征，在UNet 最底层利用LSTM融合多帧信息；</li><li>利用decoder部分上采样完成重建。</li></ul> 
<p><img src="https://images2.imgbox.com/4b/b9/vpMcsdwz_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="3SPMC_135"></a>3、SPMC层</h5> 
<p>分两步：坐标变换+双线性插值上采样<br> <img src="https://images2.imgbox.com/24/80/BTIwmkp8_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="4Detail_Fusion_Net_139"></a>4、Detail Fusion Net</h5> 
<p>利用LSTM融合多帧信息<br> <img src="https://images2.imgbox.com/5f/d6/eM4Sdr2b_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks_BRCNTPAMI2018_143"></a>四、Video Super-Resolution via Bidirectional Recurrent Convolutional Networks （BRCN-TPAMI2018）</h3> 
<p>双向RNN（无光流估计）+特征拼接</p> 
<h5><a id="1_146"></a>1、主要工作</h5> 
<ul><li>提出了一种用于多帧SR的双向循环卷积网络（BRCN），其中时间相关性可以通过递归卷积和3D前馈卷积有效地建模。</li></ul> 
<h5><a id="2BRCN_149"></a>2、BRCN结构</h5> 
<p>结构很简单，就是一个双向RNN，没有对齐操作，利用RNN隐式学习多帧空间信息。<br> <img src="https://images2.imgbox.com/6d/2f/1iAw69RK_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Building_an_EndtoEnd_SpatialTemporal_Convolutional_Network_for_Video_SuperResolution_AAAI_2017_153"></a>五、Building an End-to-End Spatial-Temporal Convolutional Network for Video Super-Resolution （AAAI 2017）</h3> 
<ul><li>双向LSTM（无光流估计）+直接拼接融合</li></ul> 
<h5><a id="1_157"></a>1、主要工作</h5> 
<ul><li>提出了一种深度时空网络（STCN）。 基于LSTM，作者提出了一种<strong>双向</strong>、<strong>多尺度</strong>、<strong>卷积LSTM</strong>（BMC-LSTM）来融合多帧信息；</li><li>STCN未进行光流法对齐。作者认为，显式建模可能不足以探索时间信息，因为有太多种类的时间关系(物体运动、颜色过渡和补丁相似)，无法完全建模。所以完全通过隐藏层隐式发掘空间信息。</li></ul> 
<h5><a id="2STCN_160"></a>2、STCN结构</h5> 
<p>其双向RNN结构与BRCN一样，分前向和后向。<br> <img src="https://images2.imgbox.com/07/75/HZcRe1wT_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="3BMCLSTM_163"></a>3、BMC-LSTM</h5> 
<p>前向分支提取前向特征，后向分支提取后向特征，然后利用1x1卷积融合。<br> <img src="https://images2.imgbox.com/23/ec/XQyBAmrS_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="4MCLSTM_166"></a>4、MC-LSTM</h5> 
<p>该模块利用前向特征或者后向特征来<strong>更新当前帧的特征</strong>。<br> 先特征拼接，然后利用多分支提取多尺度信息，最后融合多尺度特征，得到<br> <img src="https://images2.imgbox.com/f6/8f/bNegUKbn_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="FrameRecurrent_Video_SuperResolution_FRVSR_CVPR_2018_171"></a>六、Frame-Recurrent Video Super-Resolution （FRVSR CVPR 2018）</h3> 
<ul><li>RNN+利用LR图像光流估计（但是光流图作用在HR图像上）+拼接特征融合。</li><li>光流与重建联合训练</li></ul> 
<h5><a id="1_175"></a>1、出发点</h5> 
<ul><li>对每个输入帧进行多次处理和变形，增加了计算成本;</li><li>对每个输出帧进行独立条件下的估计，限制了系统产生时间一致结果的能力。</li></ul> 
<h5><a id="2_179"></a>2、主要工作</h5> 
<ul><li>提出了一个循环框架（<strong>FRVSR</strong>），使用<strong>前一帧的HR估计生成后续帧</strong>，从而产生一个时间一致结果（temporally consistent）的有效模型；</li><li>与现有方法不同的是，所提出的框架可以在不增加计算量的情况下在大时间范围内传播信息。</li></ul> 
<h5><a id="3_182"></a>3、算法流程</h5> 
<ul><li>利用FNet进行光流估计，得到LR的光流；</li><li>对LR的光流上采样，得到HR的光流；</li><li><strong>利用HR光流校正前一帧重建好的HR图像</strong>；</li><li>将校正好的t-1帧的HR利用一个网络进行下采样提取特征得到t-1帧的特征；</li><li>将t-1帧的特征与t帧特征融合，完成重建。<br> <img src="https://images2.imgbox.com/c5/f5/cT8iCpAR_o.png" alt="在这里插入图片描述"></li></ul> 
<h5><a id="4FNetSRNet_189"></a>4、FNet与SRNet</h5> 
<p><img src="https://images2.imgbox.com/f6/c7/vi67LTFJ_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="5_192"></a>5、损失函数</h5> 
<p>重建损失+光流损失<br> <img src="https://images2.imgbox.com/d9/f6/sTpaZbL3_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="TDAN_Temporally_Deformable_Alignment_Network_for_Video_SuperResolution_2018_197"></a>七、TDAN: Temporally Deformable Alignment Network for Video Super-Resolution （2018）</h3> 
<p>本文首次提出利用<strong>可变形性卷积</strong>解决多帧图像对齐问题。</p> 
<ul><li>滑动窗口（5帧）+可变形性卷积DCN对齐+LR图像直接拼接融合；</li><li>对齐与重建联合训练；</li></ul> 
<h5><a id="1_203"></a>1、出发点</h5> 
<p>现有方法利用光流法对多帧图像进行对齐，因此，这些模型的性能将很大程度上依赖于光流的预测精度，而不准确的光流将导致参考帧中出现伪影，这些伪影也将传播到重建的HR视频帧中。</p> 
<h5><a id="2_205"></a>2、主要贡献</h5> 
<ul><li>提出了一种新的用于<strong>特征级对齐</strong>的<strong>时间变形对齐网络(TDAN)</strong>，避免了以往基于光流的两阶段对齐方法。</li><li>提出了一个基于TDAN的<strong>端到端</strong>可训练的VSR框架。</li></ul> 
<h5><a id="3_208"></a>3、方法介绍</h5> 
<ul><li>网络结构：<br> 主要分两部分：TDAN对齐部分+SR重建部分<br> （1）对齐部分：将参考帧向目标帧对齐；<br> （2）SR重建部分：将对齐的LR图像拼接，输入SR网络完成重建；<br> <img src="https://images2.imgbox.com/50/2e/lkRKGP7O_o.png" alt="在这里插入图片描述"></li></ul> 
<h5><a id="4TDAN_214"></a>4、TDAN</h5> 
<p>TDAN分三部分：特征提取+对齐+对齐帧重建</p> 
<ul><li><strong>特征提取</strong><br> 提取不同帧的信息，提取的特征将用于特征的时间对齐。</li><li><strong>可变形性对齐（Deformable Alignment）+ 对齐帧重建</strong><br> DCN（Deformable Convolutional Networks）相关知识参考<a href="https://zhuanlan.zhihu.com/p/52476083" rel="nofollow">DCN</a><br> （1）对i帧和t帧的特征图进行卷积，得到偏移量θ；<br> （2）利用学习到的偏移量与i帧特征进行可变形性卷积，达到校正特征的目的<br> （3）对校正后的特征图进行重建，得到校正后的LR图像<br> （4）重复循环，得到所有参考帧的LR图像，拼接输出，作为SR重建部分的输入。</li></ul> 
<h5><a id="5_224"></a>5、损失函数</h5> 
<ul><li>训练TDAN，设计对齐损失：<br> <img src="https://images2.imgbox.com/9d/cd/cGtgjWZy_o.png" alt="在这里插入图片描述"></li><li>训练SR模型，常规的L1损失：<br> <img src="https://images2.imgbox.com/a6/8b/gRwEDEN1_o.png" alt="在这里插入图片描述"></li><li>联合训练，总体损失为：<br> <img src="https://images2.imgbox.com/2e/42/7pZtaPer_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="EDVR_Video_Restoration_with_Enhanced_Deformable_Convolutional_Networks2019_NTIRE19_231"></a>八、EDVR: Video Restoration with Enhanced Deformable Convolutional Networks（2019 NTIRE19冠军）</h3> 
<ul><li>滑动窗口+DCN对齐（级联+金字塔）+时空注意力特征融合</li></ul> 
<h5><a id="1_234"></a>1、出发点</h5> 
<ul><li>如何对齐大运动下（大偏移量）的多个帧（对齐问题）；</li><li>如何有效地融合带有运动和模糊的多个帧（多帧信息融合问题）；</li></ul> 
<h5><a id="2_237"></a>2、主要工作</h5> 
<ul><li>首先，为了处理大型运动，本文设计了一个<strong>金字塔、级联和可变形(PCD)对齐模块</strong>，在该模块中，以<strong>粗到细</strong>的方式使用可变形卷积在特征层面完成帧对齐。</li><li>其次，提出时空注意(Temporal and Spatial Attention, TSA)融合模块，在该模块中，注意力同时应用于<strong>时间和空间</strong>，允许在不同的时间和空间位置上有不同的重点。</li></ul> 
<h5><a id="3EDVR_240"></a>3、EDVR整体流程图</h5> 
<ul><li>预处理去模糊；</li><li>PCD对齐模块；</li><li>TSA多帧信息融合；</li><li>重建；</li></ul> 
<p><img src="https://images2.imgbox.com/83/89/LesBn4uc_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="4PCD_247"></a>4、PCD模块</h5> 
<p>PCD模块主要由三部分组成：特征金字塔、级联和可变形卷积对齐。<br> <img src="https://images2.imgbox.com/92/2e/bE5zCv6P_o.png" alt="在这里插入图片描述"><br> 第l层的<strong>偏移量</strong>和<strong>校正后的特征</strong>的计算公式为：<br> <img src="https://images2.imgbox.com/01/23/dPhEZplG_o.png" alt="在这里插入图片描述"><br> 式中用到的（ ，）为拼接操作，g和f为多层CNN，上采样采用bilinear。</p> 
<h5><a id="5TSA_253"></a>5、时空注意力特征融合（TSA）</h5> 
<p>帧间时间关系和帧内空间关系在融合中至关重要：<br> （1）由于遮挡、模糊区域和视差问题，不同的相邻帧信息不一样；<br> （2）前一个对齐阶段产生的不对齐会对随后的重建性能产生不利影响。<br> 为了解决上述问题，本文提出了TSA（时间空间注意力）融合模块，在每一帧上分配像素级聚合权值。<br> <img src="https://images2.imgbox.com/92/b8/5IOPRs3j_o.png" alt="在这里插入图片描述"></p> 
<ul><li>参考帧与目标帧之间相似性度量，h()即为时间注意力图： <img src="https://images2.imgbox.com/a8/29/zgWavRdH_o.png" alt="在这里插入图片描述"><br> 将时间注意力图与参考帧做主元素乘法加权，然后拼接所有加权后的，最后卷积完成特征融合。<br> <img src="https://images2.imgbox.com/8f/ce/vxKsQVuG_o.png" alt="在这里插入图片描述"></li><li>下半部分为空间注意力部分。利用U-Net结构增加感受野，得到空间Mask，最后和特征图逐位置相乘加权。</li></ul> 
<h3><a id="BasicVSR_The_Search_for_Essential_Components_in_Video_SuperResolution_and_Beyond_CVPR2021_265"></a>九、BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond （CVPR2021）</h3> 
<ul><li>双向RNN+光流估计+特征内对齐并拼接融合</li></ul> 
<h5><a id="1_269"></a>1、不同方法策略对比</h5> 
<p><img src="https://images2.imgbox.com/c2/6f/DIHeWQcY_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="2_271"></a>2、主要工作</h5> 
<ul><li>本文<strong>BasicVSR</strong>的方法主要基于<strong>BRCN</strong>，在BRCN基础上加入<strong>光流估计</strong>对特征进行对齐。</li><li>基于BasicVSR提出<strong>IconVSR</strong>，在两方面做出改进：<br> （1）第一个为<strong>information-refill</strong>。该机制利用一个额外的模块从稀疏选择的帧(关键帧)中提取特征，然后将这些特征插入到主网络中进行特征细化。<br> （2）第二个是耦合传播（<strong>coupled propagation</strong>）方案，它促进了前向和后向传播分支之间的信息交换。</li></ul> 
<h5><a id="3BasicVSR_276"></a>3、BasicVSR网络结构</h5> 
<p>基本架构为BRCN，S、W和R分别表示光流估计、对齐和残差块。<br> <img src="https://images2.imgbox.com/04/76/iwdTWKXj_o.png" alt="在这里插入图片描述"></p> 
<ul><li>相邻帧个数对结果的影响：<br> <img src="https://images2.imgbox.com/d4/b1/7GQG3g3U_o.png" alt="在这里插入图片描述"></li><li>双向传播与单向传播结果对比：<br> <img src="https://images2.imgbox.com/4d/18/NDHhy60X_o.png" alt="在这里插入图片描述"></li></ul> 
<h5><a id="4_283"></a>4、对齐问题</h5> 
<p>对齐问题有三种解决方法：</p> 
<ul><li><strong>无对齐环节</strong>：如BRCN，基于递归的方法没有额外的对齐环节。作者进行实验，在BasicVSR中加入特征域对齐操作比不加入对齐操作精度PSNR提升1.19dB。</li><li><strong>图像对齐</strong>：目前多数方法基于该方法进行，先对LR输入图像对齐，再提取特征。作者进行实验，在BasicVSR中加入图像对齐操作比加入特征对齐操作精度PSNR降低0.17dB。</li><li><strong>特征对齐</strong>：本文采用<strong>特征域内对齐</strong>方法，之前方法为LR与对齐的LR拼接，本文方法为LR与对齐的特征拼接。利用LR计算光流，在特征域内完成对齐，然后<strong>利用对齐的参考帧的特征与目标帧LR拼接</strong>，利用网络残差网络得到融合后的特征。<br> <img src="https://images2.imgbox.com/19/51/o3AoWTUB_o.png" alt="在这里插入图片描述"></li></ul> 
<h5><a id="5IconVSR__Informationrefill_mechanism_289"></a>5、IconVSR - Information-refill mechanism</h5> 
<p>在遮挡区域和图像边界上的不准确对齐是一个挑战，可以导致错误积累，特别是在我们的框架中采用长期传播。为了减轻这些错误特征带来的不良影响，我们提出了一种信息填充机制来进行特征细化。<br> <img src="https://images2.imgbox.com/b5/dc/zuQNSQdf_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/55/21/c7DNGSVh_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="6IconVSR__Coupled_Propagation_294"></a>6、IconVSR - Coupled Propagation</h5> 
<p>在BRCN中，特征在两个相反的方向独立传播，每个传播分支中的特征是基于<strong>部分信息计算</strong>的，这些信息来自<strong>之前的帧或之后的帧</strong>。<strong>为了利用序列中的信息</strong>，我们提出了一种<strong>耦合传播方案</strong>，在前向分支和后向分支之间加了一条通路，如上图所示，加了一条后向到前向的通路，这样前向分支就增加了来自后续帧的信息。由下公式可以看出，前向特征多了一项输入hib。<br> <img src="https://images2.imgbox.com/31/b6/rEaGXKyR_o.png" alt="在这里插入图片描述"><br> 但实际上在重建第i帧的信息时，前向和后向特征又进行了融合，感觉这个操作有点多余，但是实验上好像又有点效果。。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/edd4b9b5b07b514b6068aec68768d5a8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">微信小程序跳转到另一个小程序 wx.navigateToMiniProgram 方法，调试</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ef0e0916aa8eac2b5bfa454895c64aa8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">树和二叉树：二叉树基本运算及其实现</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>