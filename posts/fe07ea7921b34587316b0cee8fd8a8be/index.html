<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python 实现 GIF 动图以及视频卡通化，两脚踢碎次元壁 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python 实现 GIF 动图以及视频卡通化，两脚踢碎次元壁" />
<meta property="og:description" content="作者 | 剑客阿良_ALiang
来源 | CSDN博客
前言
今天我继续魔改一下，让该模型可以支持将gif动图或者视频，也做成卡通化效果。毕竟一张图可以那就带边视频也可以，没毛病。所以继给次元壁来了一拳，我在加两脚。
项目github地址：https://github.com/Hy-1990/hy-cartoon
环境依赖
除了上一篇文章中的依赖，还需要加一些其他依赖，requirements.txt如下：
核心代码
不废话了，先上gif代码。
gif动图卡通化
实现代码如下：
#!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2021/12/5 18:10 # @Author : 剑客阿良_ALiang # @Site : # @File : gif_cartoon_tool.py # !/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2021/12/5 0:26 # @Author : 剑客阿良_ALiang # @Site : # @File : video_cartoon_tool.py # !/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2021/12/4 22:34 # @Author : 剑客阿良_ALiang # @Site : # @File : image_cartoon_tool." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/fe07ea7921b34587316b0cee8fd8a8be/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-25T18:00:05+08:00" />
<meta property="article:modified_time" content="2022-05-25T18:00:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python 实现 GIF 动图以及视频卡通化，两脚踢碎次元壁</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/00/9a/CqaBKDUu_o.gif" alt="2ba091b8cdcb0d1d5f6f25f4def1ee0c.gif"></p> 
 <p>作者 | 剑客阿良_ALiang</p> 
 <p style="text-align:justify;">来源 | CSDN博客</p> 
 <p style="text-align:center;"><strong><img src="https://images2.imgbox.com/ed/6a/7Q48VJld_o.png" alt="6c69d398f360598dbffa16e597c52584.png"></strong></p> 
 <p style="text-align:center;"><strong>前言</strong><br></p> 
 <p>今天我继续魔改一下，让该模型可以支持将gif动图或者视频，也做成卡通化效果。毕竟一张图可以那就带边视频也可以，没毛病。所以继给次元壁来了一拳，我在加两脚。</p> 
 <p>项目github地址：https://github.com/Hy-1990/hy-cartoon</p> 
 <p style="text-align:center;"><strong><img src="https://images2.imgbox.com/ed/f6/u4NXv88z_o.png" alt="764e0573c27e26a566956dc2731bb071.png"></strong></p> 
 <p style="text-align:center;"><strong>环境依赖</strong><br></p> 
 <p>除了上一篇文章中的依赖，还需要加一些其他依赖，requirements.txt如下：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/32/b7/05cesibC_o.png" alt="f056cb0e37958631a3511272500a1bce.png"></p> 
 <p><strong>核心代码</strong></p> 
 <p>不废话了，先上gif代码。</p> 
 <p><strong>gif动图卡通化</strong></p> 
 <p>实现代码如下：</p> 
 <pre class="has"><code class="language-ruby">#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2021/12/5 18:10
# @Author  : 剑客阿良_ALiang
# @Site    : 
# @File    : gif_cartoon_tool.py
# !/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2021/12/5 0:26
# @Author  : 剑客阿良_ALiang
# @Site    :
# @File    : video_cartoon_tool.py


# !/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2021/12/4 22:34
# @Author  : 剑客阿良_ALiang
# @Site    :
# @File    : image_cartoon_tool.py


from PIL import Image, ImageEnhance, ImageSequence
import torch
from torchvision.transforms.functional import to_tensor, to_pil_image
from torch import nn
import os
import torch.nn.functional as F
import uuid
import imageio




# -------------------------- hy add 01 --------------------------
class ConvNormLReLU(nn.Sequential):
    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, pad_mode="reflect", groups=1, bias=False):
        pad_layer = {
            "zero": nn.ZeroPad2d,
            "same": nn.ReplicationPad2d,
            "reflect": nn.ReflectionPad2d,
        }
        if pad_mode not in pad_layer:
            raise NotImplementedError


        super(ConvNormLReLU, self).__init__(
            pad_layer[pad_mode](padding),
            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=stride, padding=0, groups=groups, bias=bias),
            nn.GroupNorm(num_groups=1, num_channels=out_ch, affine=True),
            nn.LeakyReLU(0.2, inplace=True)
        )




class InvertedResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, expansion_ratio=2):
        super(InvertedResBlock, self).__init__()


        self.use_res_connect = in_ch == out_ch
        bottleneck = int(round(in_ch * expansion_ratio))
        layers = []
        if expansion_ratio != 1:
            layers.append(ConvNormLReLU(in_ch, bottleneck, kernel_size=1, padding=0))


        # dw
        layers.append(ConvNormLReLU(bottleneck, bottleneck, groups=bottleneck, bias=True))
        # pw
        layers.append(nn.Conv2d(bottleneck, out_ch, kernel_size=1, padding=0, bias=False))
        layers.append(nn.GroupNorm(num_groups=1, num_channels=out_ch, affine=True))


        self.layers = nn.Sequential(*layers)


    def forward(self, input):
        out = self.layers(input)
        if self.use_res_connect:
            out = input + out
        return out




class Generator(nn.Module):
    def __init__(self, ):
        super().__init__()


        self.block_a = nn.Sequential(
            ConvNormLReLU(3, 32, kernel_size=7, padding=3),
            ConvNormLReLU(32, 64, stride=2, padding=(0, 1, 0, 1)),
            ConvNormLReLU(64, 64)
        )


        self.block_b = nn.Sequential(
            ConvNormLReLU(64, 128, stride=2, padding=(0, 1, 0, 1)),
            ConvNormLReLU(128, 128)
        )


        self.block_c = nn.Sequential(
            ConvNormLReLU(128, 128),
            InvertedResBlock(128, 256, 2),
            InvertedResBlock(256, 256, 2),
            InvertedResBlock(256, 256, 2),
            InvertedResBlock(256, 256, 2),
            ConvNormLReLU(256, 128),
        )


        self.block_d = nn.Sequential(
            ConvNormLReLU(128, 128),
            ConvNormLReLU(128, 128)
        )


        self.block_e = nn.Sequential(
            ConvNormLReLU(128, 64),
            ConvNormLReLU(64, 64),
            ConvNormLReLU(64, 32, kernel_size=7, padding=3)
        )


        self.out_layer = nn.Sequential(
            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0, bias=False),
            nn.Tanh()
        )


    def forward(self, input, align_corners=True):
        out = self.block_a(input)
        half_size = out.size()[-2:]
        out = self.block_b(out)
        out = self.block_c(out)


        if align_corners:
            out = F.interpolate(out, half_size, mode="bilinear", align_corners=True)
        else:
            out = F.interpolate(out, scale_factor=2, mode="bilinear", align_corners=False)
        out = self.block_d(out)


        if align_corners:
            out = F.interpolate(out, input.size()[-2:], mode="bilinear", align_corners=True)
        else:
            out = F.interpolate(out, scale_factor=2, mode="bilinear", align_corners=False)
        out = self.block_e(out)


        out = self.out_layer(out)
        return out




# -------------------------- hy add 02 --------------------------


def handle(gif_path: str, output_dir: str, type: int, device='cpu'):
    _ext = os.path.basename(gif_path).strip().split('.')[-1]
    if type == 1:
        _checkpoint = './weights/paprika.pt'
    elif type == 2:
        _checkpoint = './weights/face_paint_512_v1.pt'
    elif type == 3:
        _checkpoint = './weights/face_paint_512_v2.pt'
    elif type == 4:
        _checkpoint = './weights/celeba_distill.pt'
    else:
        raise Exception('type not support')
    os.makedirs(output_dir, exist_ok=True)
    net = Generator()
    net.load_state_dict(torch.load(_checkpoint, map_location="cpu"))
    net.to(device).eval()
    result = os.path.join(output_dir, '{}.{}'.format(uuid.uuid1().hex, _ext))
    img = Image.open(gif_path)
    out_images = []
    for frame in ImageSequence.Iterator(img):
        frame = frame.convert("RGB")
        with torch.no_grad():
            image = to_tensor(frame).unsqueeze(0) * 2 - 1
            out = net(image.to(device), False).cpu()
            out = out.squeeze(0).clip(-1, 1) * 0.5 + 0.5
            out = to_pil_image(out)
            out_images.append(out)
    # out_images[0].save(result, save_all=True, loop=True, append_images=out_images[1:], duration=100)
    imageio.mimsave(result, out_images, fps=15)
    return result




if __name__ == '__main__':
    print(handle('samples/gif/128.gif', 'samples/gif_result/', 3, 'cuda'))</code></pre> 
 <p><strong>代码说明：</strong></p> 
 <p>1、主要的handle方法入参分别为：gif地址、输出目录、类型、设备使用（默认cpu，可选cuda使用显卡）。</p> 
 <p>2、类型主要是选择模型，最好用3，人像处理更生动一些。</p> 
 <p style="text-align:center;"><strong><img src="https://images2.imgbox.com/c5/a6/a2J794fM_o.png" alt="e37c137dcfffcd1016d66b1f1cc7a554.png"></strong></p> 
 <p style="text-align:center;"><strong>执行验证一下</strong><br></p> 
 <p>下面是我准备的gif素材</p> 
 <p><img src="https://images2.imgbox.com/93/1e/1e4eimJz_o.gif" alt="85860528a3549bf10309954fa3e3b568.gif"></p> 
 <p>执行结果如下：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/88/4a/UOoDLiUp_o.png" alt="65ec26d18cb2c9b8b3d475d42d9c6dc0.png"></p> 
 <p>看一下效果：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/07/72/puQohUO6_o.gif" alt="38489f8f9949839844c4bb686bfdc1f7.gif"></p> 
 <p style="text-align:center;"><strong><img src="https://images2.imgbox.com/16/7f/fRoJWYQs_o.png" alt="9aaadf81eed9a5e7f7ec961af78a2503.png"></strong></p> 
 <h3><strong>视频卡通化</strong><br></h3> 
 <p>实现代码如下：</p> 
 <pre class="has"><code class="language-python">#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2021/12/5 0:26
# @Author  : 剑客阿良_ALiang
# @Site    : 
# @File    : video_cartoon_tool.py


# !/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2021/12/4 22:34
# @Author  : 剑客阿良_ALiang
# @Site    :
# @File    : image_cartoon_tool.py


from PIL import Image, ImageEnhance
import torch
from torchvision.transforms.functional import to_tensor, to_pil_image
from torch import nn
import os
import torch.nn.functional as F
import uuid
import cv2
import numpy as np
import time
from ffmpy import FFmpeg




# -------------------------- hy add 01 --------------------------
class ConvNormLReLU(nn.Sequential):
    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, pad_mode="reflect", groups=1, bias=False):
        pad_layer = {
            "zero": nn.ZeroPad2d,
            "same": nn.ReplicationPad2d,
            "reflect": nn.ReflectionPad2d,
        }
        if pad_mode not in pad_layer:
            raise NotImplementedError


        super(ConvNormLReLU, self).__init__(
            pad_layer[pad_mode](padding),
            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=stride, padding=0, groups=groups, bias=bias),
            nn.GroupNorm(num_groups=1, num_channels=out_ch, affine=True),
            nn.LeakyReLU(0.2, inplace=True)
        )




class InvertedResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, expansion_ratio=2):
        super(InvertedResBlock, self).__init__()


        self.use_res_connect = in_ch == out_ch
        bottleneck = int(round(in_ch * expansion_ratio))
        layers = []
        if expansion_ratio != 1:
            layers.append(ConvNormLReLU(in_ch, bottleneck, kernel_size=1, padding=0))


        # dw
        layers.append(ConvNormLReLU(bottleneck, bottleneck, groups=bottleneck, bias=True))
        # pw
        layers.append(nn.Conv2d(bottleneck, out_ch, kernel_size=1, padding=0, bias=False))
        layers.append(nn.GroupNorm(num_groups=1, num_channels=out_ch, affine=True))


        self.layers = nn.Sequential(*layers)


    def forward(self, input):
        out = self.layers(input)
        if self.use_res_connect:
            out = input + out
        return out




class Generator(nn.Module):
    def __init__(self, ):
        super().__init__()


        self.block_a = nn.Sequential(
            ConvNormLReLU(3, 32, kernel_size=7, padding=3),
            ConvNormLReLU(32, 64, stride=2, padding=(0, 1, 0, 1)),
            ConvNormLReLU(64, 64)
        )


        self.block_b = nn.Sequential(
            ConvNormLReLU(64, 128, stride=2, padding=(0, 1, 0, 1)),
            ConvNormLReLU(128, 128)
        )


        self.block_c = nn.Sequential(
            ConvNormLReLU(128, 128),
            InvertedResBlock(128, 256, 2),
            InvertedResBlock(256, 256, 2),
            InvertedResBlock(256, 256, 2),
            InvertedResBlock(256, 256, 2),
            ConvNormLReLU(256, 128),
        )


        self.block_d = nn.Sequential(
            ConvNormLReLU(128, 128),
            ConvNormLReLU(128, 128)
        )


        self.block_e = nn.Sequential(
            ConvNormLReLU(128, 64),
            ConvNormLReLU(64, 64),
            ConvNormLReLU(64, 32, kernel_size=7, padding=3)
        )


        self.out_layer = nn.Sequential(
            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0, bias=False),
            nn.Tanh()
        )


    def forward(self, input, align_corners=True):
        out = self.block_a(input)
        half_size = out.size()[-2:]
        out = self.block_b(out)
        out = self.block_c(out)


        if align_corners:
            out = F.interpolate(out, half_size, mode="bilinear", align_corners=True)
        else:
            out = F.interpolate(out, scale_factor=2, mode="bilinear", align_corners=False)
        out = self.block_d(out)


        if align_corners:
            out = F.interpolate(out, input.size()[-2:], mode="bilinear", align_corners=True)
        else:
            out = F.interpolate(out, scale_factor=2, mode="bilinear", align_corners=False)
        out = self.block_e(out)


        out = self.out_layer(out)
        return out




# -------------------------- hy add 02 --------------------------


def handle(video_path: str, output_dir: str, type: int, fps: int, device='cpu'):
    _ext = os.path.basename(video_path).strip().split('.')[-1]
    if type == 1:
        _checkpoint = './weights/paprika.pt'
    elif type == 2:
        _checkpoint = './weights/face_paint_512_v1.pt'
    elif type == 3:
        _checkpoint = './weights/face_paint_512_v2.pt'
    elif type == 4:
        _checkpoint = './weights/celeba_distill.pt'
    else:
        raise Exception('type not support')
    os.makedirs(output_dir, exist_ok=True)
    # 获取视频音频
    _audio = extract(video_path, output_dir, 'wav')
    net = Generator()
    net.load_state_dict(torch.load(_checkpoint, map_location="cpu"))
    net.to(device).eval()
    result = os.path.join(output_dir, '{}.{}'.format(uuid.uuid1().hex, _ext))
    capture = cv2.VideoCapture(video_path)
    size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))
    print(size)
    videoWriter = cv2.VideoWriter(result, cv2.VideoWriter_fourcc(*'mp4v'), fps, size)
    cul = 0
    with torch.no_grad():
        while True:
            ret, frame = capture.read()
            if ret:
                print(ret)
                image = to_tensor(frame).unsqueeze(0) * 2 - 1
                out = net(image.to(device), False).cpu()
                out = out.squeeze(0).clip(-1, 1) * 0.5 + 0.5
                out = to_pil_image(out)
                contrast_enhancer = ImageEnhance.Contrast(out)
                img_enhanced_image = contrast_enhancer.enhance(2)
                enhanced_image = np.asarray(img_enhanced_image)
                videoWriter.write(enhanced_image)
                cul += 1
                print('第{}张图'.format(cul))
            else:
                break
    videoWriter.release()
    # 视频添加原音频
    _final_video = video_add_audio(result, _audio, output_dir)
    return _final_video




# -------------------------- hy add 03 --------------------------
def extract(video_path: str, tmp_dir: str, ext: str):
    file_name = '.'.join(os.path.basename(video_path).split('.')[0:-1])
    print('文件名:{}，提取音频'.format(file_name))
    if ext == 'mp3':
        return _run_ffmpeg(video_path, os.path.join(tmp_dir, '{}.{}'.format(uuid.uuid1().hex, ext)), 'mp3')
    if ext == 'wav':
        return _run_ffmpeg(video_path, os.path.join(tmp_dir, '{}.{}'.format(uuid.uuid1().hex, ext)), 'wav')




def _run_ffmpeg(video_path: str, audio_path: str, format: str):
    ff = FFmpeg(inputs={video_path: None},
                outputs={audio_path: '-f {} -vn'.format(format)})
    print(ff.cmd)
    ff.run()
    return audio_path




# 视频添加音频
def video_add_audio(video_path: str, audio_path: str, output_dir: str):
    _ext_video = os.path.basename(video_path).strip().split('.')[-1]
    _ext_audio = os.path.basename(audio_path).strip().split('.')[-1]
    if _ext_audio not in ['mp3', 'wav']:
        raise Exception('audio format not support')
    _codec = 'copy'
    if _ext_audio == 'wav':
        _codec = 'aac'
    result = os.path.join(
        output_dir, '{}.{}'.format(
            uuid.uuid4(), _ext_video))
    ff = FFmpeg(
        inputs={video_path: None, audio_path: None},
        outputs={result: '-map 0:v -map 1:a -c:v copy -c:a {} -shortest'.format(_codec)})
    print(ff.cmd)
    ff.run()
    return result




if __name__ == '__main__':
    print(handle('samples/video/981.mp4', 'samples/video_result/', 3, 25, 'cuda'))</code></pre> 
 <p><strong>代码说明：</strong></p> 
 <p>1、主要的实现方法入参分别为：视频地址、输出目录、类型、fps（帧率）、设备类型（默认cpu，可选择cuda显卡模式）。</p> 
 <p>2、类型主要是选择模型，最好用3，人像处理更生动一些。</p> 
 <p>3、代码设计思路：先将视频音频提取出来、将视频逐帧处理后写入新视频、新视频和原视频音频融合。</p> 
 <p>4、视频中间会产生临时文件，没有清理，如需要可以修改代码自行清理。</p> 
 <p style="text-align:center;"><strong><img src="https://images2.imgbox.com/c2/cb/ndifE5Vl_o.png" alt="4a7a400ed7b3d399d529ec3c1b8faf96.png"></strong></p> 
 <p style="text-align:center;"><strong>验证一下</strong><br></p> 
 <p>下面是我准备的视频素材截图，我会上传到github上。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/44/8b/Jq5pCFTv_o.png" alt="e2ad2a3cee2f5c04722db01cb59bfa08.png"></p> 
 <p>执行结果</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/20/09/FUSo9J2m_o.png" alt="f56429fa349be4d0ebce42eb8daeaafb.png"></p> 
 <p>看看效果截图</p> 
 <p><img src="https://images2.imgbox.com/be/7b/0nWS1nCH_o.png" alt="3cc37630d56fc524367bae699c76d044.png"></p> 
 <p>还是很不错的哦。</p> 
 <p style="text-align:center;"><strong><img src="https://images2.imgbox.com/14/68/HB11ChqQ_o.png" alt="cf5bb75ffbaa3bda68ac1633f25c51d8.png"></strong></p> 
 <p style="text-align:center;"><strong>总结</strong><br></p> 
 <p>这次可不是没什么好总结的，总结的东西蛮多的。首先我说一下这个开源项目目前模型的一些问题。</p> 
 <p>1、我测试了不少图片，总的来说对亚洲人的脸型不能很好的卡通化，但是欧美的脸型都比较好。所以还是训练的数据不是很够，但是能理解，毕竟要专门做卡通化的标注数据想想就是蛮头疼的事。所以我建议大家在使用的时候，多关注一下项目是否更新了最新的模型。</p> 
 <p>2、视频一但有字幕，会对字幕也做处理。所以可以考虑找一些视频和字幕分开的素材，效果会更好一些。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1b/db/agSiYX0G_o.gif" alt="043c69e65bf959627481c3aa9ca11eda.gif"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e3/56/pBeZeIK5_o.png" alt="13719f0231891ecc22912489bc0916d3.png"></p> 
 <p><strong>往期回顾</strong></p> 
 <p><a href="" rel="nofollow">介绍Pandas实战中的一些高端玩法</a></p> 
 <p><a href="" rel="nofollow">真香！详解Python好用的内置函数</a></p> 
 <p><a href="" rel="nofollow">架构师说 | 别等被偷家了再说数据安全！</a></p> 
 <p><a href="" rel="nofollow">如何用一行Python代码制作一个GUI？</a></p> 
 <pre class="has"><code class="language-go">分享
点收藏
点点赞
点在看</code></pre> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1e572be5a206e84ab2078ab66329ee05/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">迷宫问题java老鼠走迷宫（回溯法，递归，二维数组）（超级容易理解）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e42e9cd01541bb17ebea2e53514131ec/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">maven安装笔记</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>