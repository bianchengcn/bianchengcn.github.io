<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习实战—集成学习 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习实战—集成学习" />
<meta property="og:description" content="文章目录 一.简介1.1 集成学习1.2 随机森林 二.集成学习—投票分类器2.1 概念2.2 代码实现 三.集成学习—bagging和pasting3.1 简介3.2 Scikit-Learn中使用bagging和pasting3.3 包外评估3.4 随机补丁和随机子空间 四.集成学习—随机森林4.1 简介4.2 API使用4.3 极端随机树4.4 特征重要性 五.集成学习—提升法Boosting5.1 简介5.2 AdaBoost5.2 AdaBoost原理解析5.3 AdaBoost的API使用5.4 梯度提升5.5 梯度提升原理5.6 梯度提升API使用 一.简介 1.1 集成学习 一群人的智慧总是比一个人强，这就是集成学习的核心思想。如果你聚合一组预测器（比如分类器或回归器）的预测，得到的预测结果也比最好的单个预测器要好。这样的一组预测器称为集成，这种技术也叫集成学习。
1.2 随机森林 我们训练一组决策树分类器，每一棵树都基于训练集不同的随机子集进行训练。做出预测时，只需要获得所有树各自的预测，然后得票最多的类别作为预测结果。这样一组决策树的集成被称为随机森林，随机森林是迄今可用的最强大的机器学习算法之一。
二.集成学习—投票分类器 2.1 概念 如果我们已经训练好了一些分类器，并且每个分类器的准确率都比较客观，为了创建一个更好的分类器，最简单的方法就是聚合每个分类器的预测，然后将的票最多的结果作为预测类别。这种大多数投票分类器称为硬投票分类器。事实上，即使每个分类器都是弱学习器（意味着它的的预测效果只比随机预测好一点），通过集成依然可以实现一个强学习器
当预测器尽可能相互独立时，集成方法的效果最优。获得多种分类器的方法之一就是使用不同的算法进行训练（例如逻辑回归、SVM、Knn等）。这会增加它们犯不同类型错误的机会，从而提高集成的准确率。
2.2 代码实现 使用到了随机森林、SVM和逻辑回归来集成一个投票分类器
from sklearn.ensemble import RandomForestClassifier #随机森林分类器 from sklearn.ensemble import VotingClassifier #投票分类器 from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score iris=load_iris() #加载数据集 X_train, X_test, Y_train, Y_test = train_test_split(iris." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/400cadda5d14241b94008d2a52ea9e0b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-05T14:25:09+08:00" />
<meta property="article:modified_time" content="2022-11-05T14:25:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习实战—集成学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_2" rel="nofollow">一.简介</a></li><li><ul><li><a href="#11___3" rel="nofollow">1.1 集成学习</a></li><li><a href="#12__5" rel="nofollow">1.2 随机森林</a></li></ul> 
  </li><li><a href="#_8" rel="nofollow">二.集成学习—投票分类器</a></li><li><ul><li><a href="#21__9" rel="nofollow">2.1 概念</a></li><li><a href="#22__14" rel="nofollow">2.2 代码实现</a></li></ul> 
  </li><li><a href="#baggingpasting_40" rel="nofollow">三.集成学习—bagging和pasting</a></li><li><ul><li><a href="#31__41" rel="nofollow">3.1 简介</a></li><li><a href="#32_ScikitLearnbaggingpasting_50" rel="nofollow">3.2 Scikit-Learn中使用bagging和pasting</a></li><li><a href="#33__64" rel="nofollow">3.3 包外评估</a></li><li><a href="#34__77" rel="nofollow">3.4 随机补丁和随机子空间</a></li></ul> 
  </li><li><a href="#_91" rel="nofollow">四.集成学习—随机森林</a></li><li><ul><li><a href="#41__92" rel="nofollow">4.1 简介</a></li><li><a href="#42_API_94" rel="nofollow">4.2 API使用</a></li><li><a href="#43__103" rel="nofollow">4.3 极端随机树</a></li><li><a href="#44__105" rel="nofollow">4.4 特征重要性</a></li></ul> 
  </li><li><a href="#Boosting_119" rel="nofollow">五.集成学习—提升法Boosting</a></li><li><ul><li><a href="#51__120" rel="nofollow">5.1 简介</a></li><li><a href="#52_AdaBoost_123" rel="nofollow">5.2 AdaBoost</a></li><li><a href="#52_AdaBoost_128" rel="nofollow">5.2 AdaBoost原理解析</a></li><li><a href="#53_AdaBoostAPI_158" rel="nofollow">5.3 AdaBoost的API使用</a></li><li><a href="#54__168" rel="nofollow">5.4 梯度提升</a></li><li><a href="#55__171" rel="nofollow">5.5 梯度提升原理</a></li><li><a href="#56_API_198" rel="nofollow">5.6 梯度提升API使用</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_2"></a>一.简介</h2> 
<h3><a id="11___3"></a>1.1 集成学习</h3> 
<p>一群人的智慧总是比一个人强，这就是集成学习的核心思想。如果你聚合一组预测器（比如分类器或回归器）的预测，得到的预测结果也比最好的单个预测器要好。这样的一组预测器称为集成，这种技术也叫集成学习。</p> 
<h3><a id="12__5"></a>1.2 随机森林</h3> 
<p>我们训练一组决策树分类器，每一棵树都基于训练集不同的随机子集进行训练。做出预测时，只需要获得所有树各自的预测，然后得票最多的类别作为预测结果。这样一组决策树的集成被称为随机森林，随机森林是迄今可用的最强大的机器学习算法之一。</p> 
<h2><a id="_8"></a>二.集成学习—投票分类器</h2> 
<h3><a id="21__9"></a>2.1 概念</h3> 
<p>如果我们已经训练好了一些分类器，并且每个分类器的准确率都比较客观，为了创建一个更好的分类器，最简单的方法就是聚合每个分类器的预测，然后将的票最多的结果作为预测类别。这种大多数投票分类器称为硬投票分类器。事实上，即使每个分类器都是弱学习器（意味着它的的预测效果只比随机预测好一点），通过集成依然可以实现一个强学习器</p> 
<blockquote> 
 <p>当预测器尽可能相互独立时，集成方法的效果最优。获得多种分类器的方法之一就是使用不同的算法进行训练（例如逻辑回归、SVM、Knn等）。这会增加它们犯不同类型错误的机会，从而提高集成的准确率。</p> 
</blockquote> 
<h3><a id="22__14"></a>2.2 代码实现</h3> 
<blockquote> 
 <p>使用到了随机森林、SVM和逻辑回归来集成一个投票分类器</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble  <span class="token keyword">import</span> RandomForestClassifier  <span class="token comment">#随机森林分类器</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble  <span class="token keyword">import</span> VotingClassifier  <span class="token comment">#投票分类器</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> SVC
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_score
iris<span class="token operator">=</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#加载数据集</span>
X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> Y_train<span class="token punctuation">,</span> Y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>  random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>
log_clf<span class="token operator">=</span>LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#逻辑回归评估</span>
rnd_clf<span class="token operator">=</span>RandomForestClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span>
svm_clf<span class="token operator">=</span>SVC<span class="token punctuation">(</span><span class="token punctuation">)</span>
voting_clf<span class="token operator">=</span>VotingClassifier<span class="token punctuation">(</span>
 estimators<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'lr'</span><span class="token punctuation">,</span>log_clf<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'rf'</span><span class="token punctuation">,</span>rnd_clf<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'svc'</span><span class="token punctuation">,</span>svm_clf<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 voting<span class="token operator">=</span><span class="token string">'hard'</span>  <span class="token comment">#硬投票#</span>
<span class="token punctuation">)</span>
voting_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span>accuracy_score<span class="token punctuation">(</span>Y_test<span class="token punctuation">,</span>voting_clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>结果输出为预测准确率（预测全对为1）<br> <img src="https://images2.imgbox.com/63/41/ibT5TQNl_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>如果voting='hard’表示为voting='soft’这时候我们的硬投票分类器就变成了软投票分类器，从前面我们知道硬投票分类器是选择具有最高票数类的集合（如果有三个投票器中有两个认为当前水果是苹果，一个认为是橘子，则苹果的票最多所以硬投票法会认为当前水果是苹果），而软投票分类器下每个分类器会计算出类别的概率，然后将概率在所有单分类器上平均，最后选出平均概率最高的类作为预测（同样我们有两个个分类器，分类器 1 以 40% 的概率预测对象是一块苹果，而分类器2 以 60% 的概率预测它是一个苹果，那么软投票分类器会认为有(40%+60%)/2=50%的概率认为这个水果上苹果，最后软投票器会选择平均概率最高的类别）</p> 
</blockquote> 
<h2><a id="baggingpasting_40"></a>三.集成学习—bagging和pasting</h2> 
<h3><a id="31__41"></a>3.1 简介</h3> 
<blockquote> 
 <p>前面介绍投票分类器时我们获得不同分类器来集成学习的第一种方法，即直接使用不同的算法。现在介绍的另一种思想就是对于每个预测器都使用相同的机器学习算法，但是在不同的训练集的子集上训练（注意子集的选择上随机的）。在对于子集的不同的选取方式（下面我们会称为抽样方式）的不同，我们可以将抽样方式分为两种类型：</p> 
 <ol><li>bagging方法（bootstrap aggregating），也叫做自举汇聚法，采样时样本放回（即原训练集不会发生变化），下一个预测器继续抽样</li><li>pasting方法，采样时样本不放回（即原训练集变小了），下一个预测器继续抽样</li></ol> 
</blockquote> 
<p><img src="https://images2.imgbox.com/c1/64/cjAldW2L_o.jpg" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>采用bagging和pasting方法，没可以通过不同的CPU内核甚至不同的服务器并行地训练预测器，类似的预测同样也是可以并行的，这就是bagging和pasting方法如此流行的原因，它是非常易于扩展的。</p> 
</blockquote> 
<h3><a id="32_ScikitLearnbaggingpasting_50"></a>3.2 Scikit-Learn中使用bagging和pasting</h3> 
<blockquote> 
 <p>下面使用了BaggingClassifier类解决分类问题，预测器使用的机器学习算法是决策树算法</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> BaggingClassifier
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier
bag_clf<span class="token operator">=</span>BaggingClassifier<span class="token punctuation">(</span>DecisionTreeClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_estimators<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>max_samples<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>bootstrap<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
bag_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
y_pred<span class="token operator">=</span>bag_clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span> 
</code></pre> 
<blockquote> 
 <p>n_estimators=500：表示有500个决策树分类器<br> max_samples=100：表示每次从训练实例中选取100个实例进行训练<br> 如果基本的预测器都可以估计类别预测（即有predict_proba()方法），则BaggingClassifier会自动使用软投票机制（如果要使用pasting，则将bootstrap=false就行）</p> 
</blockquote> 
<h3><a id="33__64"></a>3.3 包外评估</h3> 
<ol><li>简介</li></ol> 
<blockquote> 
 <p>对于bagging采样方法，我们每次采样都会将样本放回原来的训练集，所以对于每个预测器抽样基于训练集的大小都是一样的，如果我们每次采样的比例假如我们都只采用60%，那么对于每个预测器都会剩下40%的实例没有使用到，这样的实例我们称为包外实例（oob），因此我们可以直接将每个预测器模型在这没使用到的40%的数据上进行评估，而无需再单独的区设置验证集来进行评估，这种思想我们就叫做包外评估。</p> 
</blockquote> 
<ol start="2"><li>API使用</li></ol> 
<blockquote> 
 <p>在Scikit-Learn中，我们创建BaggingClassfier时，设置oob_score=TRUE我们就可以使用包外评估了</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_score
bag_clf<span class="token operator">=</span>BaggingClassifier<span class="token punctuation">(</span>DecisionTreeClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_estimators<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>bootstrap<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>n_jobs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>oob_score<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
bag_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
y_pred<span class="token operator">=</span>bag_clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>
accuracy_score<span class="token punctuation">(</span>Y_test<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="34__77"></a>3.4 随机补丁和随机子空间</h3> 
<blockquote> 
 <p>BaggingClassifier类也支持对特征进行采样（针对实例的特定的某个或某些特征进行采样），采用由两个超参数控制：max_features和bootstrap_features，由于是特征采样，因此每个预测器将用输入特征的子集（随机的）进行训练。</p> 
 <ol><li>随机补丁方法：对训练实例和特征都进行采样（只选取一部分实例的部分特征）</li><li>随机子空间法：保留所哟的训练实例但对特征进行抽样（选取所有实例的部分特征）</li></ol> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> BaggingClassifier
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier
X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> Y_train<span class="token punctuation">,</span> Y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>  random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>
bag_clf<span class="token operator">=</span>BaggingClassifier<span class="token punctuation">(</span>DecisionTreeClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_estimators<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>max_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                          bootstrap_features<span class="token operator">=</span>iris<span class="token punctuation">.</span>feature_names<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>bootstrap<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
bag_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
y_pred<span class="token operator">=</span>bag_clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>  
</code></pre> 
<h2><a id="_91"></a>四.集成学习—随机森林</h2> 
<h3><a id="41__92"></a>4.1 简介</h3> 
<blockquote> 
 <p>前面已经介绍过随机森林就是一群决策树的集成，因此我们可以将随机森林理解为一种特殊的集成学习，（决策树看我<a href="https://blog.csdn.net/qq_43456605/article/details/127608982?spm=1001.2014.3001.5501">这篇博客</a>）。由于是一种集成学习，所以随机森林应该也也有着很多的随机性，在决策树中我们通过不纯度来选择决策边界来分裂节点，但在随机森林中我们分裂节点的方法发生了些许变化，随机森林中的决策树是在随机生成的特征子集里搜索最好的特征，这样随机森林在决策树的生长上有了更多的随机性和多样性，这里用更高的偏差换取了更低的方差（偏差是算法在训练集上的错误率，方差上算法在测试集上的错误率，由于随机森林使得决策树的生长更加随机，所以它对训练数据可能拟合的效果没那么好的，导致偏差上升，但由于其随机性增强这就导致它对测试数据拟合的更好，这就是所谓的方差下降）</p> 
</blockquote> 
<h3><a id="42_API_94"></a>4.2 API使用</h3> 
<blockquote> 
 <p>这里使用的是Scikit-Learn提供的RandomForestClassifier接口</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier
rnd_clf<span class="token operator">=</span>RandomForestClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>max_leaf_nodes<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#n_jobs表示并行的意思，多余决策树之间并行运行</span>
rnd_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>上面的代码使用所有的可用的CPU内核（n_jobs=-1），训练了一个拥有500棵树的随机森林分类器（每棵树限制为最多16个椰子树节点）</p> 
</blockquote> 
<h3><a id="43__103"></a>4.3 极端随机树</h3> 
<blockquote> 
 <p>在上面我们说到随机树里单棵决策树的生长过程中每个节点的分裂时是选取的随机特征子集中最好的一个特征，如果我们在随机的特征子集中选择的不再是最好的特征而是随机选择一个，这样决策树的增长会更加的随机，这种极端随机的决策树组成的森林称为极端随机树集成。子啊scikit-Learn中使用ExtraTreesClassifier类可以创建一个极端随机树分类器，它的使用方法和RandomForestClassifier的使用方法是一样的。</p> 
</blockquote> 
<h3><a id="44__105"></a>4.4 特征重要性</h3> 
<blockquote> 
 <p>在决策树中我们使用CART剪枝训练算法来选择特征设定阈值，而被选择的节点相比于那些没有被选择的节点上更加重要的。而在随机森林中使得测量每个特征的相对重要性变得更加容易，Scikit-Learn通过查看使用该特征的树节点平均减少不纯度（为了要将表格转化为一棵树,决策树需要找出最佳结点和最佳的分枝方法,对分类树来说,衡量这个“最佳”的指标叫做“不纯度”。通常来说,不纯度越低,决策树对训练集的拟合越好）的程度来衡量该特征的重要性，也就是减少的不纯度越多就越重要。</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris
iris<span class="token operator">=</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
rnd_clf<span class="token operator">=</span>RandomForestClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>n_jobs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
rnd_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>iris<span class="token punctuation">[</span><span class="token string">"data"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>iris<span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> name <span class="token punctuation">,</span>score <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>iris<span class="token punctuation">[</span><span class="token string">"feature_names"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>rnd_clf<span class="token punctuation">.</span>feature_importances_<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span>score<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/b2/d0/0D2BCrGH_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>代码中我们通过rnd_clf.feature_importances_来获取每个特征的重要性，可以看出petal width（cm）是最重要的，并且所有特征重要性分数加起来是1（因为Scikit-learn会在训练后为每个特征自动计算该分数，然后对结果进行缩放以使得所有重要性分数之和为1）</p> 
</blockquote> 
<h2><a id="Boosting_119"></a>五.集成学习—提升法Boosting</h2> 
<h3><a id="51__120"></a>5.1 简介</h3> 
<blockquote> 
 <p>boosting是指可以将几个弱学习器结合成一个强学习器的任意集成方法。大多数提升法的总体思路是循环训练预测器，每一次都对其前序预测器做出一些改正。</p> 
</blockquote> 
<h3><a id="52_AdaBoost_123"></a>5.2 AdaBoost</h3> 
<blockquote> 
 <p>AdaBoost是目前最流行的提升法方法，它的思路也是遵循循环训练预测器，新的预测器对其前面的预测器进行纠正的方法之一就是更加关注前序欠拟合的训练实例，这样新的预测器会不断地越来越关注于难缠的问题，这就是AdaBoost技术</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/02/14/77reB6iF_o.jpg" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>当这种依序学习技术有一个重要的缺陷就是无法并行，因为每个预测器只能在前一个预测器训练完成并评估之后才能开始训练，所以在扩展方面，它的表现不如bagging和pasting方法</p> 
</blockquote> 
<h3><a id="52_AdaBoost_128"></a>5.2 AdaBoost原理解析</h3> 
<ol><li>AdaBoost算法的初始时会将每个实例权重<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           w 
          
          
          
            ( 
           
          
            i 
           
          
            ) 
           
          
         
        
       
         w^{(i)} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>最初设置为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           1 
          
         
           m 
          
         
        
       
         \frac 1 m 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，然后对第一个预测器进行训练，并根据训练集计算其加权错误率<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           r 
          
         
           1 
          
         
        
       
         r_1 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，第j个预测器的加权错误率的求法<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            r 
           
          
            j 
           
          
         
           = 
          
          
           
            
            
              ∑ 
             
             
             
               i 
              
             
               = 
              
             
               1 
              
             
            
              m 
             
            
            
            
              w 
             
             
             
               ( 
              
             
               i 
              
             
               ) 
              
             
            
           
             ( 
            
            
             
             
               y 
              
             
               ^ 
              
             
            
              j 
             
             
             
               ( 
              
             
               i 
              
             
               ) 
              
             
            
           
             ≠ 
            
            
            
              y 
             
             
             
               ( 
              
             
               i 
              
             
               ) 
              
             
            
           
             ) 
            
           
           
            
            
              ∑ 
             
             
             
               i 
              
             
               = 
              
             
               1 
              
             
            
              m 
             
            
            
            
              w 
             
             
             
               ( 
              
             
               i 
              
             
               ) 
              
             
            
           
          
         
        
          r_j=\frac {\sum_{i=1}^m w^{(i)}(\hat y_j^{(i)}\neq y^{(i)})} {\sum_{i=1}^m w^{(i)}} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.8515em; vertical-align: -1.0037em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.8478em;"><span class="" style="top: -2.3408em;"><span class="pstrut" style="height: 3.0448em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8043em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814em;"><span class="" style="top: -2.989em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.2748em;"><span class="pstrut" style="height: 3.0448em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.8478em;"><span class="pstrut" style="height: 3.0448em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8043em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.0448em;"><span class="" style="top: -2.4231em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span><span class="" style="top: -3.2198em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.413em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.0037em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></li></ol> 
<blockquote> 
 <p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            y 
           
          
            ^ 
           
          
         
           j 
          
          
          
            ( 
           
          
            i 
           
          
            ) 
           
          
         
        
       
         \hat y_j^{(i)} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.4578em; vertical-align: -0.413em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.0448em;"><span class="" style="top: -2.4231em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span><span class="" style="top: -3.2198em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.413em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是第i个实例的第j个预测器的预测</p> 
</blockquote> 
<ol start="2"><li>预测器的权重这里我们记为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           a 
          
         
           j 
          
         
        
       
         a_j 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，它的计算方法如下列公式所示。其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          β 
         
        
       
         \beta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span>是我们的学习率超参数（默认为1），可以看出我们的预测器越准确（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           r 
          
         
           j 
          
         
        
       
         r_j 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>越小）其权重就越高。</li></ol> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           a 
          
         
           j 
          
         
        
          = 
         
        
          β 
         
        
          l 
         
        
          o 
         
        
          g 
         
         
          
          
            1 
           
          
            − 
           
           
           
             r 
            
           
             j 
            
           
          
          
          
            r 
           
          
            j 
           
          
         
        
       
         a_j=\beta log \frac {1-r_j} {r_j} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.2935em; vertical-align: -0.9721em;"></span><span class="mord mathnormal" style="margin-right: 0.0197em;">βl</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.9721em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<ol start="3"><li>然后AdaBoost算法使用下面公式来更新实例的权重，从而提高了错误分类的实例的权重</li></ol> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          对于 
         
        
          i 
         
        
          = 
         
        
          1 
         
        
          , 
         
        
          2 
         
        
          , 
         
        
          . 
         
        
          . 
         
        
          . 
         
        
          . 
         
        
          . 
         
        
          , 
         
        
          m 
         
        
       
         对于i=1,2,.....,m 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord cjk_fallback">对于</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8389em; vertical-align: -0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">.....</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">m</span></span></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           w 
          
          
          
            ( 
           
          
            i 
           
          
            ) 
           
          
         
        
          ← 
         
         
         
           { 
          
          
           
            
             
              
               
               
                 w 
                
                
                
                  ( 
                 
                
                  i 
                 
                
                  ) 
                 
                
               
              
                , 
               
              
                如果 
               
               
                
                
                  y 
                 
                
                  ^ 
                 
                
               
                 j 
                
                
                
                  ( 
                 
                
                  i 
                 
                
                  ) 
                 
                
               
              
                = 
               
               
               
                 y 
                
                
                
                  ( 
                 
                
                  i 
                 
                
                  ) 
                 
                
               
              
             
            
           
           
            
             
              
               
               
                 w 
                
                
                
                  ( 
                 
                
                  i 
                 
                
                  ) 
                 
                
               
              
                e 
               
              
                x 
               
              
                p 
               
              
                ( 
               
               
               
                 a 
                
               
                 j 
                
               
              
                ) 
               
              
             
            
           
          
         
        
       
         w^{(i)}\leftarrow \begin{cases} w^{(i)},如果 \hat y_j^{(i)}=y^{(i)}\\ w^{(i)}exp(a_j)\\ \end{cases} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.938em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3em; vertical-align: -1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.7084em;"><span class="" style="top: -3.7084em;"><span class="pstrut" style="height: 3.0448em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord cjk_fallback">如果</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.0448em;"><span class="" style="top: -2.4231em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span><span class="" style="top: -3.2198em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.413em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -2.2684em;"><span class="pstrut" style="height: 3.0448em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2084em;"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span><br> 4. 然后对所有实例权重进行归一化（除以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          ∑ 
         
         
         
           i 
          
         
           = 
          
         
           1 
          
         
        
          m 
         
        
        
        
          w 
         
         
         
           ( 
          
         
           i 
          
         
           ) 
          
         
        
       
      
        \sum_{i=1}^m w^{(i)} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1877em; vertical-align: -0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8043em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>）<br> 5. 最后使用更新后的权重训练一个新的预测器，然后重复整个过程（计算新预测器的权重，更新实例权重）直到达到所需的预测器或得到完美的预测器时算法结束<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           a 
          
         
           1 
          
         
         
         
           f 
          
         
           1 
          
         
        
          + 
         
         
         
           a 
          
         
           2 
          
         
         
         
           f 
          
         
           2 
          
         
        
          + 
         
        
          . 
         
        
          . 
         
        
          . 
         
        
          + 
         
         
         
           a 
          
         
           n 
          
         
         
         
           f 
          
         
           n 
          
         
        
          = 
         
        
          F 
         
        
       
         a_1f_1+a_2f_2+...+a_nf_n=F 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord">...</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span></span></span></span></span></span></p> 
<blockquote> 
 <p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           a 
          
         
           i 
          
         
        
       
         a_i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是我们第i个预测器的权值（计算方法如上），<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           f 
          
         
           i 
          
         
        
       
         f_i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是我们第i个预测器的预测结果</p> 
</blockquote> 
<ol start="7"><li>最后进行预测，AdaBoost就是简单地计算所有预测器的预测结果，并使用预测器权重<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           a 
          
         
           j 
          
         
        
       
         a_j 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>对他们进行加权，最后得到大多数加权投票的类就是预测器给出的预测类</li></ol> 
<blockquote> 
 <p>总结：再看一下AdaBoost算法的思想，其实就是一种“前人栽树，后人乘凉”的思想，通过一次次迭代训练使得预测器越来越强大，在一次次迭代过程中，当前训练的模型只关心前一个训练的模型的错误所在，然后专注于解决这个错误，具体的Adaboost运作流程如下：</p> 
 <ol><li>初始化所有的样本的权重为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
          
            w 
           
           
           
             ( 
            
           
             i 
            
           
             ) 
            
           
          
         
           = 
          
          
          
            1 
           
          
            m 
           
          
         
        
          w^{(i)}=\frac 1 m 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></li><li>然后利用当前预测器的训练结果更新所有的样本的权重（上面的第3步），对于预测正确的样本我们权重保持不变，对于预测错误的样本我们会增加其权重</li><li>然后基于这个实例权重已经更新的训练集，我们再训练下一个弱预测器（这时会更加关注权重大的样本的，即上一次预测错误的样本的）</li><li>然后迭代上面过程</li><li>最后进行预测</li></ol> 
</blockquote> 
<h3><a id="53_AdaBoostAPI_158"></a>5.3 AdaBoost的API使用</h3> 
<p>Scikitlearn使用的是AdaBoost的多分类版本，叫做SAMME（基于多类指数损失函数的逐步添加模型），如果使用的预测器可以估算出每类的概率，可以使用SAMME的变体，SAMME.R，它和SAMME不同的是，它依赖的是类概率而不是类预测。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> AdaBoostClassifier
ada_clf<span class="token operator">=</span>AdaBoostClassifier<span class="token punctuation">(</span>DecisionTreeClassifier<span class="token punctuation">(</span>max_depth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_estimators<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>algorithm<span class="token operator">=</span><span class="token string">"SAMME.R"</span><span class="token punctuation">,</span>learning_rate<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
ada_clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>上面代码的决策树的深度为1（max_depth=1，即每棵树只有两个叶子节点），创建了200棵这样的决策树（n_estimators=200），使用的算法是SAMME.R（algorithm=“SAMME.R”），学习率<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          β 
         
        
       
         \beta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span>为0.5（learning_rate=0.5）。如果出现过拟合问题我们可以从两个方面下手：</p> 
 <ol><li>减少决策树的数量</li><li>对每个决策树进行正则化</li></ol> 
</blockquote> 
<h3><a id="54__168"></a>5.4 梯度提升</h3> 
<blockquote> 
 <p>上面上面介绍的AdaBoost算法，提升法中梯度提升算法也是使用非常广泛的，于AdaBoost的原理一样它也是逐步在集成中添加预测器，并对每一个前序预测器的错误进行改正。不同之处在于，他不像AdaBoost那样在每个迭代中调整实例的权重<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          w 
         
        
       
         w 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span></span></span></span></span>，而是让新的预测器针对前预测器的残差进行拟合。(残差在数理统计中是指实际观察值与估计值（拟合值）之间的差)</p> 
</blockquote> 
<h3><a id="55__171"></a>5.5 梯度提升原理</h3> 
<blockquote> 
 <p>下面的案例是一个简单的回归问题，来解释梯度提升算法的运作原理，使用决策树作为基础预测器，这被称为梯度树提升或者梯度提升回归树（GBRT）</p> 
</blockquote> 
<ol><li>初始：训练集上训练第一个决策树</li></ol> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier
tree_reg1<span class="token operator">=</span>DecisionTreeClassifier<span class="token punctuation">(</span>max_depth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
tree_reg1<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>决策树的深度为2</p> 
</blockquote> 
<ol start="2"><li>第二步：针对第一个预测器的残差训练第二个决策树</li></ol> 
<pre><code class="prism language-python">y2<span class="token operator">=</span>Y_train<span class="token operator">-</span>tree_reg1<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>  <span class="token comment">#计算残差</span>
tree_reg2<span class="token operator">=</span>DecisionTreeRegressor<span class="token punctuation">(</span>max_depth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
tree_reg2<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y2<span class="token punctuation">)</span>
</code></pre> 
<ol start="3"><li>第三步：将前面得到的两棵树的预测相加，对性实例进行预测</li></ol> 
<pre><code class="prism language-python">y_pred<span class="token operator">=</span><span class="token builtin">sum</span><span class="token punctuation">(</span>tree<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> tree <span class="token keyword">in</span><span class="token punctuation">(</span>tree_reg1<span class="token punctuation">,</span>tree_reg2<span class="token punctuation">)</span><span class="token punctuation">)</span>
y_pred
</code></pre> 
<blockquote> 
 <p>以上便是梯度提升的运作原理</p> 
</blockquote> 
<h3><a id="56_API_198"></a>5.6 梯度提升API使用</h3> 
<blockquote> 
 <p>scikit-learn使用的GradientBoostingRegressore类</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> GradientBoostingRegressor
gbrt<span class="token operator">=</span>GradientBoostingRegressor<span class="token punctuation">(</span>max_depth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>n_estimators<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>learning_rate<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
gbrt<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>在这里超参数learning_rate（学习率）对每棵树的贡献进行缩放，其值越小则每棵树的贡献越小，则需要更多的树来拟合训练集，这样同样最后模型的泛化效果会比较好。</p> 
</blockquote> 
<blockquote> 
 <p>总结：介绍完梯度提升我们会发现一个新的问题，就是在使用梯度提升算法时我们如何选择合适的树的数量，这里有两种思路：</p> 
 <ol><li>使用提前停止法（提前停止(英语:early stopping)是一种在使用诸如梯度下降之类的迭代优化方法时,可对抗过拟合的正则化方法）</li><li>使用XGBoost（其提供了梯度提升的优化实现）</li></ol> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6e28d3799fbd64d5a8e80a94a1fc3a45/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">在GPU上运行pytorch程序（指定单/多显卡)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8e8bc02d0ad25a74be4fd718cb9daf5d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">TODO 从流量中检测C2通信</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>