<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NLP之英文分词 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NLP之英文分词" />
<meta property="og:description" content="任务 对Excel中的摘要列的英文段落进行英文分词、词性标注，将结果导入Excel
下面展示代码`。
import nltk from nltk.tokenize import WordPunctTokenizer, sent_tokenize from nltk.corpus import stopwords from nltk.tag import pos_tag import numpy as np import pandas as pd import xlwt #英文断句分词	def wo_se_broken(sentence): #先分句，再分词 paragraph = sentence#句子与下一个句子之间要用空格隔开 sents=nltk.sent_tokenize(paragraph.lower())#sent_tokenize返回分句后的句子列表 #print(sents) words=[] for sent in sents: text_list=nltk.word_tokenize(sent)#分词 #去掉标点符号 english_punctuations=[&#39;,&#39;, &#39;.&#39;, &#39;:&#39;, &#39;;&#39;, &#39;?&#39;, &#39;(&#39;, &#39;)&#39;, &#39;[&#39;, &#39;]&#39;, &#39;&amp;&#39;, &#39;!&#39;, &#39;*&#39;, &#39;@&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;] word_1=[w for w in text_list if w not in english_punctuations] #去掉停用词(停用词均为小写形式，需先将句子转为小写形式） stops = set(stopwords." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/f83455f39aabdfc8d412c5f364241fc9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-13T19:48:08+08:00" />
<meta property="article:modified_time" content="2021-09-13T19:48:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP之英文分词</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>任务</h2> 
<p>对Excel中的摘要列的英文段落进行英文分词、词性标注，将结果导入Excel</p> 
<p>下面展示代码`。</p> 
<pre><code>import nltk
from nltk.tokenize import WordPunctTokenizer, sent_tokenize
from nltk.corpus import stopwords
from nltk.tag import pos_tag
import numpy as np
import pandas as pd
import xlwt

#英文断句分词	
def wo_se_broken(sentence):
	#先分句，再分词
	paragraph = sentence#句子与下一个句子之间要用空格隔开
	sents=nltk.sent_tokenize(paragraph.lower())#sent_tokenize返回分句后的句子列表
	#print(sents)
	words=[]
	for sent in sents:
		text_list=nltk.word_tokenize(sent)#分词
		#去掉标点符号
		english_punctuations=[',', '.', ':', ';', '?', '(', ')', '[', ']', '&amp;', '!', '*', '@', '#', '$', '%']
		word_1=[w for w in text_list if w not in english_punctuations]
		#去掉停用词(停用词均为小写形式，需先将句子转为小写形式）
		stops = set(stopwords.words("english"))#set()函数创建一个无序不重复元素集
		word_2=[w for w in word_1 if w not in stops]
		word_3=pos_tag(word_2)#词性标注
		words=words+word_3
	#print(words)
	return words
		
path='E://网页下载/20210912071950877.XLSX'
path_end='E://网页下载/0-500_processed.xls'

data = pd.read_excel(path)
df1=pd.DataFrame(data)
abstracts=df1['摘要'].values#获取摘要列数据
#print(abstracts)


#以字典形式创建DataFrame
value=[]
data={'摘要分词':value}

#摘要列断词,每行一个循环
for i in range(len(abstracts)):
	abstract=abstracts[i]
	ab_broken=wo_se_broken(abstract)
	value.append(ab_broken)
#print(data)
df2=pd.DataFrame(data,columns=['摘要分词'])
df2.to_excel(path_end,columns=['摘要分词'])
</code></pre> 
<p>一些注意的问题：<br> 1、DataFrame是一种数据结构，对于读取到的Excel数据，须先转换成DataFrame格式，才可进行操作<br> DataFrame（数据，list索引（行数），list列名）<br> 2、将结果导入Excel中时（我所得结果为断词后的列表），需先以元组方式 创建DataFrame结构，然后以DataFrame结构的结构写入Excel</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1df1e2a6a1bacd0c0a8d209b123f855e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">NIPS15 - 神经网络中的空间转换模块STN《Spatial Transformer Network》(含代码复现)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c9e605b82cbebe648c8cb17685be18d5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Andriod Studio显示Manifest merger failed with multiple errors的问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>