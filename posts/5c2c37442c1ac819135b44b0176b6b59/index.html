<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>利用MatConvNet搭建自己的网络并训练（调参） - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="利用MatConvNet搭建自己的网络并训练（调参）" />
<meta property="og:description" content="本文转载于博客：https://blog.csdn.net/qq_20259459 和 作者邮箱（ jinweizhi93@gmai.com ）。
有关调参该作者也给出了详细的介绍（https://blog.csdn.net/qq_20259459/article/details/70316511）
前期工作：下载安装matlab和下载MatConvNet以及下载GPU相关文件和配置GPU。
具体请参见我之前的文章：
1. 深度学习 2. MatConvNet（CNN）的配置和相关实验结果,CNN学习使用 ：
http://blog.csdn.net/qq_20259459/article/details/54092277
2. 深度学习 3. MatConvNet (CNN)的介绍和下载以及CPU和GPU的安装配置，Matlab2016 ：
http://blog.csdn.net/qq_20259459/article/details/54093550
准备工作：
1. 打开Matlab，配置相关文件的路径（http://blog.csdn.net/qq_20259459/article/details/54092277）
2. 输入 mex -setup cpp
3. 输入 vl_compilenn
4. 输入 compileGPU
没有报错则配置完成。
开始，新建编辑页 cnn_cifar_my ：
这是外层调参和构建imdb结构体的code。
关于调参我会在后面单取一篇来介绍。
函数相互调用顺序：主函数 function [net, info] = cnn_cifar_my(varargin) :
一、. 首先初始化网络：
opts.batchNormalization = false ; %选择batchNormalization的真假 opts.network = [] ; %初始化一个网络 opts.networkType = &#39;simplenn&#39; ; %选择网络结构 %%% simplenn %%% dagnn [opts, varargin] = vl_argparse(opts, varargin) ; %调用vl_argparse函数 sfx = opts." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/5c2c37442c1ac819135b44b0176b6b59/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-05-06T09:49:51+08:00" />
<meta property="article:modified_time" content="2019-05-06T09:49:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">利用MatConvNet搭建自己的网络并训练（调参）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本文转载于博客：<a href="https://blog.csdn.net/qq_20259459/article/details/65633407">https://blog.csdn.net/qq_20259459</a> 和 作者邮箱（ jinweizhi93@gmai.com ）。</p> 
<p>有关调参该作者也给出了详细的介绍（<a href="https://blog.csdn.net/qq_20259459/article/details/70316511">https://blog.csdn.net/qq_20259459/article/details/70316511</a>）</p> 
<p>前期工作：下载安装matlab和下载MatConvNet以及下载GPU相关文件和配置GPU。<br> 具体请参见我之前的文章：<br> 1. 深度学习 2. MatConvNet（CNN）的配置和相关实验结果,CNN学习使用 ：<br> http://blog.csdn.net/qq_20259459/article/details/54092277<br> 2. 深度学习 3. MatConvNet (CNN)的介绍和下载以及CPU和GPU的安装配置，Matlab2016 ：<br> http://blog.csdn.net/qq_20259459/article/details/54093550</p> 
<p><br> 准备工作：<br> 1. 打开Matlab，配置相关文件的路径（http://blog.csdn.net/qq_20259459/article/details/54092277）<br> 2. 输入 mex -setup cpp<br> 3. 输入 vl_compilenn<br> 4. 输入 compileGPU<br> 没有报错则配置完成。</p> 
<p><br> 开始，新建编辑页 cnn_cifar_my ：</p> 
<p>这是外层调参和构建imdb结构体的code。</p> 
<p>关于调参我会在后面单取一篇来介绍。</p> 
<p>函数相互调用顺序：主函数 function [net, info] = cnn_cifar_my(varargin) :</p> 
<p><strong>一、. 首先初始化网络：</strong></p> 
<pre class="has"><code>opts.batchNormalization = false ;                   %选择batchNormalization的真假
opts.network = [] ;                                 %初始化一个网络
opts.networkType = 'simplenn' ;                     %选择网络结构 %%% simplenn %%% dagnn
[opts, varargin] = vl_argparse(opts, varargin) ;    %调用vl_argparse函数

sfx = opts.networkType ;                                                %sfx=simplenn
if opts.batchNormalization, sfx = [sfx '-bnorm'] ; end                  %这里条件为假
opts.expDir = fullfile(vl_rootnn, 'data', ['cifar10-' sfx]) ;    %选择数据存放的路径：data\cifar-baseline-simplenn
[opts, varargin] = vl_argparse(opts, varargin) ;                        %调用vl_argparse函数

opts.dataDir = fullfile(vl_rootnn, 'data', 'cifar10') ;                   %选择数据读取的路径：data\matconvnet-1.0-beta23\data\cifar
opts.imdbPath = fullfile(opts.expDir, 'imdb.mat');                      %选择imdb结构体的路径：data\data\cifar-baseline-simplenn\imdb
opts.whitenData = true ;
opts.contrastNormalization = true ;
opts.train = struct() ;                                                 %选择训练集返回为struct型
opts = vl_argparse(opts, varargin) ;                                    %调用vl_argparse函数

%选择是否使用GPU，使用opts.train.gpus = 1，不使用：opts.train.gpus = []。
%有关GPU的安装配置请看我的博客：http://blog.csdn.net/qq_20259459/article/details/54093550
if ~isfield(opts.train, 'gpus'), opts.train.gpus = [1]; end; </code></pre> 
<p><strong>二、 调用网络结构函数 cnn_cifar_init_my (这个函数用于构造自己的网络结构) ：</strong></p> 
<pre class="has"><code>if isempty(opts.network)                                                    %如果原网络为空：
  net = cnn_cifar_init_my('batchNormalization', opts.batchNormalization, ...   %   则调用cnn_cifat_init网络结构
    'networkType', opts.networkType) ;
else                                                                        %否则：
  net = opts.network ;                                                      %   使用上面选择的数值带入现有网络
  opts.network = [] ;
end</code></pre> 
<p><strong>三、 接下来将调用得到imdb的相关函数（用于训练的数据集）:</strong></p> 
<pre class="has"><code>if exist(opts.imdbPath, 'file')                         %如果cifar中存在imdb的结构体：
  imdb = load(opts.imdbPath) ;                          %   载入imdb
else                                                    %否则：
  imdb = getMnistImdb(opts) ;                           %   调用getMnistImdb函数得到imdb并保存
  mkdir(opts.expDir) ;                                  
  save(opts.imdbPath, '-struct', 'imdb') ;
end
%arrayfun函数通过应用sprintf函数得到array中从1到10的元素并且将其数字标签转化为char文字型
net.meta.classes.name = arrayfun(@(x)sprintf('%d',x),1:10,'UniformOutput',false) ;</code></pre> 
<p><strong>四、 然后调用网络类型（simplenn,dagnn）:</strong></p> 
<pre class="has"><code>switch opts.networkType                                     %选择网络类型：
  case 'simplenn', trainfn = @cnn_train ;                   %   1.simplenn
  case 'dagnn', trainfn = @cnn_train_dag ;                  %   2.dagnn
end

%调用训练函数，开始训练：find(imdb.images.set == 3)为验证集的样本
[net, info] = trainfn(net, imdb, getBatch(opts), ...        
  'expDir', opts.expDir, ...
  net.meta.trainOpts, ...
  opts.train, ...
  'val', find(imdb.images.set == 3)) ;</code></pre> 
<p> </p> 
<p>综上所述，我们的流程是：<strong>1. 输入网络和参数的初始值。2. 构建训练网络结构。3. 建立训练数据集。4. 选择训练网络的类型</strong>。</p> 
<p> </p> 
<p>注：<br> imdb结构体：</p> 
<p>1. 这是用于cnn_train中的结构体，也就是实际训练的部分。</p> 
<p>2. 该结构体内共有4个部分，由data，label，set，class组成。</p> 
<p>            data:包含了train data和test data。</p> 
<p>             label:包含了train label和test label。</p> 
<p>            set:set的个数与label的个数是相等的，set=1表示这个数据是train data，set=3则表示这个数据是test data。   以此方法用于计算机自己判断的标准。</p> 
<p>            class:与数据中的class完全一样。</p> 
<p>3. imdb构造时遵循train在上层，test在下层的顺序。</p> 
<p>4. 相关的data需要进行泛化处理。</p> 
<p><br> 下面以我自己的数据为例构建一个自己的imdb：</p> 
<pre class="has"><code>function imdb = getMnistImdb(opts)
%% --------------------------------------------------------------
%   函数名：getMnistImdb
%   功能：  1.从mnist数据集中获取data
%           2.将得到的数据减去mean值
%           3.将处理后的数据存放如imdb结构中
% ------------------------------------------------------------------------
% Preapre the imdb structure, returns image data with mean image subtracted

load('TR.mat'); 
load('TT.mat');
load('TRL.mat');
load('TTL.mat');

x1 = TR;
x2 = TT;
y1 = TRL;
y2 = TTL;

%set = 1 对应训练；set = 3 对应的是测试
set = [ones(1,numel(y1)) 3*ones(1,numel(y2))];              %numel返回元素的总数
data = single(reshape(cat(3, x1, x2),128,256,1,[]));          %将x1的训练数据集和x2的测试数据集的第三个维度进行拼接组成新的数据集，并且转为single型减少内存
dataMean = mean(data(:,:,:,set == 1), 4);                   %求出训练数据集中所有的图像的均值
data = bsxfun(@minus, data, dataMean) ;                     %利用bsxfun函数将数据集中的每个元素逐个减去均值

%将数据存入imdb结构中
imdb.images.data = data ;                                   %data的大小为[128 256 1 70000]。 （60000+10000） 这里主要看上面的data的size。
imdb.images.data_mean = dataMean;                           %dataMean的大小为[128 256]
imdb.images.labels = cat(2, y1', y2') ;                       %拼接训练数据集和测试数据集的标签，拼接后的大小为[1 70000]
imdb.images.set = set ;                                     %set的大小为[1 70000]，unique(set) = [1 3]
imdb.meta.sets = {'train', 'val', 'test'} ;                 %imdb.meta.sets=1用于训练，imdb.meta.sets=2用于验证，imdb.meta.sets=3用于测试

%arrayfun函数通过应用sprintf函数得到array中从0到9的元素并且将其数字标签转化为char文字型
imdb.meta.classes = arrayfun(@(x)sprintf('%d',x),0:9,'uniformoutput',false) ;</code></pre> 
<p>注：<br> 1. data = single(reshape(cat(3, x1, x2),128,256,1,[]));  这里[128，256，1]是我的数据的size。如果你是三维的数据，比如是cifar则需要将这里的1变为3。且cat的3需要变为4。<br> 2. 针对三维数据切勿轻易使用reshape函数，尽可能的用cat函数组建，因为reshape是基于纵向来构造的。</p> 
<p><br> 下面我将为大家介绍如何构建自己的网络：</p> 
<p>1. Conv.layer：</p> 
<pre class="has"><code>net.layers{end+1} = struct('type', 'conv', ...          %卷积层C，randn函数产生4维标准正态分布矩阵，设置偏置有20个
                           'weights', {<!-- -->{0.05*randn(3,3,1,32, 'single'), ...
                           zeros(1, 32, 'single')}}, ...  %filter大小是3*3*1
                           'learningRate', lr, ...
                           'stride', 1, ...             %stride = 1
                           'pad', 0) ;         </code></pre> 
<p>注：<br> （1）weights既是filter。这里的3*3为filter的大小（长和宽），1是input的图片的厚度（如果图片是rgb则这里将是3），32是此层filter的个数。<br> （2）stride等于该filter的移动步伐。<br> （3）当filter的size等于1*1的时候，表示为fully connection。</p> 
<p>2. Rule.layer:</p> 
<pre class="has"><code>net.layers{end+1} = struct('type', 'relu') ; </code></pre> 
<p>3. maxPooling.layer:</p> 
<pre class="has"><code>net.layers{end+1} = struct('type', 'pool', ...          %池化层P
                           'method', 'max', ...
                           'pool', [2 2], ...           %池化核大小为2*2
                           'stride', 2, ...
                           'pad', 0) ;</code></pre> 
<p>4. dropout.layer:</p> 
<pre class="has"><code>net.layers{end+1} = struct('type', 'dropout', 'name', 'dropout2', 'rate', 0.5) ;</code></pre> 
<p>注：这里我们给的drop rate 是0.5 。</p> 
<p>5. softmax.layer:</p> 
<pre class="has"><code>net.layers{end+1} = struct('type', 'softmaxloss') ;     %softmax层</code></pre> 
<p><br> 下面我将说明网络构建的思路：<br> 1. 一般来说作为最原始的lenet的网络结构，我们最好的构造是C-R-C-R-P为一个block。<br> 2. C层之后一定要加上R层，这是构建原理之一，linear的C加上nonlinear的R,相信学过NN的或者能用的这个人都会知道吧。<br> 3. softmax的input必须为1*1的size。为了实现这个就必须计算整体网络的构造，我的建议是画图，自己先在纸上画好自己的网络结构，计算好最后为1*1。<br> 4. 关于图片缩小的计算公式：<br>       （1）Conv.layer: [(N-F）/stride]+1<br>                这里N是input的size，F是filter的size。<br>       （2）Pooling.layer: 一般说来pooling层是不用改变的，都是缩小2分之1。<br> 5. 如果data的size过小，而希望增加C层来进行深度的网络构造，那么我们就需要用到padding。<br> 公式：padding size = (F-stride)/2 这里F是filter的size。这样我们的C层就不会减小图片，从而进行构造深度网络。</p> 
<p><br> 整体思路：<br> 我们训练的过程中用到的无非就三个大的步骤：得到数据，训练，结果分析。<br> 数据：<br> 1.得到好的且可以训练的数据是很重要的，许多时候不能收敛的情况都是坏的或者错误的数据处理操作导致的。因为我们大部分用的是现有的数据库，所以我们在下载数据库之后需要了解清楚数据库的构成，比如Cifar-10和-100的数据库就是条状的，需要进行相关的处理。这个我们前面也说过。<br> 2.数据的分成一般来说是train data =70% validation data =15% test data =15%. <br> 3.数据输入正确以后，我们需要进行相关的normalization的处理。（这部非常的重要）<br> 4.正则化处理结束之后，我们就可以将相关的data，label, class，class name都放入imdb结构体中，imdb我在前一篇说过了，这才是训练的本体结构。<br> 训练：<br> 1.如果我们确认input的数据是对的，那么我们可以开始我们的训练了。<br> 2.构建训练的网络。网络的构建或者是网络中某一层的创建都是现在深度学习论文的大热，好的网络结构的提出也是论文的大热。所以我们需要不断的训练，不断的改参数，才能自己找到更好的网络结构。在之前的文章中我一直都是用的是LeNet的网络结构和vgg的网络结构。因为这些都是最基础的网络构型，其他的比如NIN这种后代网络都是衍生体。所以我还是会使用LeNet的网络结构来介绍。如果大家有兴趣的话可以阅读相关的paper。<br> 3. 我们知道最最常用的网络单元有：Convolution, ReLu, Pooling(max,average）, Dropout., Fully connection, Softmax.<br> 4. 对于LeNet中，我们习惯用C-R-P的block。而在vgg中提出了小的filter组合提升性能的想法：C-R-C-R-P的block。<br> 5.Dropout一般来说我们使用在FC层之前的，不过在最新的论文中也有人提出将D层用在前面的网络中减轻参数。这里我还是提倡大家不要轻易多增加D层，只要在FC前添加即可，rate一般为0.5<br> 6. FC和S层没有什么好说的，不过在之前GoogleNet的网络中已经去除了FC，从而大大的降低了参数。不过这里GN只是一个特列我们不要轻易模仿。<br> 7.我个人最最提倡的还是vgg的构型，我个人的训练中vgg的构型还是效果非常好的。<br> 8.整个网络我们为的就是得到有效的features。<br> 9. FC层的input的大小需为1*1，因此我这里建议正方形的数据库，个人尝试过长方形，可以通过改变filter最终得到1*1的FC的input。但是效果非常不好，我也不能一定说长方形不好，但是还是再次强调大家的数据库的大小最好是为正方形。（这也是目前大家所默认的）<br> 10. 为了得到FC的input是1*1的大小，我们就需要巧妙的用C层和P层来减小原始输入数据的大小。这里的两个公式是需要大家记好的：<br> 关于图片缩小的计算公式：<br> （1）Conv.layer:   [(N-F）/stride]+1   这里N是input的size，F是filter的size。<br> （2）如果data的size过小，而希望增加C层来进行深度的网络构造，那么我们就需要用到padding。 公式：padding size = (F-stride)/2 这里F是filter的size。这样我们的C层就不会减小图片，从而进行构造深度网络。<br> （3）MaxPooling: output = [(N-F）/stride]+1 这里N是input的size，F是filter的size。 但是一般来说我们使用 F=2，S=2的结构，也就是缩小一半。这里的意思就是我们取每一个2*2的区域内最大值（feature最为突出的值）组成新的feature maps。</p> 
<p><br><br> 原文：https://blog.csdn.net/qq_20259459/article/details/65633407 </p> 
<p>原文：https://blog.csdn.net/qq_20259459/article/details/69526065 </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/09a17ba19857c8bcc1da2015c1385521/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">经典CNN模型特点总结</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/36796b3bd8edc6d012b6dc451b19a283/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">&#34;GET /favicon.ico HTTP/1.1&#34; 404</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>