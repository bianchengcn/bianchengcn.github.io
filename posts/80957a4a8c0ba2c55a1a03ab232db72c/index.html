<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【CNN】经典网络LeNet——最早发布的卷积神经网络之一 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【CNN】经典网络LeNet——最早发布的卷积神经网络之一" />
<meta property="og:description" content="前言 LeNet是Yann LeCun于1988年提出的用于数字识别的网络结构，可以说LeNet是深度CNN网络的基石，AlexNet、VGG、GoogLeNet、ResNet等都是在VGG基础上加入各类激活函数或加深网络演变而来的，所以理解LeNet对于现在主流CNN深度学习架构的理解有很大帮助。
关于LeNet详细的介绍可以阅读，《Gradient-Based Learning Applied to Document Recognition》，对LeNet的架构做了详细的介绍，并对LeNet与其他算法做了详细的对比。并且这是第一篇通过反向传播成功训练卷积神经网络的研究。
一，介绍 LeNet主要的出现契机是手写数字的识别，并在邮政和银行发挥了非常重要的角色。但是，这个网络在当时流行度没那么高，但是知名度最高的还是MNIST数据集。
所有的都是黑白图。
对于LeNet,总体来看，LeNet（LeNet-5）由两个部分组成：
卷积编码层：由两个卷积层组成全连接密集块：由是哪个全连接层组成 架构图如下图所示：
输入的是28 * 28的单通道图片 得到6输出通道的28 * 28的feature map, 通过池化层,得到 6通道 14 * 14的特征图，最后通过卷积操作得到16 输出通道的特征图，在通过池化得到16通道的5 * 5特征图，最后使用三个全连接层，拉成10通道输出，得到0 ~ 9的数字识别结果。
还有一些超参数：
到c1 是: kernel_size = 5,padding = 2.到s2 是:kernel_size = 5,stride = 2到c3 是:kernel_size = 5到s4 是: kernel_size = 2,stride = 2 卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用卷积核和一个sigmoid激活函数去代替。
对于LeNet是早期成功的神经网络，先使用卷积层来学习图片的空间信息，然后使用全连接层来转换到别的空间。
这个思想，影响了早期神经网络的训练模式，现在几乎不这样。
二，代码实现 按照卷积的计算公式和上面的超参数，通过卷积的输出计算公式搭建网络:
2.1 搭建网络 导入所需要的包
import torch from torch import nn import torchvision from torchvision import transforms from torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/80957a4a8c0ba2c55a1a03ab232db72c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-26T21:54:03+08:00" />
<meta property="article:modified_time" content="2022-11-26T21:54:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【CNN】经典网络LeNet——最早发布的卷积神经网络之一</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>前言</h2> 
<p>LeNet是Yann LeCun于1988年提出的用于数字识别的网络结构，可以说LeNet是深度CNN网络的基石，AlexNet、VGG、GoogLeNet、ResNet等都是在VGG基础上加入各类激活函数或加深网络演变而来的，所以理解LeNet对于现在主流CNN深度学习架构的理解有很大帮助。</p> 
<p>关于LeNet详细的介绍可以阅读，<a href="http://lushuangning.oss-cn-beijing.aliyuncs.com/CNN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/Gradient-Based_Learning_Applied_to_Document_Recognition.pdf" rel="nofollow">《Gradient-Based Learning Applied to Document Recognition》</a>，对LeNet的架构做了详细的介绍，并对LeNet与其他算法做了详细的对比。并且这是第一篇通过<strong>反向传播成功训练卷积神经网络</strong>的研究。</p> 
<h2><a id="_6"></a>一，介绍</h2> 
<p><code>LeNet</code>主要的出现契机是<strong>手写数字的识别</strong>，并在邮政和银行发挥了非常重要的角色。但是，这个网络在当时流行度没那么高，但是知名度最高的还是<a href="http://yann.lecun.com/exdb/mnist/" rel="nofollow">MNIST数据集</a>。<br> <img src="https://images2.imgbox.com/1d/a6/DueiVvLu_o.png" alt="在这里插入图片描述"><br> 所有的都是黑白图。<br> 对于<code>LeNet</code>,总体来看，<code>LeNet（LeNet-5）</code>由两个部分组成：</p> 
<ul><li>卷积编码层：由两个卷积层组成</li><li>全连接密集块：由是哪个全连接层组成</li></ul> 
<p>架构图如下图所示：<br> <img src="https://images2.imgbox.com/3b/3d/aNzs3KIT_o.png" alt="在这里插入图片描述"><br> 输入的是<code>28 * 28</code>的单通道图片 得到<code>6</code>输出通道的<code>28 * 28</code>的<code>feature map</code>, 通过池化层,得到 <code>6</code>通道 <code>14 * 14</code>的特征图，最后通过卷积操作得到<code>16 </code>输出通道的特征图，在通过池化得到<code>16</code>通道的<code>5 * 5</code>特征图，最后使用三个全连接层，拉成<code>10</code>通道输出，得到<code>0 ~ 9</code>的数字识别结果。<br> 还有一些超参数：</p> 
<ol><li>到<code>c1</code> 是: <code>kernel_size</code> = 5,<code>padding</code> = 2.</li><li>到<code>s2 </code>是:<code>kernel_size</code> = 5,<code>stride </code>= 2</li><li>到<code>c3 </code>是:<code>kernel_size </code>= 5</li><li>到<code>s4 </code>是: <code>kernel_size</code> = 2,<code>stride</code> = 2</li></ol> 
<p>卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层，虽然<code>ReLU</code>和<code>最大汇聚层</code>更有效，但它们在20世纪90年代还没有出现。每个卷积层使用卷积核和一个<code>sigmoid</code>激活函数去代替。</p> 
<p>对于<code>LeNet</code>是早期成功的神经网络，<strong>先使用卷积层来学习图片的空间信息，然后使用全连接层来转换到别的空间</strong>。<br> 这个思想，影响了早期神经网络的训练模式，现在几乎不这样。</p> 
<h2><a id="_28"></a>二，代码实现</h2> 
<p>按照卷积的计算公式和上面的超参数，通过卷积的输出计算公式搭建网络:<br> <img src="https://images2.imgbox.com/a8/bb/vildYnKf_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="21__31"></a>2.1 搭建网络</h3> 
<p>导入所需要的包</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
</code></pre> 
<p>按照上图图示搭建网络结构</p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>搭建好网络之后，我们需要通过大小为的单通道（黑白）图像通过<code>LeNet</code>。通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的。</p> 
<pre><code class="prism language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span> <span class="token string">'output shape: \t'</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/8c/7d/kfTR6mFN_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="22__62"></a>2.2 模型的训练</h3> 
<p>现在我们已经实现了LeNet,让我们来看看在Fashion-MNIST数据集上的表现。<br> 先读取对应的数据集：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">load_data_fashion_mnist</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> resize<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Download the Fashion-MNIST dataset and then load it into memory.
    Defined in :numref:`sec_fashion_mnist`"""</span>
    trans <span class="token operator">=</span> <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">if</span> resize<span class="token punctuation">:</span>
        trans<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span>resize<span class="token punctuation">)</span><span class="token punctuation">)</span>
    trans <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>trans<span class="token punctuation">)</span>
    mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
        root<span class="token operator">=</span><span class="token string">"../data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
        root<span class="token operator">=</span><span class="token string">"../data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                            num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>mnist_test<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                            num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>设置训练的小批量大小</p> 
<pre><code class="prism language-python">batch_size <span class="token operator">=</span> <span class="token number">256</span>
train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> load_data_fashion_mnist<span class="token punctuation">(</span>batch_size <span class="token operator">=</span> batch_size<span class="token punctuation">)</span>
</code></pre> 
<p>设置超参数和epoches的数量</p> 
<pre><code class="prism language-python">device <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-3</span><span class="token punctuation">)</span>
<span class="token comment"># 损失函数</span>
loss_func <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
epoches <span class="token operator">=</span> <span class="token number">120</span>
costs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
</code></pre> 
<p>开始训练</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sum_loss <span class="token operator">=</span> <span class="token number">0</span>
    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span>batch_y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 如果是存在gpu的话，直接使用gpu来训练</span>
        <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            batch_x <span class="token operator">=</span> batch_x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            batch_y <span class="token operator">=</span> batch_y<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 梯度清零</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> net<span class="token punctuation">(</span>batch_x<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> step <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            costs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
            sum_loss <span class="token operator">+=</span> loss
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">,mini_batch:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">,mini_loss:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>sum_loss <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
            sum_loss <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token comment"># 验证</span>
    net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    correct <span class="token operator">=</span> <span class="token number">0.0</span>
    total <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span>test_x<span class="token punctuation">,</span> test_y<span class="token punctuation">)</span> <span class="token keyword">in</span> test_iter<span class="token punctuation">:</span>
        <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            test_x <span class="token operator">=</span> test_x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            test_y <span class="token operator">=</span> test_y<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        test_output <span class="token operator">=</span> net<span class="token punctuation">(</span>test_x<span class="token punctuation">)</span>
        <span class="token comment"># 只返回最大数的那个索引</span>
        predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>test_output<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token comment">#         计算总数</span>
        total <span class="token operator">+=</span> test_y<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment">#     计算预测的正确数目</span>
        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> test_y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'correct:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>correct<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'total:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>total<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Test acc:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token punctuation">(</span>correct <span class="token operator">/</span> total <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>绘制损失图与得到最终结论：</p> 
<pre><code class="prism language-python"><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    costs <span class="token operator">=</span> <span class="token punctuation">[</span>cost<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> cost <span class="token keyword">in</span> costs<span class="token punctuation">]</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    costs <span class="token operator">=</span> <span class="token punctuation">[</span>cost<span class="token punctuation">.</span>numpy <span class="token keyword">for</span> cost <span class="token keyword">in</span> costs<span class="token punctuation">]</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>costs<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'number of iteration'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'loss in train'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'LeNet'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>最后得到的loss图示：<br> <img src="https://images2.imgbox.com/59/a7/chYbLiLt_o.png" alt="在这里插入图片描述"><br> 与一般的验证精度：<br> <img src="https://images2.imgbox.com/bf/72/BjrATNFl_o.png" alt="在这里插入图片描述"><br> 总体来说表现良好。</p> 
<h2><a id="_154"></a>三，总结</h2> 
<ul><li> <p>卷积神经网络（CNN）是一类使用卷积层的网络。</p> </li><li> <p>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。</p> </li><li> <p>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</p> </li><li> <p>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</p> </li><li> <p>LeNet是最早发布的卷积神经网络之一。</p> </li></ul> 
<hr> 
<p>参考：<br> <a href="https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/lenet.html" rel="nofollow">https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/lenet.html</a><br> <a href="https://blog.csdn.net/qq_43960768/article/details/124652618?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-124652618-blog-122780852.pc_relevant_multi_platform_whitelistv4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-124652618-blog-122780852.pc_relevant_multi_platform_whitelistv4&amp;utm_relevant_index=5">https://blog.csdn.net/qq_43960768/article/details/124652618?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-124652618-blog-122780852.pc_relevant_multi_platform_whitelistv4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-124652618-blog-122780852.pc_relevant_multi_platform_whitelistv4&amp;utm_relevant_index=5</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5214ff3c1e66b4061ff1a016185debe9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">用auto.js写了一个抖音点赞、关注的脚本</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/97e6b44967c382339f67936d3f9a5ec1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">[Vue warn]: Error in render: “TypeError: Cannot read properties of undefined (reading ‘matched‘)“</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>