<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【实战】Python爬虫之代理使用详解 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【实战】Python爬虫之代理使用详解" />
<meta property="og:description" content="在Python爬虫中，代理的使用非常常见。代理的主要作用是隐藏客户端的真实IP地址，从而实现更高的网络访问速度和更好的访问隐私保护。下面我们将通过Python爬虫的实例，带你详细了解Python爬虫中代理的使用方法。
目录
## 1. 代理原理和作用
## 2. Python爬虫代理的使用方式
## 3. 代理IP的获取
## 4. 多线程和多进程使用代理
## 5. 请求头的设置
总结
## 1. 代理原理和作用 代理是一种中间层服务器，在客户端和目标服务器之间传送请求和响应。代理可以缓存请求结果，从而大大减少网络请求的次数，也可以隐藏客户端真实IP地址，避免被目标服务器识别。
代理主要有以下作用：
- 隐藏客户端的真实IP地址，防止被服务器识别。
- 帮助访问本地系统无法访问的服务器。
- 缓存目标服务器的结果，避免重复请求浪费资源。
- 通过代理负载均衡优化网络请求的响应速度。
## 2. Python爬虫代理的使用方式 Python爬虫代理的使用，可以通过更改HTTP请求头信息或通过某些库辅助实现。例如，urllib和requests库中都已经提供了代理IP相关的设置方法。urllib库的代理IP设置可以通过创建代理处理器（proxy handler）实现：
import urllib.request proxy_handler = urllib.request.ProxyHandler({&#39;http&#39;: &#39;http://127.0.0.1:8000&#39;}) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) response = urllib.request.urlopen(&#39;http://httpbin.org/ip&#39;) print(response.read().decode()) 代码中，即通过proxy_handler设置http代理进行访问。可以将这个代理handler作为参数传入build_opener创建一个opener，再通过urllib.request.install_opener()方法将opener设置为默认opener，最终通过response读取url对应的数据。如果proxy_handler、opener、install_opener方法都不清楚的话，可参阅Python标准库文档。
同样requests库中提供的代理IP设置代码如下：
import requests proxies = { &#34;http&#34;: &#34;http://127.0.0.1:8000&#34;, &#34;https&#34;: &#34;http://127.0.0.1:8000&#34;, } response = requests.get(&#39;http://httpbin.org/ip&#39;,proxies=proxies) print(response.content.decode()) 其中，在requests库中可以通过proxies参数设置代理IP，实现对目标网站的访问。
## 3. 代理IP的获取 在使用Python爬虫代理的过程中，要获取可用的代理IP非常关键。使用免费代理IP时，需要注意代理IP的质量和失效率，避免使用低质量的代理IP而导致爬虫失败或者被封禁。这里推荐站大爷代理IP供大家参考。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/56c73cba8d4d05ff67f564b17934fd1e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-16T15:34:48+08:00" />
<meta property="article:modified_time" content="2023-06-16T15:34:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【实战】Python爬虫之代理使用详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在Python爬虫中，代理的使用非常常见。代理的主要作用是隐藏客户端的真实IP地址，从而实现更高的网络访问速度和更好的访问隐私保护。下面我们将通过Python爬虫的实例，带你详细了解Python爬虫中代理的使用方法。</p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%23%23%201.%20%E4%BB%A3%E7%90%86%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%9C%E7%94%A8-toc" style="margin-left:0px;"><a href="#%23%23%201.%20%E4%BB%A3%E7%90%86%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%9C%E7%94%A8" rel="nofollow">## 1. 代理原理和作用</a></p> 
<p id="%23%23%202.%20Python%E7%88%AC%E8%99%AB%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F-toc" style="margin-left:0px;"><a href="#%23%23%202.%20Python%E7%88%AC%E8%99%AB%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F" rel="nofollow">## 2. Python爬虫代理的使用方式</a></p> 
<p id="%23%23%203.%20%E4%BB%A3%E7%90%86IP%E7%9A%84%E8%8E%B7%E5%8F%96-toc" style="margin-left:0px;"><a href="#%23%23%203.%20%E4%BB%A3%E7%90%86IP%E7%9A%84%E8%8E%B7%E5%8F%96" rel="nofollow">## 3. 代理IP的获取</a></p> 
<p id="%23%23%204.%20%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86-toc" style="margin-left:0px;"><a href="#%23%23%204.%20%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86" rel="nofollow">## 4. 多线程和多进程使用代理</a></p> 
<p id="%23%23%205.%20%E8%AF%B7%E6%B1%82%E5%A4%B4%E7%9A%84%E8%AE%BE%E7%BD%AE-toc" style="margin-left:0px;"><a href="#%23%23%205.%20%E8%AF%B7%E6%B1%82%E5%A4%B4%E7%9A%84%E8%AE%BE%E7%BD%AE" rel="nofollow">## 5. 请求头的设置</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p></p> 
<h2 id="%23%23%201.%20%E4%BB%A3%E7%90%86%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%9C%E7%94%A8">## 1. 代理原理和作用</h2> 
<p>代理是一种中间层服务器，在客户端和目标服务器之间传送请求和响应。代理可以缓存请求结果，从而大大减少网络请求的次数，也可以隐藏客户端真实IP地址，避免被目标服务器识别。</p> 
<p style="text-align:center;"><img alt="" height="264" src="https://images2.imgbox.com/6f/34/DJxH9OsU_o.jpg" width="600"></p> 
<p> </p> 
<p>代理主要有以下作用：</p> 
<p>- 隐藏客户端的真实IP地址，防止被服务器识别。<br> - 帮助访问本地系统无法访问的服务器。<br> - 缓存目标服务器的结果，避免重复请求浪费资源。<br> - 通过代理负载均衡优化网络请求的响应速度。</p> 
<h2 id="%23%23%202.%20Python%E7%88%AC%E8%99%AB%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F">## 2. Python爬虫代理的使用方式</h2> 
<p>Python爬虫代理的使用，可以通过更改HTTP请求头信息或通过某些库辅助实现。例如，urllib和requests库中都已经提供了代理IP相关的设置方法。urllib库的代理IP设置可以通过创建代理处理器（proxy handler）实现：</p> 
<pre><code class="language-python">import urllib.request

proxy_handler = urllib.request.ProxyHandler({'http': 'http://127.0.0.1:8000'})
opener = urllib.request.build_opener(proxy_handler)
urllib.request.install_opener(opener)

response = urllib.request.urlopen('http://httpbin.org/ip')
print(response.read().decode())</code></pre> 
<p>代码中，即通过proxy_handler设置http代理进行访问。可以将这个代理handler作为参数传入build_opener创建一个opener，再通过urllib.request.install_opener()方法将opener设置为默认opener，最终通过response读取url对应的数据。如果proxy_handler、opener、install_opener方法都不清楚的话，可参阅Python标准库文档。</p> 
<p>同样requests库中提供的代理IP设置代码如下：</p> 
<pre><code class="language-python">import requests

proxies = {
    "http": "http://127.0.0.1:8000",
    "https": "http://127.0.0.1:8000",
}
response = requests.get('http://httpbin.org/ip',proxies=proxies)
print(response.content.decode())</code></pre> 
<p>其中，在requests库中可以通过proxies参数设置代理IP，实现对目标网站的访问。</p> 
<h2 id="%23%23%203.%20%E4%BB%A3%E7%90%86IP%E7%9A%84%E8%8E%B7%E5%8F%96">## 3. 代理IP的获取</h2> 
<p>在使用Python爬虫代理的过程中，要获取可用的代理IP非常关键。使用免费代理IP时，需要注意代理IP的质量和失效率，避免使用低质量的代理IP而导致爬虫失败或者被封禁。这里推荐站大爷代理IP供大家参考。</p> 
<p style="text-align:center;"><img alt="" height="246" src="https://images2.imgbox.com/bf/61/0NpfjcHq_o.jpg" width="500"></p> 
<p> </p> 
<h2 id="%23%23%204.%20%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86">## 4. 多线程和多进程使用代理</h2> 
<p>在Python中，可以使用多线程和多进程技术，实现同时使用多个代理IP的效果，从而进一步提高爬取效率。其中，多线程技术可以使用threading库，多进程技术可以使用multiprocessing库。为了在爬虫中使用多个代理IP，可以将代理列表在多个线程及进程中共享，并对其进行有效地管理。</p> 
<p>以下是使用多线程同时使用多个代理IP的示例代码：</p> 
<pre><code class="language-python">import requests
import threading

proxies = ["http://127.0.0.1:8000", "http://127.0.0.1:8001", "http://127.0.0.1:8002", "http://127.0.0.1:8003"]
lock = threading.Lock()

def request_data(url, proxy):
    with requests.session() as s:
        s.proxies = {'http': proxy}
        response = s.get(url=url)
        print(response.text)

def main():
    url = "http://httpbin.org/ip"
    threads = []
    for proxy in proxies:
        thread = threading.Thread(target=request_data, args=(url, proxy))
        thread.start()
        threads.append(thread)

    for thread in threads:
        thread.join()

if __name__ == '__main__':
    main()</code></pre> 
<p>上述代码中，首先定义了一个代理列表`proxies`，然后定义了一个`request_data`函数用于进行爬取数据。利用每个线程使用不同的代理对目标网站进行访问，最终将多个线程的结果显示出来。其中，通过使用`threading.Lock()` 来进行线程锁，防止线程之间的代理混乱。</p> 
<h2 id="%23%23%205.%20%E8%AF%B7%E6%B1%82%E5%A4%B4%E7%9A%84%E8%AE%BE%E7%BD%AE">## 5. 请求头的设置</h2> 
<p>在Python爬虫的实际应用中，有许多网站通过检查HTTP请求头的信息来判断是否为爬虫程序。针对这种情况，我们需要设置一些自定义的HTTP请求头信息，以掩盖采集的真实性质。</p> 
<p>可以通过requests库中的headers参数，以及urllib库中的Request对象来进行设置请求头信息。否则，操作系统或库默认的请求头信息会尝试插入到请求头中。<br> 以下是通过requests库的headers参数来设置请求头的示例代码：</p> 
<pre><code class="language-python">import requests

headers = {
    "Host": "httpbin.org",
    "Connection": "keep-alive",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "User-Agent": "Mozilla/5.0 (Windows NT x.y; rv:10.0) Gecko/20100101 Firefox/10.0",
    "Accept-Encoding": "gzip, deflate",
    "Accept-Language": "en-us,en;q=0.5",
    "DNT": "1"}

proxies = {"http": "http://127.0.0.1:8000"}

response = requests.get(url, headers=headers, proxies=proxies)</code></pre> 
<p>上面代码中，通过headers参数设置了一个自定义的HTTP请求头信息，其中包括了用户代理、连接类型、通信协议等信息。这些信息可以根据具体的目标网站情况适当进行更改。同时，同样通过proxies设置代理IP，实现对目标网站的优质访问。</p> 
<h2 id="%E6%80%BB%E7%BB%93">总结</h2> 
<p>Python爬虫中代理的使用需要注意代理IP的质量和失效率，动态切换代理IP，多线程和多进程共享代理IP，以及设置HTTP请求头信息等方面。通过这些方法有效地优化爬虫，可以实现高效地爬取目标页面数据的目的。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a98059a62c68cd1a09328b96f4ed40c1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">前端通过post请求上传图片到oss,使用服务端签名直传</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c850a5067cb0d3c7d321ffd97c41a130/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">oneforall安装教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>