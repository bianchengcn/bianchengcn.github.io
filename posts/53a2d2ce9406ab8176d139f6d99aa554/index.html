<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【LLM多模态】Cogview3、DALL-E3、CogVLM、CogVideo模型 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【LLM多模态】Cogview3、DALL-E3、CogVLM、CogVideo模型" />
<meta property="og:description" content="note 文章目录 noteVisualGLM-6B模型图生文：CogVLM-17B模型1. 模型架构2. 模型效果 文生图：CogView3模型DALL-E3模型CogVideo模型网易伏羲-丹青模型Reference VisualGLM-6B模型 VisualGLM 是一个依赖于具体语言模型的多模态模型，而CogVLM则是一个更广阔的系列，不仅有基于GLM的双语模型，也有基于Llama2系列的英文模型。这次开源的 17B 模型就是基于Vicuna-7B 的英文模型。
图生文：CogVLM-17B模型 多模态模型CogVLM-17B（开源）：
Github：https://github.com/THUDM/CogVLM
Huggingface：https://huggingface.co/THUDM/CogVLM
魔搭社区：https://www.modelscope.cn/models/ZhipuAI/CogVLM
Paper：https://github.com/THUDM/CogVLM/blob/main/assets/cogvlm-paper.pdf
1. 模型架构 思想：视觉优先
之前的多模态模型：通常都是将图像特征直接对齐到文本特征的输入空间去，并且图像特征的编码器通常规模较小，这种情况下图像可以看成是文本的“附庸”，效果自然有限。
模型共包含四个基本组件：ViT 编码器，MLP 适配器，预训练大语言模型（GPT-style）和视觉专家模块。
ViT编码器：在 CogVLM-17B 中，采用预训练的 EVA2-CLIP-E。MLP 适配器：MLP 适配器是一个两层的 MLP（SwiGLU），用于将 ViT 的输出映射到与词嵌入的文本特征相同的空间。预训练大语言模型：CogVLM 的模型设计与任何现有的 GPT-style的预训练大语言模型兼容。具体来说，CogVLM-17B 采用 Vicuna-7B-v1.5 进行进一步训练；也选择了 GLM 系列模型和 Llama 系列模型做了相应的训练。视觉专家模块：在每层添加一个视觉专家模块，以实现深度的视觉 - 语言特征对齐。具体来说，每层视觉专家模块由一个 QKV 矩阵和一个 MLP 组成。 训练方式：
模型在15亿张图文对上预训练了4096个A100*days，并在构造的视觉定位（visual grounding）数据集上进行二阶段预训练。在对齐阶段，CogVLM使用了各类公开的问答对和私有数据集进行监督微调，使得模型能回答各种不同类型的提问。 2. 模型效果 CogVLM 可以在不牺牲任何 NLP 任务性能的情况下，实现视觉语言特征的深度融合。训练的 CogVLM-17B 是目前多模态权威学术榜单上综合成绩第一的模型，在14个数据集上取得了state-of-the-art或者第二名的成绩。这些基准大致分为三类（共 14 个），包括图像字幕（Image Captioning）、视觉问答（Visual QA）、视觉定位（Visual Grounding）。
文生图：CogView3模型 链接：https://github.com/THUDM/CogView
DALL-E3模型 论文：https://cdn.openai.com/papers/dall-e-3.pdf" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/53a2d2ce9406ab8176d139f6d99aa554/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-30T22:57:32+08:00" />
<meta property="article:modified_time" content="2024-01-30T22:57:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【LLM多模态】Cogview3、DALL-E3、CogVLM、CogVideo模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="note_0"></a>note</h2> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#note_0" rel="nofollow">note</a></li><li><a href="#VisualGLM6B_3" rel="nofollow">VisualGLM-6B模型</a></li><li><a href="#CogVLM17B_6" rel="nofollow">图生文：CogVLM-17B模型</a></li><li><ul><li><a href="#1__15" rel="nofollow">1. 模型架构</a></li><li><a href="#2__29" rel="nofollow">2. 模型效果</a></li></ul> 
  </li><li><a href="#CogView3_36" rel="nofollow">文生图：CogView3模型</a></li><li><a href="#DALLE3_40" rel="nofollow">DALL-E3模型</a></li><li><a href="#CogVideo_42" rel="nofollow">CogVideo模型</a></li><li><a href="#_53" rel="nofollow">网易伏羲-丹青模型</a></li><li><a href="#Reference_56" rel="nofollow">Reference</a></li></ul> 
</div> 
<p></p> 
<h2><a id="VisualGLM6B_3"></a>VisualGLM-6B模型</h2> 
<p>VisualGLM 是一个依赖于具体语言模型的多模态模型，而CogVLM则是一个更广阔的系列，不仅有基于GLM的双语模型，也有基于Llama2系列的英文模型。这次开源的 17B 模型就是基于Vicuna-7B 的英文模型。</p> 
<h2><a id="CogVLM17B_6"></a>图生文：CogVLM-17B模型</h2> 
<p>多模态模型CogVLM-17B（开源）：<br> Github：https://github.com/THUDM/CogVLM<br> Huggingface：https://huggingface.co/THUDM/CogVLM<br> 魔搭社区：https://www.modelscope.cn/models/ZhipuAI/CogVLM<br> Paper：https://github.com/THUDM/CogVLM/blob/main/assets/cogvlm-paper.pdf</p> 
<h3><a id="1__15"></a>1. 模型架构</h3> 
<p>思想：视觉优先<br> 之前的多模态模型：通常都是将图像特征直接对齐到文本特征的输入空间去，并且图像特征的编码器通常规模较小，这种情况下图像可以看成是文本的“附庸”，效果自然有限。<br> <img src="https://images2.imgbox.com/5b/e6/F0xmcV7J_o.png" alt="在这里插入图片描述"><br> 模型共包含四个基本组件：ViT 编码器，MLP 适配器，预训练大语言模型（GPT-style）和视觉专家模块。</p> 
<ul><li>ViT编码器：在 CogVLM-17B 中，采用预训练的 EVA2-CLIP-E。</li><li>MLP 适配器：MLP 适配器是一个两层的 MLP（SwiGLU），用于将 ViT 的输出映射到与词嵌入的文本特征相同的空间。</li><li>预训练大语言模型：CogVLM 的模型设计与任何现有的 GPT-style的预训练大语言模型兼容。具体来说，CogVLM-17B 采用 Vicuna-7B-v1.5 进行进一步训练；也选择了 GLM 系列模型和 Llama 系列模型做了相应的训练。</li><li>视觉专家模块：在每层添加一个视觉专家模块，以实现深度的视觉 - 语言特征对齐。具体来说，每层视觉专家模块由一个 QKV 矩阵和一个 MLP 组成。</li></ul> 
<p>训练方式：</p> 
<ul><li>模型在15亿张图文对上预训练了4096个A100*days，并在构造的视觉定位（visual grounding）数据集上进行二阶段预训练。</li><li>在对齐阶段，CogVLM使用了各类公开的问答对和私有数据集进行监督微调，使得模型能回答各种不同类型的提问。</li></ul> 
<h3><a id="2__29"></a>2. 模型效果</h3> 
<p>CogVLM 可以在不牺牲任何 NLP 任务性能的情况下，实现视觉语言特征的深度融合。训练的 CogVLM-17B 是目前多模态权威学术榜单上综合成绩第一的模型，在14个数据集上取得了state-of-the-art或者第二名的成绩。这些基准大致分为三类（共 14 个），包括图像字幕（Image Captioning）、视觉问答（Visual QA）、视觉定位（Visual Grounding）。<br> <img src="https://images2.imgbox.com/39/0d/1YCxOVOC_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="CogView3_36"></a>文生图：CogView3模型</h2> 
<p>链接：https://github.com/THUDM/CogView</p> 
<h2><a id="DALLE3_40"></a>DALL-E3模型</h2> 
<p>论文：https://cdn.openai.com/papers/dall-e-3.pdf</p> 
<h2><a id="CogVideo_42"></a>CogVideo模型</h2> 
<p>论文链接：https://arxiv.org/abs/2205.15868<br> 代码链接：https://github.com/THUDM/CogVideo</p> 
<p>模型训练方法：</p> 
<ul><li>首先基于本文作者团队提出的文本合成图像模型CogView2，CogView2是一个包含60亿参数的预训练transformer模型，CogVideo可以看做是CogView2的视频升级版本，CogVideo共有94亿个参数，并在540万个文本视频对上进行了训练。</li><li>CogVideo的训练主要基于本文提出的多帧分层生成框架，具体来说就是先根据CogView2通过输入文本生成几帧图像，然后再根据这些图像进行插帧提高帧率完成整体视频序列的生成。为了更好的在嵌入空间中对齐文本和视频片段，提高模型对文本预训练知识的迁移，作者提出了一种双通道注意力机制来提高性能。</li><li>此外为了应对模型超大的参数和长视频序列的存储压力，作者将Swin Transformer[4]中的滑动窗口引入到了本文的自回归视频生成任务中</li></ul> 
<p>多帧率分层训练方法：<br> <img src="https://images2.imgbox.com/b9/fc/31lUFD4g_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_53"></a>网易伏羲-丹青模型</h2> 
<p>丹青模型基于原生中文语料数据及网易自有高质量图片数据训练，与其他文生图模型相比，丹青模型的差异化优势在于对中文的理解能力更强，对中华传统美食、成语、俗语、诗句的理解和生成更为准确。比如，丹青模型生成的图片中，鱼香肉丝没有鱼，红烧狮子头没有狮子。基于对中文场景的理解，丹青模型生成的图片更具东方美学，能生成“飞流直下三千尺”的水墨画，也能生成符合东方审美的古典美人。</p> 
<h2><a id="Reference_56"></a>Reference</h2> 
<p>[1] https://github.com/THUDM/CogVLM<br> [2] <a href="https://mp.weixin.qq.com/s/a0Y3YcFqUKibOs_cbCxEnA" rel="nofollow">CogVLM：智谱AI 新一代多模态大模型</a><br> [3] <a href="https://mp.weixin.qq.com/s/9rF657oxOMExbxLdkPha-g" rel="nofollow">CogView：通过Transformer掌握文本到图像的生成</a><br> [4] <a href="https://mp.weixin.qq.com/s/pFqYFCFUdVzsA7NrXWgclw" rel="nofollow">清华联合BAAI提出第一个开源预训练文本视频生成模型CogVideo</a><br> [5] <a href="https://mp.weixin.qq.com/s/sQCpj6V-JCdm-R82JIaNVQ" rel="nofollow">OpenAI最新的文本生成图像大模型DALL·E3</a><br> [6] <a href="https://blog.csdn.net/qq_44681809/article/details/133950855">（2023，DALL-E3，两步微调，标题重建）通过更好的标题改进图像生成</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2e9958ee8ace51642cd028e8d2dbd263/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">在 Android 中使用 C/C&#43;&#43;：初学者综合指南</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fb808fa34f1f1952da5b2dfcd18fc6e8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">算法训练营day18, 二叉树7</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>