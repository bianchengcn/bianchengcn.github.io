<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pyhton爬虫基础（六）urllib中的urlopen - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pyhton爬虫基础（六）urllib中的urlopen" />
<meta property="og:description" content="学习爬虫，最初的操作便是模拟浏览器向服务器发出请求。
我们只需要关心请求的链接是什么，需要的参数是什么，以及如何设置可选的请求头就行了，不需要深入了解它是怎样传输和通信的。
一、使用urllib 它是Pyhton内置的HTTP请求库，包含四个模块。
request：它是最基本的HTTP请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只用给库方法传入URL以及额外的参数，就可以模拟实现这个过程。
error：异常处理模块。
parse：一个工具模块，提供了许多处理URL方法。如拆分、解析、合并等。
robotparser：主要用识别网站的robots.txt文件，判断网站是否能爬取，一般不用。
1.发送请求 使用urllib的request模块，我们可以方便地实现请求的发送并得到响应。下面是具体用法。
(1)urlopen（） urllib.request模块提供了最基本的构造HTTP请求的方法，它还带有处理授权验证、重定向、浏览器Cookies以及其他内容。
import urllib.request resonse=urllib.request.urlopen(&#39;https://www.python.org&#39;) print(resonse.read().decode(&#39;utf-8&#39;)) 只需两行代码就实现Python官网的抓取，输出了网页的源代码。得到源代码后，我们就可以从中提取链接、网图地址、文本信息。
接下来看看它返回的是啥，用type（）方法输出响应类型：
import urllib.request resonse=urllib.request.urlopen(&#39;https://www.python.org&#39;) print(type(resonse)) 输出结果如下：
可以发现它是一个HTTPResonse类型的对象，主要包含read（）、readinto（）、getheader（name）、getheaders（）、fileno（）等方法，以及msg、version、status、reason、debuglevel、closed等属性。
得到这个对象后，我们把它赋值为response变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。
data参数
data参数是可选的。如果要添加bytes类型的内容，则需要通过bytes（）方法转化。如传递了这个参数，则它的请求方式就不再是GET方式而是POST方式。
该方法第一个参数需要的是str类型，需要用urllib.parse模块里的urlencode（）方法来将参数字典转化为字符串：第二个参数指定编码格式。
timeout参数
timeout参数用于设置超时时间，单位为秒，意思就是如果超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。支持HTTP、HTTPS、HTP请求
因此，可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用try except语句来实现，
import socket import urllib.request import urllib.error try: response=urllib.request.urlopen(&#39;http://httpbin.org/get&#39;,timeout=0.1) except urllib.error.URLError as e: if isinstance(e.reason, socket.timeout): print(&#39;TIME OUT&#39;) 其他参数
context参数必须是ssl,SSLContext类型，用来指定SSL设置。
cafile和capath这俩参数分别指定CA证书和它的路径，这个在请求HTTPS链接时有用。
cadefault参数现在已经弃用了，其默认值为False。
更详细信息" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/c7801cdda2adfa1757d0f3dfa2b7e791/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-10T23:01:16+08:00" />
<meta property="article:modified_time" content="2023-06-10T23:01:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pyhton爬虫基础（六）urllib中的urlopen</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>学习爬虫，最初的操作便是模拟浏览器向服务器发出请求。</p> 
<p>我们只需要关心请求的链接是什么，需要的参数是什么，以及如何设置可选的请求头就行了，不需要深入了解它是怎样传输和通信的。</p> 
<h2>一、使用urllib</h2> 
<p>它是Pyhton内置的HTTP请求库，包含四个模块。</p> 
<p>request：它是最基本的HTTP请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只用给库方法传入URL以及额外的参数，就可以模拟实现这个过程。</p> 
<p>error：异常处理模块。</p> 
<p>parse：一个工具模块，提供了许多处理URL方法。如拆分、解析、合并等。</p> 
<p>robotparser：主要用识别网站的robots.txt文件，判断网站是否能爬取，一般不用。</p> 
<h3>1.发送请求</h3> 
<p>使用urllib的request模块，我们可以方便地实现请求的发送并得到响应。下面是具体用法。</p> 
<h4>(1)urlopen（）</h4> 
<p>urllib.request模块提供了最基本的构造HTTP请求的方法，它还带有处理授权验证、重定向、浏览器Cookies以及其他内容。</p> 
<pre><code class="language-python">import urllib.request
resonse=urllib.request.urlopen('https://www.python.org')
print(resonse.read().decode('utf-8'))</code></pre> 
<p>只需两行代码就实现Python官网的抓取，输出了网页的源代码。得到源代码后，我们就可以从中提取链接、网图地址、文本信息。</p> 
<p>接下来看看它返回的是啥，用type（）方法输出响应类型：</p> 
<pre><code class="language-python">import urllib.request
resonse=urllib.request.urlopen('https://www.python.org')
print(type(resonse))</code></pre> 
<p>输出结果如下：</p> 
<p><img alt="" height="47" src="https://images2.imgbox.com/9e/9c/AUKREjJO_o.png" width="370"></p> 
<p>可以发现它是一个HTTPResonse类型的对象，主要包含read（）、readinto（）、getheader（name）、getheaders（）、fileno（）等方法，以及msg、version、status、reason、debuglevel、closed等属性。</p> 
<p>得到这个对象后，我们把它赋值为response变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。</p> 
<p><strong>data参数</strong></p> 
<p>data参数是可选的。如果要添加bytes类型的内容，则需要通过bytes（）方法转化。如传递了这个参数，则它的请求方式就不再是GET方式而是POST方式。</p> 
<p>该方法第一个参数需要的是str类型，需要用urllib.parse模块里的urlencode（）方法来将参数字典转化为字符串：第二个参数指定编码格式。</p> 
<p><strong>timeout参数</strong></p> 
<p>timeout参数用于设置超时时间，单位为秒，意思就是如果超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。支持HTTP、HTTPS、HTP请求</p> 
<p>因此，可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用try except语句来实现，</p> 
<pre><code class="language-python">import socket
import urllib.request
import urllib.error
try:
    response=urllib.request.urlopen('http://httpbin.org/get',timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')</code></pre> 
<p> <strong>其他参数</strong></p> 
<p>context参数必须是ssl,SSLContext类型，用来指定SSL设置。</p> 
<p>cafile和capath这俩参数分别指定CA证书和它的路径，这个在请求HTTPS链接时有用。</p> 
<p>cadefault参数现在已经弃用了，其默认值为False。</p> 
<p><a class="link-info" href="http://https/3/library/urllib.request.html" rel="nofollow" title="更详细信息">更详细信息</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ab46247967a3a7a01aa764ccf73db4e9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">编译原理 | 课程设计 — PL/0编译程序语义分析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1ee8ca1f75e8e23992cf114770a28777/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">推荐两个阅读论文的ai辅助网站</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>