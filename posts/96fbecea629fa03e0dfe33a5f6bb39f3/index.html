<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文简述】GigaMVS: A Benchmark for Ultra-large-scale Gigapixel-level 3D Reconstruction（TPAMI 2021） - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文简述】GigaMVS: A Benchmark for Ultra-large-scale Gigapixel-level 3D Reconstruction（TPAMI 2021）" />
<meta property="og:description" content="一、论文简述 1. 第一作者：Jianing Zhang
2. 发表年份：2021
3. 发表期刊：TPAMI 4. 关键词：多视角立体视觉基准，十亿像素图像数据集，大规模场景重建
5. 探索动机：从特征工程方法到数据驱动方法，多视图立体视觉(MVS)方法得到了快速发展和广泛研究，它可以从多幅图像中重建三维几何和纹理。然而，目前还没有包含大规模场景的3D几何图形和小细节的高分辨率观测数据集来对算法进行基准测试。
6. 核心思想：为此，本文提出了GigaMVS，这是第一个用于超大规模场景的基于十亿像素图像的3D重建基准。真实几何图形由激光扫描仪捕获，覆盖超大尺度场景，平均面积为8667m2，最大面积为32007m2。由于超大规模、复杂遮挡和千兆像素级的图像，GigaMVS暴露了现有MVS算法可扩展性和效率差的问题。本文在几何和纹理测量方面彻底研究了最先进的方法，指出了现有方法的弱点和未来工作的有希望的机会。本文相信GigaMVS可以使3D重建社区受益，并支持平衡鲁棒性，可扩展性和准确性的新算法的发展。
7. 论文下载：
https://github.com/THU-luvision/GigaMVS
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9547729
二、实现过程 1. 概述 在众多的三维重建方案中，多视角立体视觉(MVS)方法可以被动地从多幅图像中重建三维几何形状和精细纹理，被认为是一种有效、高效、低成本的方法3D建模方法。近年来，从特征工程方法到数据驱动方法，MVS方法得到了迅速发展和广泛研究。同时，MVS基准在推进和分析最新技术方面发挥着越来越重要的作用MVS方法。从历史上看，基准测试具有从小型对象到大型户外场景，从受控到自然照明条件，以及从低分辨率到高分辨率图像的增长趋势。
审查现有的基准，他们很难同时获得大尺度场景的三维几何和高分辨率观测数据。例如，优秀的坦克和寺庙[1]基准关注的是大型物体，如火车(35m2)和小寺庙(713m2)，然而，这是唯一获得低分辨率观测(800万像素)的途径。
审查现有的基准，他们很难获得大规模场景的三维几何和高分辨率观测。 例如，优秀的Tanks
and Temples基准侧重于像火车（35m2）和小寺庙（713m2）这样的大型物体，然而，然而，这是唯一的低分辨率观测(800万像素)。 相比之下，ETH3D基准提供高分辨率图像（2400万像素）。 而它只扫描实验室和庭院等中等规模的建筑（100m2）。随着MVS向大规模场景移动，它有望有一个具有挑战性的基准，包括许多详细的观察，以支持新算法的开发和研究。
本文提出了一种新的MVS基准，称为GigaMVS，它支持使用十亿像素级图像重建超大规模的3D模型。基准也被称为Palace&amp;Refelo，具有以下特点：（1）多尺度：激光扫描的三维模型包含了palace尺度的场景和Refolo尺度的局部细节，其点云由多达15亿个激光融合点组成。（2）大规模：收集的场景，最多32007m2，平均比最近的大规模数据集大20倍。（3）高分辨率：大量的千兆像素级图像（同时具有宽视场和高空间分辨率）可以为多尺度重建提供极高分辨率的细节，比现有的高分辨率三维重建基准大10倍左右。
在评估方面，基准测试提供了来自激光扫描仪的精确的地面真实3D模型，其中嘈杂的点云或不可避免的移动物体通过后处理被仔细抛光或消除。在评估过程中，不仅测量几何性能，还测量视觉质量，这可以作为三维重建基准的补充评估协议。考虑到视点稀疏度和图像分辨率会影响三维重建的性能，设置了不同的空间角度分辨率来研究现有的MVS算法。
总之，本文提出了一种新的以千兆像素级图像为输入的超大规模三维场景重建基准，即GigamVS。 MVS社区面临的挑战和机遇可以归纳如下：
大规模：扫描的场景是超大规模的，其面积比最近大规模MVS数据集的面积大一个数量级。 这种可扩展性对基于图像的三维重建技术的鲁棒性和效率提出了极大的挑战。
千兆像素图像：输入的千兆像素级图像具有宽视场和高分辨率的特性，支持高度多尺度的观察，如Palace尺度的场景和Relievo尺度的局部细节。这样的超高分辨率图像对现有算法的可扩展性和效率提出了挑战。
评估：基准提供密集的激光扫描真实三维模型，以及预先简明标记的相机位姿进行评估。除了传统的几何测量之外，还提出了纹理测量，它计算了人工图像与真实图像之间的视觉精度和完整性。
鉴于上述特点，基准GigamVS区别于现有的基准，这必将对社区产生影响，推动基于MVS的三维重建算法的发展，特别是对现实世界中具有挑战性的大规模场景的重建。
2. 相关 2.1. MVS基准
著名的Middlebury基准提供低分辨率图像(640 X 480)、激光扫描的真实模型和精确的相机位姿。作为最早的基准，Middlebury基准只扫描受控光照条件下的小物体。最近，Aanæs等人构建了包含数十个不同材料和光照条件的室内物体的DTU数据集。在DTU数据集中，100多张图像是由受控的机械臂在受控照明的实验室中捕获的。DTU数据集的图像分辨率(1600 X 1200)高于Middlebury基准。虽然DTU数据集可以为训练提供更多的图像，但受控的实验室环境和固定的摄像机轨迹使任务更容易完成。此外，许多在DTU数据集上训练的基于学习的模型对现实场景不具有鲁棒性。
由于室内基准测试的局限性，最近的基准测试更多地关注真实的室外场景。与Middlebury基准测试不同，EPFL基准测试中的图像是在室外环境中拍摄的，分辨率为620万像素。然而，EPFL的基准，只包括三个室外建筑立面，缺乏足够的数据来更好地评估。同样，Merrell等人提出的UNC数据集。同样侧重于室外场景重建，但场景规模比EPFL基准更大。UNC数据集强调在没有受控光线和常规相机姿势的情况下捕获图像的逼真条件。然而，UNC数据集只提供了一个场景，这是一个很大的限制。
Knapitsch等人从受控条件转向现实条件，构建了Tanks and Temples基准，该基准包含更多的室内和室外场景，并提供了过多的视频图像以及图像3d模型。但是，相机位姿不包括在这个基准中，因为这个基准是用于评估完整的重建管道，包括来自运动和多视图立体的结构。虽然Tanks and Temples的基准宣称专注于大规模场景的重建，但超过300平方米的场景只有3个，3万平方米左右的场景只有1个。
ETH3D基准提供了一个低分辨率(40万像素)和一个高分辨率(2400万像素)集。与Tanks and Temples基准中的评估方案类似，ETH3D中的指标通过测量精度、完整性和超过一定阈值的F分数来明确评估3D点云。然而，它只能扫描实验室和庭院等少数中型建筑(~100 m2)。在GigaMVS基准测试中，遵循几何评估协议，但用于评估的3D点云要大20倍，其中包含宫殿规模的场景和浮雕规模的局部细节。
最近的BlendedMVS数据集利用了Altizure在线平台生成100多个大型户外3D模型，并合成相应的多视图图像。然而，BlendedMVS数据集不能作为基准，因为它的三维真实模型是由Altizure重建算法生成的，而不是精确的扫描仪。相比之下，GigaMVS基准可以提供精确的3D模型进行全面评估。
一些基准测试评估了没有真实三维模型的重建结果。Chil等人利用渲染后的图像对基于人类观察者的实验重建结果进行评价。Waechter等通过将渲染后的图像与对应的真实图像进行对比，评估重建结果的质量。这些方法可以同时对重建的纹理质量和几何形状进行评价，但由于评价的视角有限，对几何形状的评价不够精确。在我们的评价方法中，除了基于真实三维模型的评价外，还采用了基于图像的纹理评价方法。
高分辨率图像。考虑到细节丰富的高分辨率图像对算法的可扩展性和鲁棒性构成挑战，Yang等提出了一种用于立体匹配的高分辨率合成数据集，输入图像分辨率为507万像素。ETH3D提供高分辨率图像，与2400万像素图像进行多视点立体匹配。近年来，随着DLSR、阵列相机等记录设备的飞速发展，以及智能手机内置的功能强大的智能摄像头，可以轻松捕捉到超高分辨率甚至千兆像素级的图像，从而引发了高分辨率数据集的革命。Wang等人提出了PANDA数据集，这是第一个千兆像素级的以人为中心的视频数据集。为了捕获大视场的高分辨率图像，PANDA使用了特殊的相机阵列，提供的图像分辨率超过25000X14000，这对现有的计算机视觉算法提出了很大的挑战。
大规模户外场景的多尺度三维重建需要高分辨率的图像，同时具有大视场和远距离细节。受PANDA数据集的启发，采用了千兆像素的成像管道来捕获输入图像，这增加了基准测试的难度。通过实验，可以清楚地看到，更高分辨率的图像可以提供更详细的信息。但是现有的算法，特别是基于学习的方法，无法处理这种高分辨率的输入图像。
2.2. 基于MVS的三维重建
MVS重建大致可分为深度图融合算法和体方法。深度图融合算法将复杂的MVS问题解耦为基于视图的深度图估计和深度图融合。一些实现良好的算法和开源的MVS库都是从这个管道开始的。例如，PMVS是一种基于块的多视点立体图像，用于处理超大规模的非结构化照片集；OpenMVS是一个开源的多视点立体图像库，它提供了一套算法，用于从局部相机和地标生成密集的点云。一些工作旨在通过使用鲁棒描述符来实现高效匹配，设计超高分辨率图像集的高效匹配管道。这些方法从匹配关键点的稀疏集开始，在过滤虚假匹配之前，将其反复扩展到附近的像素对应，然后在多个视图上聚合图像相似度。而准密集的工作等研究则侧重于更精确、鲁棒的匹配结果，甚至可以用于未校准的图像。
自第一个基于学习的MVS方法以来，许多基于学习的MVS方法都表现出了良好的性能。他们将2D图像或特征投影到3D体中，使用部署在场景空间或参考摄像机空间中的3D代价体，并通过3D CNN进行正则化。与直接通过2D特征进行处理不同，在场景空间中处理3D体的主要优点是，相机位置将隐式地包含在3D体中，并且场景的3D几何形状可以通过3D卷积层显式地预测。此外，在卷积过程中，网络在不考虑图像畸变和各种光照的情况下，以高度并行的方式获得更鲁棒的patch匹配结果。一些方法进一步将代价体改进为可变形的点云结构或点云结构，以考虑内存和效率。
体方法将三维空间划分为规则网格，并使用隐式函数或显式表面表示在全局框架中表示和优化。为了提高重构的可扩展性，一些方法要么采用八叉树表示，或者允许分层多尺度结构首次揭示了稀疏MVS问题，并在实际场景中显示出巨大的潜力。然而，这些表示中的体素点并不能直接表示具有形状和颜色的邻域信息，因此难以在超大区域内生成高保真曲面。
3. 数据集情况 GigaMVS的显著特点是为超大规模的现实场景(高达32007平方米)提供真实的3d模型和高分辨率图像(高达十亿像素级)。提出了一种新的数据收集管道来捕获和重建大规模的三维场景。利用激光扫描仪获取真值几何。然后，通过拼接多个高分辨率图像来捕获十亿像素的图像。最后，将纹理映射到真实几何，生成具有十亿像素纹理的大规模3D模型。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/96fbecea629fa03e0dfe33a5f6bb39f3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-11T16:55:40+08:00" />
<meta property="article:modified_time" content="2024-01-11T16:55:40+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文简述】GigaMVS: A Benchmark for Ultra-large-scale Gigapixel-level 3D Reconstruction（TPAMI 2021）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="text-align:justify;"><span style="color:#333333;"><strong>一、论文简述</strong></span></h2> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>1. 第一作者：</strong>Jianing Zhang</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>2. 发表年份：</strong>2021</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>3. 发表期刊：</strong>TPAMI </span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>4. 关键词：</strong>多视角立体视觉基准，十亿像素图像数据集，大规模场景重建</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>5. 探索动机：</strong>从特征工程方法到数据驱动方法，多视图立体视觉(MVS)方法得到了快速发展和广泛研究，它可以从多幅图像中重建三维几何和纹理。然而，目前还没有包含大规模场景的3D几何图形和小细节的高分辨率观测数据集来对算法进行基准测试。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>6. 核心思想：</strong>为此，本文提出了GigaMVS，这是第一个用于超大规模场景的基于十亿像素图像的3D重建基准。真实几何图形由激光扫描仪捕获，覆盖超大尺度场景，平均面积为8667m2，最大面积为32007m2。由于超大规模、复杂遮挡和千兆像素级的图像，GigaMVS暴露了现有MVS算法可扩展性和效率差的问题。本文在几何和纹理测量方面彻底研究了最先进的方法，指出了现有方法的弱点和未来工作的有希望的机会。本文相信GigaMVS可以使3D重建社区受益，并支持平衡鲁棒性，可扩展性和准确性的新算法的发展。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>7. 论文下载：</strong></span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><a class="link-info" href="https://github.com/THU-luvision/GigaMVS" title="https://github.com/THU-luvision/GigaMVS">https://github.com/THU-luvision/GigaMVS</a></span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><a class="link-info" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9547729" rel="nofollow" title="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9547729">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9547729</a></span></p> 
<h2 style="text-align:justify;"><span style="color:#333333;"><strong>二、实现过程</strong></span></h2> 
<h4 style="text-align:justify;"><span style="color:#333333;"><strong>1. 概述</strong></span></h4> 
<p style="text-align:justify;"><span style="color:#333333;">在众多的三维重建方案中，多视角立体视觉(MVS)方法可以被动地从多幅图像中重建三维几何形状和精细纹理，被认为是一种有效、高效、低成本的方法3D建模方法。近年来，从特征工程方法到数据驱动方法，MVS方法得到了迅速发展和广泛研究。同时，MVS基准在推进和分析最新技术方面发挥着越来越重要的作用MVS方法。从历史上看，基准测试具有从小型对象到大型户外场景，从受控到自然照明条件，以及从低分辨率到高分辨率图像的增长趋势。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;">审查现有的基准，他们很难同时获得大尺度场景的三维几何和高分辨率观测数据。例如，优秀的坦克和寺庙[1]基准关注的是大型物体，如火车(35m2)和小寺庙(713m2)，然而，这是唯一获得低分辨率观测(800万像素)的途径。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">审查现有的基准，他们很难获得大规模场景的三维几何和高分辨率观测。 例如，优秀的Tanks<br> and Temples基准侧重于像火车（35m2）和小寺庙（713m2）这样的大型物体，然而，然而，这是唯一的低分辨率观测(800万像素)。 相比之下，ETH3D基准提供高分辨率图像（2400万像素）。 而它只扫描实验室和庭院等中等规模的建筑（100m2）。随着MVS向大规模场景移动，它有望有一个具有挑战性的基准，包括许多详细的观察，以支持新算法的开发和研究。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">本文提出了一种新的MVS基准，称为GigaMVS，它支持使用十亿像素级图像重建超大规模的3D模型。基准也被称为Palace&amp;Refelo，具有以下特点：（1）多尺度：激光扫描的三维模型包含了palace尺度的场景和Refolo尺度的局部细节，其点云由多达15亿个激光融合点组成。（2）大规模：收集的场景，最多32007m2，平均比最近的大规模数据集大20倍。（3）高分辨率：大量的千兆像素级图像（同时具有宽视场和高空间分辨率）可以为多尺度重建提供极高分辨率的细节，比现有的高分辨率三维重建基准大10倍左右。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">在评估方面，基准测试提供了来自激光扫描仪的精确的地面真实3D模型，其中嘈杂的点云或不可避免的移动物体通过后处理被仔细抛光或消除。在评估过程中，不仅测量几何性能，还测量视觉质量，这可以作为三维重建基准的补充评估协议。考虑到视点稀疏度和图像分辨率会影响三维重建的性能，设置了不同的空间角度分辨率来研究现有的MVS算法。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">总之，本文提出了一种新的以千兆像素级图像为输入的超大规模三维场景重建基准，即GigamVS。 MVS社区面临的挑战和机遇可以归纳如下：</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;"><strong>大规模：</strong>扫描的场景是超大规模的，其面积比最近大规模MVS数据集的面积大一个数量级。 这种可扩展性对基于图像的三维重建技术的鲁棒性和效率提出了极大的挑战。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;"><strong>千兆像素图像：</strong>输入的千兆像素级图像具有宽视场和高分辨率的特性，支持高度多尺度的观察，如Palace尺度的场景和Relievo尺度的局部细节。这样的超高分辨率图像对现有算法的可扩展性和效率提出了挑战。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;"><strong>评估：</strong>基准提供密集的激光扫描真实三维模型，以及预先简明标记的相机位姿进行评估。除了传统的几何测量之外，还提出了纹理测量，它计算了人工图像与真实图像之间的视觉精度和完整性。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">鉴于上述特点，基准GigamVS区别于现有的基准，这必将对社区产生影响，推动基于MVS的三维重建算法的发展，特别是对现实世界中具有挑战性的大规模场景的重建。</span></p> 
<h4 style="margin-left:0px;text-align:justify;"><span style="color:#333333;"><strong>2. 相关</strong></span></h4> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;"><strong>2.1. MVS基准</strong></span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">著名的Middlebury基准提供低分辨率图像(640 X 480)、激光扫描的真实模型和精确的相机位姿。作为最早的基准，Middlebury基准只扫描受控光照条件下的小物体。最近，Aanæs等人构建了包含数十个不同材料和光照条件的室内物体的DTU数据集。在DTU数据集中，100多张图像是由受控的机械臂在受控照明的实验室中捕获的。DTU数据集的图像分辨率(1600 X 1200)高于Middlebury基准。虽然DTU数据集可以为训练提供更多的图像，但受控的实验室环境和固定的摄像机轨迹使任务更容易完成。此外，许多在DTU数据集上训练的基于学习的模型对现实场景不具有鲁棒性。</span></p> 
<ul><li style="margin-left:0px;text-align:justify;"> <p><span style="color:#333333;">由于室内基准测试的局限性，最近的基准测试更多地关注真实的室外场景。与Middlebury基准测试不同，EPFL基准测试中的图像是在室外环境中拍摄的，分辨率为620万像素。然而，EPFL的基准，只包括三个室外建筑立面，缺乏足够的数据来更好地评估。同样，Merrell等人提出的UNC数据集。同样侧重于室外场景重建，但场景规模比EPFL基准更大。UNC数据集强调在没有受控光线和常规相机姿势的情况下捕获图像的逼真条件。然而，UNC数据集只提供了一个场景，这是一个很大的限制。</span></p> </li><li style="margin-left:0px;text-align:justify;"> <p><span style="color:#333333;">Knapitsch等人从受控条件转向现实条件，构建了Tanks and Temples基准，该基准包含更多的室内和室外场景，并提供了过多的视频图像以及图像3d模型。但是，相机位姿不包括在这个基准中，因为这个基准是用于评估完整的重建管道，包括来自运动和多视图立体的结构。虽然Tanks and Temples的基准宣称专注于大规模场景的重建，但超过300平方米的场景只有3个，3万平方米左右的场景只有1个。</span></p> </li><li style="margin-left:0px;text-align:justify;"> <p><span style="color:#333333;">ETH3D基准提供了一个低分辨率(40万像素)和一个高分辨率(2400万像素)集。与Tanks and Temples基准中的评估方案类似，ETH3D中的指标通过测量精度、完整性和超过一定阈值的F分数来明确评估3D点云。然而，它只能扫描实验室和庭院等少数中型建筑(~100 m2)。在GigaMVS基准测试中，遵循几何评估协议，但用于评估的3D点云要大20倍，其中包含宫殿规模的场景和浮雕规模的局部细节。</span></p> </li><li style="margin-left:0px;text-align:justify;"> <p><span style="color:#333333;">最近的BlendedMVS数据集利用了Altizure在线平台生成100多个大型户外3D模型，并合成相应的多视图图像。然而，BlendedMVS数据集不能作为基准，因为它的三维真实模型是由Altizure重建算法生成的，而不是精确的扫描仪。相比之下，GigaMVS基准可以提供精确的3D模型进行全面评估。</span></p> </li><li style="margin-left:0px;text-align:justify;"> <p><span style="color:#333333;">一些基准测试评估了没有真实三维模型的重建结果。Chil等人利用渲染后的图像对基于人类观察者的实验重建结果进行评价。Waechter等通过将渲染后的图像与对应的真实图像进行对比，评估重建结果的质量。这些方法可以同时对重建的纹理质量和几何形状进行评价，但由于评价的视角有限，对几何形状的评价不够精确。在我们的评价方法中，除了基于真实三维模型的评价外，还采用了基于图像的纹理评价方法。</span></p> </li></ul> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>高分辨率图像。</strong>考虑到细节丰富的高分辨率图像对算法的可扩展性和鲁棒性构成挑战，Yang等提出了一种用于立体匹配的高分辨率合成数据集，输入图像分辨率为507万像素。ETH3D提供高分辨率图像，与2400万像素图像进行多视点立体匹配。近年来，随着DLSR、阵列相机等记录设备的飞速发展，以及智能手机内置的功能强大的智能摄像头，可以轻松捕捉到超高分辨率甚至千兆像素级的图像，从而引发了高分辨率数据集的革命。Wang等人提出了PANDA数据集，这是第一个千兆像素级的以人为中心的视频数据集。为了捕获大视场的高分辨率图像，PANDA使用了特殊的相机阵列，提供的图像分辨率超过25000X14000，这对现有的计算机视觉算法提出了很大的挑战。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;">大规模户外场景的多尺度三维重建需要高分辨率的图像，同时具有大视场和远距离细节。受PANDA数据集的启发，采用了千兆像素的成像管道来捕获输入图像，这增加了基准测试的难度。通过实验，可以清楚地看到，更高分辨率的图像可以提供更详细的信息。但是现有的算法，特别是基于学习的方法，无法处理这种高分辨率的输入图像。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>2.2. 基于MVS的三维重建</strong></span></p> 
<p style="text-align:justify;"><span style="color:#333333;">MVS重建大致可分为深度图融合算法和体方法。深度图融合算法将复杂的MVS问题解耦为基于视图的深度图估计和深度图融合。一些实现良好的算法和开源的MVS库都是从这个管道开始的。例如，PMVS是一种基于块的多视点立体图像，用于处理超大规模的非结构化照片集；OpenMVS是一个开源的多视点立体图像库，它提供了一套算法，用于从局部相机和地标生成密集的点云。一些工作旨在通过使用鲁棒描述符来实现高效匹配，设计超高分辨率图像集的高效匹配管道。这些方法从匹配关键点的稀疏集开始，在过滤虚假匹配之前，将其反复扩展到附近的像素对应，然后在多个视图上聚合图像相似度。而准密集的工作等研究则侧重于更精确、鲁棒的匹配结果，甚至可以用于未校准的图像。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">自第一个基于学习的MVS方法以来，许多基于学习的MVS方法都表现出了良好的性能。他们将2D图像或特征投影到3D体中，使用部署在场景空间或参考摄像机空间中的3D代价体，并通过3D CNN进行正则化。与直接通过2D特征进行处理不同，在场景空间中处理3D体的主要优点是，相机位置将隐式地包含在3D体中，并且场景的3D几何形状可以通过3D卷积层显式地预测。此外，在卷积过程中，网络在不考虑图像畸变和各种光照的情况下，以高度并行的方式获得更鲁棒的patch匹配结果。一些方法进一步将代价体改进为可变形的点云结构或点云结构，以考虑内存和效率。</span></p> 
<p style="margin-left:0px;text-align:justify;"><span style="color:#333333;">体方法将三维空间划分为规则网格，并使用隐式函数或显式表面表示在全局框架中表示和优化。为了提高重构的可扩展性，一些方法要么采用八叉树表示，或者允许分层多尺度结构首次揭示了稀疏MVS问题，并在实际场景中显示出巨大的潜力。然而，这些表示中的体素点并不能直接表示具有形状和颜色的邻域信息，因此难以在超大区域内生成高保真曲面。</span></p> 
<h4 style="text-align:justify;"><span style="color:#333333;"><strong>3. 数据集情况</strong></span></h4> 
<p style="text-align:justify;"><span style="color:#333333;">GigaMVS的显著特点是为超大规模的现实场景(高达32007平方米)提供真实的3d模型和高分辨率图像(高达十亿像素级)。提出了一种新的数据收集管道来捕获和重建大规模的三维场景。利用激光扫描仪获取真值几何。然后，通过拼接多个高分辨率图像来捕获十亿像素的图像。最后，将纹理映射到真实几何，生成具有十亿像素纹理的大规模3D模型。</span></p> 
<p class="img-center"><img alt="" height="347" src="https://images2.imgbox.com/c5/ad/ErmhhsET_o.png" width="1039"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图1所示。圆明园大喷泉遗址，面积约7200平方米。提供了具有相机轨迹的真实3D模型，与13.26亿个点和800亿像素级图像相关联。红框表示摄像机在空间中的位置。 </span></p> 
<p class="img-center"><img alt="" height="791" src="https://images2.imgbox.com/bb/dd/UhgWIrJq_o.png" width="791"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图2所示。在GigaMVS基准测试中，高分辨率(十亿像素级)属性的说明。捕获的十亿像素级图像具有宽视场和高分辨率，支持极高地多尺度观测，例如宫殿尺度的场景和浮雕尺度的局部细节。</span></p> 
<p class="img-center"><img alt="" height="328" src="https://images2.imgbox.com/1c/ae/vIzjKYg0_o.png" width="632"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图3所示。使用(a)“捕获的标准图像”和(b)“拼接的十亿像素图像”作为输入的COLMAP重建结果的比较。 </span></p> 
<p class="img-center"><img alt="" height="317" src="https://images2.imgbox.com/9d/3c/czK7vHPt_o.png" width="598"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图4所示。扫描点云数据中存在的伪影说明，包括镜子/玻璃反射的点，动态物体，如行走的人等。这些红点标记的伪影已经被后处理去除。</span></p> 
<p class="img-center"><img alt="" height="710" src="https://images2.imgbox.com/ef/ff/D32ydstJ_o.png" width="983"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图5所示。(a)所有视点的校准相机位姿说明。用于生成高分辨率纹理的多幅图像之间的位移相对于远处的场景非常小，可以视为单中心。(b)拼接的十亿像素级图像。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><img alt="" height="463" src="https://images2.imgbox.com/92/89/eZeq1G0L_o.png" width="1022"></span></p> 
<p style="text-align:justify;"><span style="color:#333333;">图6所示。GigaPan方案与我们的方案缝合图像的比较。</span></p> 
<p class="img-center"><img alt="" height="542" src="https://images2.imgbox.com/c5/07/TKjXpo4X_o.png" width="1044"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图7所示。使用GigaPan和我们的解决方案重建拼接图像的点云。 </span></p> 
<p class="img-center"><img alt="" height="458" src="https://images2.imgbox.com/95/98/plhIhw6J_o.png" width="1022"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图8所示。几何和纹理之间的配准说明。将点云叠加在相应的图像上，其中锐利的边缘表示匹配效果良好。 </span></p> 
<p class="img-center"><img alt="" height="320" src="https://images2.imgbox.com/75/ce/hLFXYv71_o.png" width="1110"></p> 
<p style="text-align:justify;"><span style="color:#333333;">前四列总结了真实点云统计数据，包括占地面积、高度、点数(百万)和用于评估的距离阈值t。最后五列总结了图像参数，包括ISO、f值、快门速度、拍摄的千兆像素图像数量和图像分辨率。 </span></p> 
<p class="img-center"><img alt="" height="1153" src="https://images2.imgbox.com/8f/8c/cBBt03ka_o.png" width="785"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图9所示。基准测试中代表性场景的真实模型。为了更好的可视化，遮挡树木被透明地渲染。 </span></p> 
<h4 style="text-align:justify;"><span style="color:#333333;"><strong>4. 评估</strong></span></h4> 
<p style="text-align:justify;"><span style="color:#333333;">设计了一个评估协议，使用基准来评估MVS算法的几何和纹理性能。几何评估主要关注算法重建与真实几何的逐点距离误差，而纹理评估主要关注捕获的图像与重建的投影之间的视觉相似性。这样一个评估协议的目的是鼓励MVS算法同时关注几何精度和纹理保真度。</span></p> 
<h4 style="text-align:justify;"><span style="color:#333333;"><strong>5. 讨论</strong></span></h4> 
<p style="text-align:justify;"><span style="color:#333333;">在基准GigaMVS中，千兆像素的输入带来了关于“可扩展性”、“十亿像素”、“稀疏性”、“遮挡”和“效率”的新见解。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>可扩展性。</strong>对于大型户外场景，不同内容的尺度通常相差很大。如图9所示，在数据集中，具有代表性的博物馆场景既包含了大型建筑，也包含了小规模的精美雕塑，其尺度变化可能达到10000倍。这种显著的跨尺度特性给尺度自适应MVS算法的发展带来了新的挑战，因为当前MVS算法中的体素和点云等表示，即使采用精心设计的由粗到精策略或八叉树结构，也只能达到1000倍的尺度变化。一种可能的解决方案是连续隐式表示，它可以以任意高分辨率呈现场景。结合自适应采样策略，如Level of Detail (LOD) ，表征和重建可以灵活地随不同尺度而平稳高效地变化。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>十亿像素。</strong>数据集中的十亿像素图像不仅包含局部高分辨率，还包含具有大尺度结构的全局宽视场。换句话说，除了高分辨率图像提供的局部细节外，使用拼接的十亿像素图像的一个关键原因是宽视场提供的全局结构。在图3展示了与捕获的标准图像相比，使用十亿像素图像的有效性。最近，研究人员开始研究十亿像素级的场景重建，例如ACORN提出了一种新的神经场景表示方法，用于超大规模、高分辨率的3D场景。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>稀疏。</strong>对于超大规模的户外场景，捕获密集采样的图像是非常繁琐的，因此稀疏设置将更加实用和经济。然而，高度变化的可扩展性和复杂的遮挡将进一步挑战稀疏输入下的重建算法。本文研究了现有方法在不同稀疏度下的性能。研究表明，现有算法难以处理输入稀疏的超大规模重建。最近，研究人员开始研究非常稀疏的MVS，他们采用几何感知的视图选择策略来处理稀疏设置带来的挑战。数据集GigaMVS可以更好地对这些算法进行基准测试和贡献，因为稀疏的MVS对于超大规模的户外场景更有价值和挑战性。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>遮挡。</strong>现实世界大规模场景中的复杂遮挡对重建算法的鲁棒性提出了许多挑战。例如，基准中的代表性场景(海盐厅，大喷泉)存在严重的遮挡。因此，R-MVSNet等最先进的基于学习的方法只是将多视图特征与不相关的遮挡视图混合在一起，因此深度图的估计结果很差，如图14所示。更高级的遮挡感知迫切需要MVS方法来解决严重的遮挡问题。</span></p> 
<p class="img-center"><img alt="" height="588" src="https://images2.imgbox.com/cd/b7/Tvf5EdmQ_o.png" width="546"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图14所示。基于学习的深度图预测方法(R-MVSNet)与传统的深度图预测方法(COLMAP)的比较。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>效率。</strong>超大规模的场景和千兆像素的图像无疑给计算效率带来了巨大的挑战。传统的COLMAP方法是逐像素迭代重建三维场景，耗费大量的计算时间和资源。基于学习的方法具有更并行化的推理计算，因为学习的卷积网络可以同时处理来自更大感受野的更多像素。然而，它们的训练速度慢，内存消耗大。为了克服高计算复杂度，一种可能的解决方案是结构自适应重建。例如，在墙壁和地板等结构简单的区域分配的计算资源较少，而充满复杂结构和精细细节的区域需要更多的计算资源才能更好地建模。另一个有希望的解决方案可能是语义感知重建。在分割和分类的辅助下，基于区域的立体匹配被证明具有更高的可扩展性和时间效率，因为具有强几何基元的语义感知区域可以显着减少可能的立体对。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>失败案例。</strong>尽管基于学习的MVS方法在DTU和T&amp;T等数据集上取得了很大的成功，但我们发现这些方法在复杂区域下不能产生很好的结果。以图14中的圆明园废墟(大喷泉)为例，R-MVSNet无法估算出具有合理三维结构的深度图。它成功地预测了中央主楼的深度图，但仍然无法生成像旗杆一样的薄结构。由于训练数据缺乏多样性，基于学习的方法鲁棒性和稳定性较低。在基准和算法的研究中仍有很大的改进空间。</span></p> 
<p style="text-align:justify;"><span style="color:#333333;"><strong>隐私。</strong>需要注意的是，在大尺度场景的扫描过程中，不可避免地会捕捉到行人。为了保护数据隐私，我们对图像中的所有行人进行了模糊处理，并在真实3D模型中删除了相应的点云。</span></p> 
<h4 style="text-align:justify;"><span style="color:#333333;"><strong>6. 未来工作</strong></span></h4> 
<p style="text-align:justify;"><span style="color:#333333;">超越GigaMVS基准。目前基于学习的深度估计和立体匹配方法已经引入了语义信息来提高性能。然而，由于现有的MVS基准中缺乏这些信息，目前基于学习的MVS方法很少解决语义感知的三维重建问题。在GigaMVS基准的基础上，逐步构建了语义GigaMVS基准，提供语义级信息(如语义、实例、场景图等)，探索基于语义的三维重建的潜力。“中央主楼”场景的语义标签示意图如图15所示，该场景面积为32007平方米，高度为53米。标签与多尺度语义内容(右)相关，如扶手、雕塑和草坪，以及它们的实例(左)。Semantic-GigaMVS基准测试将在未来的工作中报告。</span></p> 
<p class="img-center"><img alt="" height="438" src="https://images2.imgbox.com/7a/3f/Kjmr9Mf0_o.png" width="1120"></p> 
<p style="text-align:justify;"><span style="color:#333333;">图15所示。“中央主楼”场景的语义/实例标签说明，该场景占地32007平方米，高度53米。标签与多尺度语义内容(右)相关，如扶手、雕塑和草坪，以及它们的实例(左)。 </span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d594cca4b5d08a07c72f61ba5076f57b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">el-table合计行单元格合并、合并行金额四舍五入保留两位小数、合计行样式修改</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/da1321db5135216766498778703fdee4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">TypeError: LinearRegression.__init__() got an unexpected keyword argument ‘normalize‘</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>