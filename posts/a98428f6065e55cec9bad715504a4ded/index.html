<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习实战——循环神经网络（RNN、LSTM、GRU） - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习实战——循环神经网络（RNN、LSTM、GRU）" />
<meta property="og:description" content="忆如完整项目/代码详见github：https://github.com/yiru1225（转载标明出处 勿白嫖 star for projects thanks）
目录
系列文章目录
一、实验综述
1.实验工具及内容
2.实验数据
3.实验目标
4.实验步骤
二、循环神经网络综述
1.循环神经网络简介
1.1 循环神经网络背景
1.2 循环神经网络概念与原理
1.3 循环神经网络发展历程
2.循环神经网络相关知识导入
2.1 序列模型
2.2 文本预处理
2.3 语言模型
2.4 数据集
三、经典循环神经网络原理、实现与优化
1.RNN
1.1 原理
1.2 代码实现（自购建）
1.3 代码实现（API）
1.4 消融实验
1.4.1 num_hiddens
1.4.2 num_steps
1.4.3 batch_size
1.4.4 lr
1.4.5 epoch
1.4.6 综述
2.LSTM
2.1 原理
2.2 代码实现
2.3 消融实验
3.GRU
3.1 原理
3.2 代码实现
3.3 消融实验
4.对比分析
四、高级循环神经网络架构介绍与选择实现
1.深度循环神经网络" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/a98428f6065e55cec9bad715504a4ded/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-23T22:30:00+08:00" />
<meta property="article:modified_time" content="2023-04-23T22:30:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习实战——循环神经网络（RNN、LSTM、GRU）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>       忆如完整项目/代码详见github：<span style="color:#fe2c24;"><strong>https://github.com/yiru1225</strong></span>（转载标明出处 勿白嫖 star for projects thanks）</p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95-toc" style="margin-left:0px;"><a href="#%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95" rel="nofollow">系列文章目录</a></p> 
<p id="%E4%B8%80%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%BC%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%BC%E8%BF%B0" rel="nofollow">一、实验综述</a></p> 
<p id="1.%E5%AE%9E%E9%AA%8C%E5%B7%A5%E5%85%B7%E5%8F%8A%E5%86%85%E5%AE%B9-toc" style="margin-left:40px;"><a href="#1.%E5%AE%9E%E9%AA%8C%E5%B7%A5%E5%85%B7%E5%8F%8A%E5%86%85%E5%AE%B9" rel="nofollow">1.实验工具及内容</a></p> 
<p id="2.%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE-toc" style="margin-left:40px;"><a href="#2.%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE" rel="nofollow">2.实验数据</a></p> 
<p id="3.%E5%AE%9E%E9%AA%8C%E7%9B%AE%E6%A0%87-toc" style="margin-left:40px;"><a href="#3.%E5%AE%9E%E9%AA%8C%E7%9B%AE%E6%A0%87" rel="nofollow">3.实验目标</a></p> 
<p id="4.%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#4.%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4" rel="nofollow">4.实验步骤</a></p> 
<p id="%E4%B8%80%E3%80%81%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0" rel="nofollow">二、循环神经网络综述</a></p> 
<p id="1.%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B-toc" style="margin-left:40px;"><a href="#1.%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B" rel="nofollow">1.循环神经网络简介</a></p> 
<p id="1.1%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%83%8C%E6%99%AF-toc" style="margin-left:80px;"><a href="#1.1%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%83%8C%E6%99%AF" rel="nofollow">1.1 循环神经网络背景</a></p> 
<p id="1.2%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86-toc" style="margin-left:80px;"><a href="#1.2%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86" rel="nofollow">1.2 循环神经网络概念与原理</a></p> 
<p id="1.3%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B-toc" style="margin-left:80px;"><a href="#1.3%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B" rel="nofollow">1.3 循环神经网络发展历程</a></p> 
<p id="2.%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AF%BC%E5%85%A5-toc" style="margin-left:40px;"><a href="#2.%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AF%BC%E5%85%A5" rel="nofollow">2.循环神经网络相关知识导入</a></p> 
<p id="2.1%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#2.1%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B" rel="nofollow">2.1 序列模型</a></p> 
<p id="2.2%20%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86-toc" style="margin-left:80px;"><a href="#2.2%20%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86" rel="nofollow">2.2 文本预处理</a></p> 
<p id="2.3%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#2.3%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" rel="nofollow">2.3 语言模型</a></p> 
<p id="2.4%20%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:80px;"><a href="#2.4%20%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">2.4 数据集</a></p> 
<p id="%E4%BA%8C%E3%80%81%E7%BB%8F%E5%85%B8%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%E3%80%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BC%98%E5%8C%96-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E7%BB%8F%E5%85%B8%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%E3%80%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BC%98%E5%8C%96" rel="nofollow">三、经典循环神经网络原理、实现与优化</a></p> 
<p id="1.RNN-toc" style="margin-left:40px;"><a href="#1.RNN" rel="nofollow">1.RNN</a></p> 
<p id="1.1%20%E5%8E%9F%E7%90%86-toc" style="margin-left:80px;"><a href="#1.1%20%E5%8E%9F%E7%90%86" rel="nofollow">1.1 原理</a></p> 
<p id="1.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E8%87%AA%E8%B4%AD%E5%BB%BA%EF%BC%89-toc" style="margin-left:80px;"><a href="#1.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E8%87%AA%E8%B4%AD%E5%BB%BA%EF%BC%89" rel="nofollow">1.2 代码实现（自购建）</a></p> 
<p id="1.3%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88API%EF%BC%89-toc" style="margin-left:80px;"><a href="#1.3%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88API%EF%BC%89" rel="nofollow">1.3 代码实现（API）</a></p> 
<p id="1.4%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-toc" style="margin-left:80px;"><a href="#1.4%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" rel="nofollow">1.4 消融实验</a></p> 
<p id="1.4.1%20num_hiddens-toc" style="margin-left:80px;">    <a href="#1.4.1%20num_hiddens" rel="nofollow">1.4.1 num_hiddens</a></p> 
<p id="1.4.2%20num_steps-toc" style="margin-left:80px;">    <a href="#1.4.2%20num_steps" rel="nofollow">1.4.2 num_steps</a></p> 
<p id="1.4.3%20batch_size-toc" style="margin-left:80px;">    <a href="#1.4.3%20batch_size" rel="nofollow">1.4.3 batch_size</a></p> 
<p id="1.4.4%20lr-toc" style="margin-left:80px;">    <a href="#1.4.4%20lr" rel="nofollow">1.4.4 lr</a></p> 
<p id="1.4.5%20epoch-toc" style="margin-left:80px;">    <a href="#1.4.5%20epoch" rel="nofollow">1.4.5 epoch</a></p> 
<p id="1.4.6%20%E7%BB%BC%E8%BF%B0-toc" style="margin-left:80px;">    <a href="#1.4.6%20%E7%BB%BC%E8%BF%B0" rel="nofollow">1.4.6 综述</a></p> 
<p id="2.LSTM-toc" style="margin-left:40px;"><a href="#2.LSTM" rel="nofollow">2.LSTM</a></p> 
<p id="2.1%20%E5%8E%9F%E7%90%86-toc" style="margin-left:80px;"><a href="#2.1%20%E5%8E%9F%E7%90%86" rel="nofollow">2.1 原理</a></p> 
<p id="2.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:80px;"><a href="#2.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">2.2 代码实现</a></p> 
<p id="2.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-toc" style="margin-left:80px;"><a href="#2.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" rel="nofollow">2.3 消融实验</a></p> 
<p id="3.GRU-toc" style="margin-left:40px;"><a href="#3.GRU" rel="nofollow">3.GRU</a></p> 
<p id="3.1%20%E5%8E%9F%E7%90%86-toc" style="margin-left:80px;"><a href="#3.1%20%E5%8E%9F%E7%90%86" rel="nofollow">3.1 原理</a></p> 
<p id="3.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:80px;"><a href="#3.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">3.2 代码实现</a></p> 
<p id="3.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-toc" style="margin-left:80px;"><a href="#3.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" rel="nofollow">3.3 消融实验</a></p> 
<p id="4.%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90-toc" style="margin-left:40px;"><a href="#4.%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90" rel="nofollow">4.对比分析</a></p> 
<p id="%E4%B8%89%E3%80%81%E9%AB%98%E7%BA%A7%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%80%89%E6%8B%A9%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E9%AB%98%E7%BA%A7%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%80%89%E6%8B%A9%E5%AE%9E%E7%8E%B0" rel="nofollow">四、高级循环神经网络架构介绍与选择实现</a></p> 
<p id="1.%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:40px;"><a href="#1.%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">1.深度循环神经网络</a></p> 
<p id="1.1%20%E5%8E%9F%E7%90%86-toc" style="margin-left:80px;"><a href="#1.1%20%E5%8E%9F%E7%90%86" rel="nofollow">1.1 原理</a></p> 
<p id="1.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:80px;"><a href="#1.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">1.2 代码实现</a></p> 
<p id="1.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-toc" style="margin-left:80px;"><a href="#1.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" rel="nofollow">1.3 消融实验</a></p> 
<p id="2.%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:40px;"><a href="#2.%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">2.双向循环神经网络</a></p> 
<p id="3.%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C-toc" style="margin-left:40px;"><a href="#3.%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C" rel="nofollow">3.稠密连接网络</a></p> 
<p id="4.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:40px;"><a href="#4.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">4.机器翻译与数据集</a></p> 
<p id="5.%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84-toc" style="margin-left:40px;"><a href="#5.%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84" rel="nofollow">5.编码器-解码器架构</a></p> 
<p id="6.%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0-toc" style="margin-left:40px;"><a href="#6.%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0" rel="nofollow">6.序列到序列学习</a></p> 
<p id="%E4%BA%94%E3%80%81%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81%E6%80%BB%E7%BB%93" rel="nofollow">五、总结</a></p> 
<p id="1.%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA-toc" style="margin-left:40px;"><a href="#1.%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA" rel="nofollow">1.实验结论</a></p> 
<p id="2.%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-toc" style="margin-left:40px;"><a href="#2.%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99" rel="nofollow">2. 参考资料</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95">系列文章目录</h2> 
<p>本系列博客重点在深度学习相关实践（有问题欢迎在评论区讨论指出，或直接私信联系我）。</p> 
<p>第一章  <a href="https://blog.csdn.net/weixin_51426083/article/details/129340099?spm=1001.2014.3001.5501" title="深度学习实战——不同方式的模型部署（CNN、Yolo）_如何部署cnn_@李忆如的博客">深度学习实战——不同方式的模型部署（CNN、Yolo）_如何部署cnn_@李忆如的博客</a></p> 
<p>第二章  <a href="https://blog.csdn.net/weixin_51426083/article/details/129966937?spm=1001.2014.3001.5502" title="深度学习实战——卷积神经网络/CNN实践(LeNet、Resnet)_@李忆如的博客-CSDN博客">深度学习实战——卷积神经网络/CNN实践(LeNet、Resnet)_@李忆如的博客-CSDN博客</a></p> 
<p>第三章  深度学习实战——循环神经网络（RNN、LSTM、GRU）</p> 
<hr> 
<p id="main-toc"></p> 
<p><strong>梗概</strong></p> 
<p>本篇博客主要介绍几种循环神经网络的原理，并进行了代码实践与优化（内含代码与数据集）。</p> 
<hr> 
<h2 id="%E4%B8%80%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%BC%E8%BF%B0">一、实验综述</h2> 
<p>本章主要对实验思路、环境、步骤进行综述，梳理整个实验报告架构与思路，方便定位。</p> 
<h3 id="1.%E5%AE%9E%E9%AA%8C%E5%B7%A5%E5%85%B7%E5%8F%8A%E5%86%85%E5%AE%B9"><strong><strong>1.实验工具及</strong></strong><strong><strong>内容</strong></strong></h3> 
<p><span style="color:#000000;">本次实验</span><span style="color:#000000;">主要</span><span style="color:#000000;">使用</span><strong><span style="color:#000000;"><strong>Pycharm</strong></span></strong><span style="color:#000000;">完成几种循环神经网络的代码</span><strong><span style="color:#000000;"><strong>Pytorch架构</strong></span></strong><span style="color:#000000;">实现与优化</span><span style="color:#000000;">，</span><span style="color:#000000;">并通过不同参数的消融实验采集数据分析后进行</span><strong><span style="color:#000000;"><strong>性能对比</strong></span></strong><span style="color:#000000;">。另外，通过论文与资料研读了高级循环神经网络，并尝试完成其训练/推理的实现与对比，并给出了一定优化思路。</span></p> 
<h3 id="2.%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE"><strong><strong>2.实验数据</strong></strong></h3> 
<p><span style="color:#000000;">本次实验</span><span style="color:#000000;">大部分数据来自循环神经网络</span><strong><span style="color:#000000;"><strong>模型官方数据集</strong></span></strong><span style="color:#000000;">，部分测试数据来源于</span><strong><span style="color:#000000;"><strong>网络</strong></span></strong><span style="color:#000000;">。</span></p> 
<h3 id="3.%E5%AE%9E%E9%AA%8C%E7%9B%AE%E6%A0%87"><strong><strong>3.实验目标</strong></strong></h3> 
<p><span style="color:#000000;">本次实验目标主要是深度剖析循环神经网络的原理与模型定义，并了解不同参数的意义与对模型的贡献度（性能影响），通过实践完成不同模型、参数情况的性能对比，指导真实项目开发中应用。</span></p> 
<h3 id="4.%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4"><strong><strong>4.实验步骤</strong></strong></h3> 
<p>本次实验大致流程如表1所示：</p> 
<p style="text-align:center;">表1 实验流程</p> 
<table align="center" border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:324.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1．实验思路综述</span></p> </td></tr><tr><td style="vertical-align:top;width:324.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">2</span><span style="color:#000000;">.</span><span style="color:#000000;">循环神经网络综述</span></p> </td></tr><tr><td style="vertical-align:top;width:324.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">3</span><span style="color:#000000;">.</span><span style="color:#000000;">经典循环神经网络原理、实现与优化</span></p> </td></tr><tr><td style="vertical-align:top;width:324.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">4</span><span style="color:#000000;">.</span><span style="color:#000000;">高级循环神经网络架构介绍与选择实现</span></p> </td></tr><tr><td style="vertical-align:top;width:324.35pt;"></td></tr></tbody></table> 
<h2 id="%E4%B8%80%E3%80%81%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0" style="margin-left:.0001pt;text-align:justify;"><strong><strong>二、循环神经网络综述</strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">本实验无论是实践RNN还是其他高级/现代架构，都属于循环神经网络，故本章先对循环神经网络的概念与原理做一定综述，并简述其发展历程。</p> 
<h3 id="1.%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.循环神经网络简介</strong></strong></h3> 
<h4 id="1.1%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%83%8C%E6%99%AF" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.1 循环神经网络背景</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">通过实验1和实验2，我们主要处理的都是两种类型的数据：<strong><strong>表格数据</strong></strong><strong><strong>或</strong></strong><strong><strong>图像数据</strong></strong>。对于图像数据，我们设计了专门的卷积神经网络架构来为这类特殊的数据结构建模，即可以有效利用图像的像素位置/标签等信息，在之前的实验中对CNN的原理、实现、训练评估、部署全流程都有涉及，在此不赘述。</p> 
<p style="margin-left:.0001pt;text-align:justify;">但到目前为止我们默认数据都来自于某种分布，并且所有样本都是独立同分布的，即<strong><strong>没有太关注数据的顺序/上下文</strong></strong>。然而，大多数的数据并非如此。例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。</p> 
<p style="margin-left:.0001pt;text-align:justify;">我们以一个NLP的命名实体识别例子论证上面所说，见表2，网络对比如图1：</p> 
<p style="margin-left:.0001pt;text-align:center;">表2 命名实体识别样例</p> 
<table border="1" cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">第一句话：I like eating <span style="color:#ff0000;">apple</span>！（我喜欢吃苹果！）</p> <p style="margin-left:.0001pt;text-align:justify;">第二句话：The <span style="color:#ff0000;">a</span><span style="color:#ff0000;">pple</span> is a great company！（苹果真是一家很棒的公司！）</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">分析：任务是要给apple打Label，我们都知道两个apple分别是水果、公司，假设现在有大量的已经标记好的数据以供训练模型，当我们使用全连接神经网络/卷积神经网络，做法是把apple这个单词的特征向量输入到我们的模型中，在输出结果时，让我们的label里，正确的label概率最大，但我们的语料库中，有的apple的label是水果，有的是公司，这将导致<strong><strong>预测的准确程度，取决于训练集中哪个label多一些</strong></strong>，这样的模型是没有意义的。问题就出在了我们没有结合上下文去训练模型，而是单独的在训练apple这个单词的label。</p> 
<p class="img-center"><img alt="" height="324" src="https://images2.imgbox.com/63/eb/kqIdQPuH_o.png" width="477"></p> 
<p style="margin-left:.0001pt;text-align:center;">图1 网络架构对比（FCN vs CNN vs RNN）</p> 
<p style="margin-left:.0001pt;text-align:justify;">另一个问题来自这样一个事实：我们<strong><strong>不仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续</strong></strong>。这在时间序列分析中是相当常见的，可以用来预测股市的波动、 患者的体温曲线或者赛车所需的加速度。同理，我们需要能够处理这些数据的特定模型。</p> 
<h4 id="1.2%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.2 循环神经网络概念与原理</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：循环神经网络类似CNN，是一类网络，具体原理在后面两章详解。</p> 
<p style="margin-left:.0001pt;text-align:justify;">根据1.1及过往实验分析，CNN可以有效地处理空间信息，但<strong><strong>对于数据间的关联性与数据预测的表现仍有局限</strong></strong>，而本实验中主要解析的循环神经网络（recurrent neural network，RNN为经典代表）应运而生，可以<strong><strong>更好地处理序列信息</strong></strong><strong><strong>及语义信息</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">循环神经网络的核心原理通过<strong><strong>引入状态变量存储过去的信息和当前的输入</strong></strong>，从而可以确定当前的输出。即<strong><strong>拥有记忆的能力</strong></strong>，并且会根据这些记忆的内容来进行推断，这也是它能利用上下文去处理序列信息的重要原因。</p> 
<h4 id="1.3%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.3 循环神经网络发展历程</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">1982年，美国加州理工学院物理学家John hopfield 发明了一种单层反馈神经网络 <strong><strong>Hopfield network</strong></strong>，用来解决组合优化问题。这是最早的RNN的雏形。1986年随着recurrent，提出 Jordan network，1990年<strong><strong>对Jordan network简化,并采用BP算法进行训练</strong></strong>，便有了如今最简单的包含单个自连接节点的RNN模型。</p> 
<p style="margin-left:.0001pt;text-align:justify;">在此之后，为解决<strong><strong>梯度爆炸和梯度消失</strong></strong>的问题，LSTM出现了，针对其他问题，还有GRU、双向循环神经网络、seq2seq等现代循环神经网络架构出现，具体发展见图2与图3：</p> 
<p class="img-center"><img alt="" height="506" src="https://images2.imgbox.com/7f/02/tkBghdUw_o.png" width="554"></p> 
<p style="text-align:center;">图2 循环神经网络发展历程 - 图形式 </p> 
<p class="img-center"><img alt="" height="751" src="https://images2.imgbox.com/9d/60/j7ZohjZU_o.png" width="428"></p> 
<p style="margin-left:.0001pt;text-align:center;">图3 循环神经网络发展历程 - 表形式</p> 
<h3 id="2.%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AF%BC%E5%85%A5" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.循环神经网络相关知识导入</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">在正式进入循环神经网络的架构与实现前，我们还要对相关知识进行一些引入。</p> 
<p style="margin-left:.0001pt;text-align:justify;">许多使用循环网络的例子都是基于文本数据的，因此我们将在本实验中重点介绍语言模型。在对序列数据进行更详细的回顾之后，我们将介绍文本预处理的实用技术。然后，我们将讨论语言模型的基本概念，并将此讨论作为循环神经网络设计的灵感。最后，我们描述了循环神经网络的梯度计算方法，以探讨训练此类网络时可能遇到的问题。</p> 
<h4 id="2.1%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.1 序列模型</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">根据1.1与1.2我们知道循环神经网络是为了更好处理序列数据而存在的，对序列数据/模型的定义我们在本部分做一下补充。</p> 
<p style="margin-left:.0001pt;text-align:justify;">序列数据我们在上文简介中有提到，本质上是<strong><strong>有一定上下文/随时间变化的数据</strong></strong>，比如用户评价、股票价格。以用户评价为例，比如电影评价与时间可能会出现锚定（anchoring）效应、享乐适应、季节性等现象，一些其他场景总结如表3：</p> 
<p style="margin-left:.0001pt;text-align:center;">表3 不同场景的序列数据样例</p> 
<table border="1" cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"></p> 
    <ol><li style="text-align:justify;">在使用程序时，许多用户都有很强的特定习惯。 例如，在学生放学后社交媒体应用更受欢迎。在市场开放时股市交易软件更常用。</li><li style="text-align:justify;">预测明天的股价要比过去的股价更困难，尽管两者都只是估计一个数字。 毕竟，先见之明比事后诸葛亮难得多。 在统计学中，前者（对超出已知观测范围进行预测）称为外推法， 而后者（在现有观测值之间进行估计）称为内插法。</li><li style="text-align:justify;">在本质上，音乐、语音、文本和视频都是连续的。 如果它们的序列被我们重排，那么就会失去原有的意义。 比如，一个文本标题“狗咬人”远没有“人咬狗”那么令人惊讶，尽管组成两句话的字完全相同。</li><li style="text-align:justify;">地震具有很强的相关性，即大地震发生后，很可能会有几次小余震， 这些余震的强度比非大地震后的余震要大得多。 事实上，地震是时空相关的，即余震通常发生在很短的时间跨度和很近的距离内。</li><li style="text-align:justify;">人类之间的互动也是连续的，这可以从微博上的争吵和辩论中看出。</li></ol><p style="margin-left:.0001pt;text-align:justify;"></p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">    所以和RNN的背景保持一致，如何<strong><strong>使用序列数据/数据相关性去构建模型</strong></strong>（如使用时间动力学）即“序列模型”的核心问题。而构建一个序列模型，核心是<strong><strong>统计工具+模型选择</strong></strong>。以股票预测为例，统计案例如图4所示：</p> 
<p class="img-center"><img alt="" height="283" src="https://images2.imgbox.com/0f/69/AooyLp77_o.png" width="485"></p> 
<p style="margin-left:.0001pt;text-align:center;">图4 序列数据统计样例（富时100指数价格）</p> 
<p style="margin-left:.0001pt;text-align:justify;">    而我们输入模型需要转换为数理的表达，即用xt表示价格。 请注意，t对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在t日的股市中表现良好，于是通过以下式1途径预测：</p> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="x_{t} \sim P\left(x_{t} \mid x_{t-1}, \ldots, x_{1}\right)" class="mathcode" src="https://images2.imgbox.com/fd/27/utTdExW6_o.png"></p> 
<p style="text-align:center;">式1 股票预测样例式 </p> 
<p>而为了实现这个预测，我们常需要引入模型或策略，常见的有<strong><strong>隐变量自回归模型、马尔可夫模型、因果关系</strong></strong>等，如图5所示： </p> 
<p class="img-center"><img alt="" height="297" src="https://images2.imgbox.com/8b/41/dzLxLkOb_o.png" width="554"></p> 
<p style="text-align:center;">图5 序列模型常见选择与核心定义 </p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：对于直到时间步t的观测序列，其在时间步t+k的预测输出是“k步预测”。随着我们对预测时间t值的增加，会造成<strong><strong>误差的快速累积和预测质量的极速下降</strong></strong>。</p> 
<h4 id="2.2%20%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.2 文本预处理</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">对于序列数据处理问题，我们在2.1节中评估了所需的统计工具和预测时面临的挑战。 这样的数据存在许多种形式，文本是最常见例子之一。 例如，一篇文章可以被简单地看作一串单词序列，甚至是一串字符序列。本部分，我们将解析文本的常见预处理步骤如表4：</p> 
<p style="margin-left:.0001pt;text-align:center;">表4 文本预处理步骤</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="text-align:justify;"><strong><strong>1、读取数据集</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">    将文本作为字符串加载到内存中。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">2、<strong><strong>词元化</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">将字符串拆分为词元（如单词和字符，例：['the', 'time', 'machine', 'by', 'h', 'g', 'wells']）。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">3、<strong><strong>构建词表</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">为方便模型使用，建立词表（string-&gt;num，例：[('&lt;unk&gt;', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8)]），将拆分的词元映射到数字索引（分list）。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">4、<strong><strong>功能整合</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">将文本转换为数字索引序列，且打包所有函数，通过load_corpus_time_machine返回corpus（词元索引列表）和vocab（语料库的词表），例：(170580, 28)。</p> </td></tr></tbody></table> 
<h4 id="2.3%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.3 语言模型</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">2.2中我们了解了如何将文本数据映射为词元，本质上它们还是序列数据，所以可以使用2.1的方法对其进行预测，<strong><strong>仅仅只能得到一个“合理”的预测</strong></strong>。仍未<strong><strong>让模型真正“理解”文本</strong></strong>。但这仍是有意义的（如语义歧义判别），故我们需要在本节对语言模型和数据集的核心概念做一定补充，也便于后续平滑地过渡到循环神经网络的原理。</p> 
<p style="margin-left:.0001pt;text-align:justify;">首先语言模型的核心问题与序列数据/模型保持一致，即“<strong><strong>如何对一个文档， 甚至是一个词元序列进行建模？</strong></strong>”，基本概率模型与“自回归+假设”保持一致，如图5所示。为了训练语言模型，我们需要<strong><strong>计算单词的概率</strong></strong>， 以及给定前面<strong><strong>几个单词后出现某个单词的条件概率</strong></strong>。 这些概率本质上就是语言模型的参数。常见的一些方法如图6：</p> 
<p class="img-center"><img alt="" height="238" src="https://images2.imgbox.com/93/20/u2NiykB9_o.png" width="554"></p> 
<p style="margin-left:.0001pt;text-align:center;">图6 单词概率/条件概率常见计算方法</p> 
<p style="margin-left:.0001pt;text-align:justify;">但这样的模型很容易无效，主要因为我们<strong><strong>需要存储所有计数，且没有考虑单词的意思</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">补充：序列建模的近似公式由马尔可夫模型与n元语法推导，如式2：</p> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="\begin{array}{c} P\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=P\left(x_{1}\right) P\left(x_{2}\right) P\left(x_{3}\right) P\left(x_{4}\right) \\ P\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=P\left(x_{1}\right) P\left(x_{2} \mid x_{1}\right) P\left(x_{3} \mid x_{2}\right) P\left(x_{4} \mid x_{3}\right) \\ P\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=P\left(x_{1}\right) P\left(x_{2} \mid x_{1}\right) P\left(x_{3} \mid x_{1}, x_{2}\right) P\left(x_{4} \mid x_{2}, x_{3}\right) \end{array}" class="mathcode" src="https://images2.imgbox.com/5b/6c/wQuTpHXv_o.png"></p> 
<p style="margin-left:.0001pt;text-align:center;">式2 马尔可夫模型与n元语法-&gt;序列建模的近似公式</p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：通常，涉及一个、两个和三个变量的概率公式分别被称为 一元语法（unigram）、二元语法（bigram）和三元语法（trigram）模型。这可以指导我们如何设计更好的模型。</p> 
<p style="margin-left:.0001pt;text-align:justify;">    接下来我们通过真实数据上的<strong><strong>自然语言统计</strong></strong>来说一说其他核心知识。经一元、二元、三元语法的词元频率统计（样例如图7），我们可以发现<strong><strong>词频以一种明确的方式迅速衰减</strong></strong>。将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足<strong><strong>齐普夫定律</strong></strong>，即第i个最常用单词的频率ni如式3：</p> 
<p style="text-align:center;"><img alt="\begin{array}{c} n_{i} \propto \frac{1}{i^{\alpha}} \\ \log n_{i}=-\alpha \log i+c \end{array}" class="mathcode" src="https://images2.imgbox.com/96/66/qUDieYY0_o.png"></p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：其中是a刻画分布的指数，c是常数。</p> 
<p class="img-center"><img alt="" height="254" src="https://images2.imgbox.com/e9/50/AWY4jl7h_o.png" width="343"></p> 
<p style="margin-left:.0001pt;text-align:center;">图7 词元频率统计样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">同时根据图7，我们可以归纳出几个特点（循环神经网络的核心背景）：</p> 
<ol><li style="text-align:justify;">除了一元语法词，单词序列似乎也遵循齐普夫定律，尽管式3中的指数a更小（指数的大小受序列长度的影响）。</li><li style="text-align:justify;">词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望。</li><li style="text-align:justify;">很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用基于深度学习的模型。</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;">最后还有一个问题，<strong><strong>如何读取长序列数据</strong></strong>？</p> 
<p style="margin-left:.0001pt;text-align:justify;">由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题，一个样例问题（如何分割文本）如图8，常见的解决方案是<strong><strong>随机采样</strong></strong>（每个样本都是在原始的长序列上任意捕获的子序列）与<strong><strong>顺序分区</strong></strong>（在基于小批量的迭代过程中保留了拆分的子序列的顺序）。</p> 
<p class="img-center"><img alt="" height="161" src="https://images2.imgbox.com/d0/4e/RJ38KOlt_o.png" width="292"></p> 
<p style="margin-left:.0001pt;text-align:center;">图8 长序列数据读取/处理问题</p> 
<p style="margin-left:.0001pt;text-align:justify;">那<strong><strong>如何去度量语言模型的质量呢</strong></strong>，这是后续部分中用于评估基于循环神经网络模型的关键，答案是<strong><strong>困惑度（Perplexity）</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么，比如我们要用语言模型对“It is raining …”续写，几个样例如表5：</p> 
<p style="margin-left:.0001pt;text-align:center;">表5 语言模型预测样例</p> 
<table border="1" cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">“It is raining outside”（外面下雨了）</p> <p style="margin-left:.0001pt;text-align:justify;">“It is raining banana tree”（香蕉树下雨了）</p> <p style="margin-left:.0001pt;text-align:justify;">“It is raining piouw;kcj pwepoiut”（piouw;kcj pwepoiut下雨了）</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">显然，第一个回答是最合理的，如果量化地去度量这种合理性判别指标呢，核心是<strong><strong>计算序列的似然概率+softmax回归</strong></strong>，即我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量，困惑度即其的指数，如式4所示，故困惑度的本质即“<strong><strong>下一个词元的实际选择数的调和平均数</strong></strong>”。</p> 
<p style="text-align:center;"><img alt="\exp \left(-\frac{1}{\mathrm{n}} \sum_{\mathrm{t}=1}^{\mathrm{n}} \log P\left(x_{t} \mid x_{t-1}, \ldots, x_{1}\right)\right)" class="mathcode" src="https://images2.imgbox.com/59/94/9imVb1ye_o.png"></p> 
<p style="margin-left:.0001pt;text-align:justify;">至此，循环神经网络的核心前置知识引入完成。</p> 
<h4 id="2.4%20%E6%95%B0%E6%8D%AE%E9%9B%86" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.4 数据集</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">在本部分我们对后文实验用到的主要数据集做一下介绍，本实验以文字数据集为样例探究不同循环神经网络的效果，主要使用的是<strong><strong>H.G.Wells的时光机器数据集</strong></strong>，本质上是一本书，详情可见：<a href="https://book.douban.com/subject/25786043/" rel="nofollow" title="时间机器 The Time Machine (豆瓣) (douban.com)">时间机器 The Time Machine (豆瓣) (douban.com)</a>，导入方式见Code1：</p> 
<table align="center" border="1" cellspacing="0" style="width:402.9pt;"><tbody><tr><td style="vertical-align:top;width:402.9pt;"> <p style="margin-left:.0001pt;text-align:center;">Code1 数据集导入（时光机器 - 文字）</p> </td></tr><tr><td style="vertical-align:top;width:402.9pt;"> <p style="margin-left:.0001pt;text-align:left;"><strong><span style="color:#007020;"><strong>from</strong></span></strong> <strong><span style="color:#0e84b5;"><strong>d2l</strong></span></strong> <strong><span style="color:#007020;"><strong>import</strong></span></strong> torch <strong><span style="color:#007020;"><strong>as</strong></span></strong> d2l</p> <p style="margin-left:.0001pt;text-align:left;">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</p> </td></tr></tbody></table> 
<h2 id="%E4%BA%8C%E3%80%81%E7%BB%8F%E5%85%B8%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%E3%80%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BC%98%E5%8C%96" style="margin-left:.0001pt;text-align:justify;"><strong><span style="color:#000000;"><strong>三、经典循环神经网络原理、实现与优化</strong></span></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">上一章中我们对循环神经网络的背景、概念、发展历程做了梳理，并将序列模型、文本预处理、语言模型与数据集等核心前置知识做了补充，本章即将进入经典循环神经网络的原理详解、代码实现、参数与网络优化，本章主要以<strong><strong>RNN（经典）、LSTM、GRU</strong></strong>为例。</p> 
<h3 id="1.RNN" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.RNN</strong></strong></h3> 
<h4 id="1.1%20%E5%8E%9F%E7%90%86" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.1 原理</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">参考论文：<a href="https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1" rel="nofollow" title="Finding Structure in Time - 1990 (wiley.com)">Finding Structure in Time - 1990 (wiley.com)</a> </p> 
<p style="margin-left:.0001pt;text-align:justify;">参考资料：<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="nofollow" title="The Unreasonable Effectiveness of Recurrent Neural Networks ">The Unreasonable Effectiveness of Recurrent Neural Networks </a></p> 
<p style="margin-left:.0001pt;text-align:justify;">第一章铺垫了那么多，让我们正式进入最经典的循环神经网络。RNN最重要、最核心的创新是<strong><strong>循环</strong></strong>，正是因为循环才可以利用数据的相关性/上下文，从而在序列数据中表现优秀。我们来看这么一个展开/迭代例子（标准结构）如图9：</p> 
<p class="img-center"><img alt="" height="262" src="https://images2.imgbox.com/9b/3a/UcSoZZra_o.png" width="377"></p> 
<p style="margin-left:.0001pt;text-align:center;">图9 RNN标准结构-循环本质/迭代推导</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：CNN中我们知道神经网络是<strong><strong>分层顺序激活</strong></strong>的，而RNN<strong><strong>通过循环将训练“学”到的东西蕴藏在权值W中</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">    补充：左侧是折叠起来的样子，右侧是展开的样子，左侧中h旁边的箭头代表此结构中的“循环“体现在隐层。图中O代表输出，y代表样本给出的确定值，L代表损失函数。</p> 
<p style="margin-left:.0001pt;text-align:justify;">泛化一点讲，RNN的核心结构如图10所示：</p> 
<p class="img-center"><img alt="" height="137" src="https://images2.imgbox.com/49/88/ejRVy3Xj_o.png" width="107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图10 RNN核心结构</p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：神经网络A（包含若干层）输入向量为xt，输出向量为ht，它允许网络将这一步的输出传递到下一步作为输入，堆叠/展开后与图9保持一致。</p> 
<p style="margin-left:.0001pt;text-align:justify;">把神经网络看作函数f，其中的权重为w，那RNN 本质上是<strong><strong>循环/</strong></strong><strong><strong>递推函数</strong></strong>，如式5：</p> 
<p style="text-align:center;"><img alt="h_{(t)}=f\left(h_{t-1}, x_{t}, w\right)" class="mathcode" src="https://images2.imgbox.com/ba/11/fehp25MF_o.png"></p> 
<p style="margin-left:.0001pt;text-align:center;">式5 RNN本质函数</p> 
<p style="margin-left:.0001pt;text-align:justify;">根据RNN简介与核心定义，总结其特点如下：</p> 
<ol><li style="text-align:justify;">（1）权值共享，图中的W全是相同的，U和V也一样</li><li style="text-align:justify;">（2）前面的输出会影响后面的输出，适合处理序列数据</li><li style="text-align:justify;">（3）损失也是随着序列的推荐而不断积累的</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;">而根据RNN的不同堆叠形式/数据的不同输入输出，产生了多种变体（非优化），如图11所示：</p> 
<p class="img-center"><img alt="" height="311" src="https://images2.imgbox.com/75/27/dFghpNMS_o.png" width="993"></p> 
<p style="margin-left:.0001pt;text-align:center;">图11 常见RNN变体汇总</p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：上图中每个正方形代表一个向量，箭头代表函数。输入向量是红色，输出向量是蓝色，绿色向量装的是RNN的状态，总结如表6：</p> 
<p style="margin-left:.0001pt;text-align:center;">表6 不同变体中RNN状态（对应图11左至右）</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>1、one to one</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">非RNN的普通过程，从固定尺寸的输入到固定尺寸的输出（比如图像分类），也即输入是x，经过变换Wx+b和激活函数f得到输出y。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>2、one to many</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">输出是序列（例如图像标注：输入是一张图像，输出是单词的序列），同时还有一种结构是把输入信息X作为每个阶段的输入。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>3、many to one</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">输入是序列（例如情绪分析：输入是一个句子，输出是对句子属于正面还是负面情绪的分类）。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>4、many to many（n to n）</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">输入输出都是序列（比如机器翻译：RNN输入一个英文句子输出一个法文句子）。或同步的输入输出序列（比如视频分类中，我们将对视频的每一帧都打标签）。</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：当然除了图11，还有Encoder-Decoder（n to m，Seq2Seq）等重要RNN变体这里没有全部体现。</p> 
<p style="margin-left:.0001pt;text-align:justify;">而以上主要是从概念、结构部分的原理解析，接下来让我们进入<strong><strong>数理推导</strong></strong>部分。对于神经网络最重要的是<strong><strong>前向传播/反向传播</strong></strong>的部分（如何更新参数），RNN也是如此。</p> 
<p style="margin-left:.0001pt;text-align:justify;">同样先展开一个典型的RNN，如图12所示：</p> 
<p class="img-center"><img alt="" height="307" src="https://images2.imgbox.com/b6/39/h8S7aIXR_o.png" width="783"></p> 
<p style="margin-left:.0001pt;text-align:center;">图12 典型RNN展开</p> 
<p style="margin-left:.0001pt;text-align:justify;">图12中，有一条单向流动的信息流是从输入单元到达隐藏单元的，同时另一条单向流动的信息流从隐藏单元到达输出单元。在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”，并且隐藏层的输入还包括上一隐藏层的状态，即隐藏层内的节点可以自连也可以互连。（这实际上就是LSTM，后文详解）</p> 
<p style="margin-left:.0001pt;text-align:justify;">右侧为计算时便于理解记忆而产开的结构。简单说，x为输入层，o为输出层，s为隐含层，而t指第几次的计算；V,W,U为权重，其中计算第t次的隐含层状态时如式6：</p> 
<p style="text-align:center;"><img alt="s_{t}=f\left(U * x_{t}+W * s_{t-1}\right)" class="mathcode" src="https://images2.imgbox.com/b1/1a/q8RZWAiW_o.png"></p> 
<p style="margin-left:.0001pt;text-align:center;">式6 隐含层状态计算</p> 
<p style="margin-left:.0001pt;text-align:justify;">    即通过此实现当前输入结果与之前的计算挂钩的目的，更直观的表达可见图13：</p> 
<p class="img-center"><img alt="" height="694" src="https://images2.imgbox.com/01/2d/b4ZbQP93_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图13 RNN“记忆”核心</p> 
<p style="margin-left:.0001pt;text-align:justify;">根据上述描述与图13，我们可以推理RNN前向传播条件如式7，loss常用<strong><strong>重构误差</strong></strong>或<strong><strong>交叉熵</strong></strong>，如式8和式9：</p> 
<p class="img-center"><img alt="" height="442" src="https://images2.imgbox.com/a6/7e/J8O8AXV3_o.png" width="510"></p> 
<p>同理根据RNN展开去推理反向传播，常出现梯度消失问题，同样因激活函数产生，在此不展开，详见：<a href="https://blog.csdn.net/qq_32172681/article/details/100060263" title="循环神经网络RNN论文解读_循环神经网络论文_纸上得来终觉浅～的博客-CSDN博客">循环神经网络RNN论文解读_循环神经网络论文_纸上得来终觉浅～的博客-CSDN博客</a>。 </p> 
<p style="margin-left:.0001pt;text-align:justify;">最后我们从<strong><strong>是否有隐状态</strong></strong>与<strong><strong>基于RNN的字符级语言模型</strong></strong>作为RNN原理部分的结尾，核心知识总结如图14所示：</p> 
<p class="img-center"><img alt="" height="529" src="https://images2.imgbox.com/55/7c/kz9pMbMz_o.png" width="1002"></p> 
<p style="margin-left:.0001pt;text-align:center;">图14 RNN网络架构总结及字符级语言模型样例</p> 
<h4 id="1.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E8%87%AA%E8%B4%AD%E5%BB%BA%EF%BC%89" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.2 代码实现（自购建）</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">    Tips：RNN及其变体是非常经典且有意义的工作，故代码实现有多种方式，总体来说分为自购建与API调用，本实验RNN分别采用自购建和API调用作为双实现样例，其他架构基本均使用API单实现，参考代码来自李沐老师，详见：<a href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html" rel="nofollow" title="8.5. 循环神经网络的从零开始实现 — 动手学深度学习 2.0.0 documentation (d2l.ai)">8.5. 循环神经网络的从零开始实现 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">根据1.1中对RNN原理/架构的解析，以及基于RNN的字符级语言模型的定义，我们在本部分实现从0到1的RNN实现，代码文件为<strong><strong>RNN（0to1）.py</strong></strong>，在此仅作核心代码的解析。其中，RNN的自购建步骤总结如表7，输入输出编码如图15所示：</p> 
<p style="margin-left:.0001pt;text-align:center;">表7 RNN自购建流程</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>输入</strong></strong>：数据集（本实验基本均为H.G.Wells的时光机器数据集 - 文字）</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>1、独热编码</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">即NLP中的基本操作one-hot encoding，将文本预处理（string-&gt;num），并将索引映射为互补相同的单位向量，方便后续模型读入。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>2、初始化模型参数</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">需要定义隐藏层参数（重要）、输出层参数、附加梯度等模型参数。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>3、模型/网络定义</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">根据需求与RNN定义去搭建模型，包括隐状态返回（初始化时）、计算与输出，以及模型的激活与迭代。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>4、预测</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">定义预测函数来生成prefix（一个用户提供的包含多个字符的字符串）之后的新字符。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>5、梯度裁剪</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">根据1.1中的论述，正常的RNN反向传播会产生O（T）的矩阵乘法链，T较大时可能导致梯度爆炸或消失，故需要进行梯度裁剪。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>6、训练</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">将处理后数据“喂”给模型，进行迭代训练（顺序分区/随机抽样），以困惑度或epoch作为停止训练指标。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>输出</strong></strong>：训练好的模型/文本预测结果</p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="334" src="https://images2.imgbox.com/79/ab/MKYpzo1q_o.png" width="1106"></p> 
<p style="margin-left:.0001pt;text-align:center;">图15 RNN输入/输出编码形式</p> 
<p style="margin-left:.0001pt;text-align:justify;">本部分主要解析RNN模型代码与梯度裁剪代码，网络模型及解析如Code2：</p> 
<pre><code class="language-python"># 初始化时返回隐状态（张量，形状为（批量大小，隐藏单元数））
def init_rnn_state(batch_size, num_hiddens, device):
return (torch.zeros((batch_size, num_hiddens), device=device), )
# 定义如何在一个时间步内计算隐状态和输出（函数作为激活函数）
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量大小，词表大小)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    # X的形状：(批量大小，词表大小)
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
# 定义类去包装函数（并存储从零开始实现的循环神经网络模型的参数）
class RNNModelScratch: #@save
    """从零开始实现的循环神经网络模型"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)
# 模型样例定义类去包装函数（检查输出是否具有正确的形状）
num_hiddens = 512
net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)
state = net.begin_state(X.shape[0], d2l.try_gpu())
Y, new_state = net(X.to(d2l.try_gpu()), state)
Y.shape, len(new_state), new_state[0].shape</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：我们可以看到输出形状是（时间步数x批量大小，词表大小）， 而隐状态形状保持不变，即（批量大小，隐藏单元数）。</p> 
<p style="margin-left:.0001pt;text-align:justify;">而关于梯度裁剪，从数理逻辑来说它的常见方案如式10（<strong><strong>通过将梯度g投影回给定半径 （例如θ</strong></strong><strong><strong>）的球来裁剪梯度g</strong></strong>），一个代码样例如Code3：</p> 
<p style="text-align:center;"><img alt="g \leftarrow \min \left(1, \frac{\theta}{\|g\|}\right) g" class="mathcode" src="https://images2.imgbox.com/ec/86/U0oNeiOI_o.png"></p> 
<p style="margin-left:.0001pt;text-align:center;">式10 梯度裁剪常见方案</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：通过这样做，我们知道梯度范数永远不会超过θ， 并且更新后的梯度完全与g的原始方向对齐，有一定的稳定性。</p> 
<pre><code class="language-python">def grad_clipping(net, theta):  #@save
    """裁剪梯度"""
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm &gt; theta:
        for param in params:
            param.grad[:] *= theta / norm</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">其他部分代码详见附件，在完成代码的编写后我们进入实验/RNN的测试，本实验用到的默认样例参数总结于表8中，作为后续对比实验与消融实验的baseline。</p> 
<p style="margin-left:.0001pt;text-align:center;">表8 RNN模型默认参数样例（本实验）</p> 
<table border="1" cellspacing="0" style="margin-left:36.7pt;width:316.5pt;"><tbody><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">参数名</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">取值</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">batch_size</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">32</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">num_steps（小批量数据时间步）</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">35</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">num_hiddens</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">512</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">vocab（词元数）</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">10000</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">num_epoch</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">500</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">lr</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">1</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">optimizer</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">SGD</p> </td></tr><tr><td style="vertical-align:top;width:170.95pt;"> <p style="margin-left:.0001pt;text-align:center;">激活函数</p> </td><td style="vertical-align:top;width:145.55pt;"> <p style="margin-left:.0001pt;text-align:center;">Tanh</p> </td></tr></tbody></table> 
<p>    万事俱备，让我们正式开始自购建模型的训练与测试，数据集加载/初始化与训练过程如图16所示，单次测试结果如图17所示：</p> 
<p style="text-align:center;"><img alt="" height="93" src="https://images2.imgbox.com/48/54/CZ2LEgjq_o.png" width="691"><img alt="" height="262" src="https://images2.imgbox.com/27/c6/lP9YYxRI_o.png" width="396"></p> 
<p style="margin-left:.0001pt;text-align:center;">图18 自购建RNN结果样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图18，处理后的数据能正常进入RNN模型，<strong><strong>经过500次epoch，最终困惑度为1.2，在个人cpu上速度为24505.1词元/秒</strong></strong>，验证了自购建模型设计与代码实现的合理性与正确性。</p> 
<p style="margin-left:.0001pt;text-align:justify;">根据第一章2.3我们知道RNN是有顺序分区和随机抽样两种策略的，上面的样例是顺序分区的，我们再来测一个随机抽样方案的RNN，代码部分只要在训练函数中加入“use_random_iter=True”，如Code4，测试结果样例如图19所示：</p> 
<pre><code class="language-python">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),
          use_random_iter=True)  # 随机</code></pre> 
<p style="text-align:center;"><img alt="" height="96" src="https://images2.imgbox.com/68/ba/YzMOCeU0_o.png" width="696"><img alt="" height="274" src="https://images2.imgbox.com/9a/b5/tKgTSbVE_o.png" width="390"> </p> 
<p style="margin-left:.0001pt;text-align:center;">图19 自购建RNN结果样例（随机抽样）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图19，<strong><strong>经过500次epoch，RNN随机抽样模型的最终困惑度为1.4，在个人cpu上速度为22995.0词元/秒，速度与性能均略低于顺序分区</strong></strong>（本参数组合样例中）。</p> 
<p style="margin-left:.0001pt;text-align:justify;">为了验证自购建RNN不同方案的速度与性能对比，使用两种RNN<strong><strong>均做20次实验</strong></strong>，取<strong><strong>平均词元/秒与平均困惑度</strong></strong>分别作为度量指标，结果数据汇总于表9，效果对比如图20所示：</p> 
<p style="margin-left:.0001pt;text-align:center;">表9 顺序分区RNN vs 随机抽样RNN（数据汇总）</p> 
<table align="center" cellspacing="0" style="width:201.8pt;"><tbody><tr><td style="width:63.75pt;"> <p style="margin-left:.0001pt;text-align:center;"></p> </td><td style="width:69.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td></tr><tr><td style="width:63.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">顺序分区</span></p> </td><td style="width:69.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.26</span></span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">24701.5</span></span></p> </td></tr><tr><td style="width:63.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">随机抽样</span></p> </td><td style="width:69.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.43</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">23078.2</span></p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="" height="475" src="https://images2.imgbox.com/57/14/IacypJgv_o.png" width="756"></p> 
<p style="margin-left:.0001pt;text-align:center;">图20 顺序分区RNN vs 随机抽样RNN（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据表9与图20，我们可以看到<strong><strong>顺序分区RNN的平均困惑度小于随机抽样RNN，且词元/秒前者大于后者</strong></strong>，故在本数据集&amp;本参数组合下可<strong><strong>验证顺序分区RNN速度与性能均优于随机抽样RNN</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">而对于梯度爆炸或消失的问题，本实验同样可以尝试去删除梯度裁剪这一步去探究。一个简单方法是“把train_epoch_ch8里的gradient_clip函数打成注释，并打印loss”，经测试，本样例中<strong><strong>顺序分区出现问题的概率远远大于随机采样</strong></strong>（约99% vs 1%），而由于本样例只是tiny example，而<strong><strong>其他很多情况下没有gradient_clip 会导致loss变成nan</strong></strong>，原因不赘述。</p> 
<h4 id="1.3%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88API%EF%BC%89" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.3 代码实现（API）</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">通过1.2自购建的方式可以实现不同方案/策略的RNN，但无论是代码实现难度、效率/性能都不是最优选择，由于RNN类模型是经典模型，故Tensorflow、Pytorch等主流框架中均做了定义（API）与优化，便于我们快速搭建模型并应用，在本部分做一下探究。</p> 
<p style="margin-left:.0001pt;text-align:justify;">通过API的代码实现非常简洁，全流程为<strong><strong>数据集读入-&gt;模型定义/引入（通过API）-&gt;训练与预测</strong></strong>。代码核心即模型的引入，如Code5所示，而用于控制与管理函数的RNNModel类定义与自购建RNN中的RNNModelScratch类似，这里不赘述，完整代码见RNN（API）.py。</p> 
<pre><code class="language-python">rnn_layer = nn.RNN(len(vocab), num_hiddens)</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：这里只包含隐藏的循环层，输出层需要单独创建。</p> 
<p style="margin-left:.0001pt;text-align:justify;">完成代码编写后，进入模型测试，数据集和参数与表8一致，结果样例如图21所示：</p> 
<p style="text-align:center;"><img alt="" height="98" src="https://images2.imgbox.com/11/0c/C3lWtJx9_o.png" width="696"><img alt="" height="273" src="https://images2.imgbox.com/42/64/noO3psLi_o.png" width="405"> </p> 
<p style="margin-left:.0001pt;text-align:center;">图21 RNN（API）结果样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如图21，<strong><strong>经过500次epoch，最终困惑度为1.0，在个人cpu上速度为28759.6词元/秒</strong></strong>，验证了API模型设计与代码实现的合理性与正确性。且比较表9，<strong><strong>API实现在实现难度、速度与性能上均优于自购建RNN</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">为验证RNN（API）与自购建RNN的速度与性能对比，使用两种RNN<strong><strong>均做20次实验</strong></strong>，取<strong><strong>平均词元/秒与平均困惑度</strong></strong>分别作为度量指标，结果数据汇总于表10，效果对比如图22：</p> 
<p style="margin-left:.0001pt;text-align:center;">表10 RNN（API） vs 自购建RNN（数据汇总）</p> 
<table align="center" cellspacing="0" style="width:246.05pt;"><tbody><tr><td style="width:160px;"> <p style="margin-left:.0001pt;text-align:center;"></p> </td><td style="width:75px;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td></tr><tr><td style="width:160px;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">自购建RNN（顺序）</span></p> </td><td style="width:75px;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.26</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">24701.5</span></p> </td></tr><tr><td style="width:160px;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">自购建RNN（随机）</span></p> </td><td style="width:75px;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.43</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">23078.2</span></p> </td></tr><tr><td style="width:160px;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">RNN（API）</span></p> </td><td style="width:75px;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.03</span></span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">28992.3</span></span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="578" src="https://images2.imgbox.com/2a/91/Kj9bI242_o.png" width="962"></p> 
<p style="margin-left:.0001pt;text-align:center;">图22 RNN（API） vs 自购建RNN（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据表10与图22，我们可以看到<strong><strong>RNN（API）的困惑度均低于两种自购建RNN，且词元/秒也是最高</strong></strong>，故在本数据集&amp;本参数组合下可<strong><strong>验证RNN（API）全方位相对与自购建RNN的优越性</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">至此，不同方案、不同实现的RNN代码解析与测试结束，总体来说，使用API提供的RNN是省时省力的较优选择，但除了表8中的默认参数选择（自拟）与RNN的基本实现，仍有较大的探索和优化空间，在下两部分着重解析。</p> 
<h4 id="1.4%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.4 消融实验</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">前面两部分无论是自购建RNN（顺序分区与随机抽取）还是RNN（API），均是基于表8的参数，但根据实验2我们知道<strong><strong>参数的选择对同样的模型在同样数据集的效果有很大影响</strong></strong>，常见的超参数总结于表11：</p> 
<p style="margin-left:.0001pt;text-align:center;">表11 重要/常见超参数总结</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">1、损失函数：</p> <p style="margin-left:.0001pt;text-align:justify;">损失可以衡量模型的预测值和真实值的不一致性，由一个非负实值函数损失函数定义</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">2、优化器：</p> <p style="margin-left:.0001pt;text-align:justify;">为使损失最小，定义loss后可根据不同优化方式定义对应的优化器</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">3、epoch：</p> <p style="margin-left:.0001pt;text-align:justify;">学习回合数，表示整个训练过程要遍历多少次训练集</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">4、学习率：</p> <p style="margin-left:.0001pt;text-align:justify;">学习率描述了权重参数每次训练之后以多大的幅度（step）沿梯下降的方向移动</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">5、归一化：</p> <p style="margin-left:.0001pt;text-align:justify;">    在训练神经神经网络中通常需要对原始数据进行归一化，以提高网络的性能</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">6、Batchsize：</p> <p style="margin-left:.0001pt;text-align:justify;">每次计算损失loss使用的训练数据数量</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="text-align:justify;">7、网络超参数：</p> <p style="margin-left:.0001pt;text-align:justify;">    包括输入图像的大小，各层的超参数（卷积核数、尺寸、步长，池化尺寸、步长、方法，激活函数等）</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">而对于循环神经网络则主要关注<strong><strong>num_hiddens</strong></strong>。接下来我们进行一些消融实验来探究参数选择对RNN的影响。</p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：消融实验构造的RNN全部基于API。</p> 
<h4 id="1.4.1%20num_hiddens" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.4.1 num_hiddens</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">num_hiddens即隐藏层数量，是影响RNN效果的重要超参数，我们保持表8中其他参数不变，仅改变num_hiddens，每个取值进行20组实验取平均值，探究困惑度与词元/秒的变化趋势，数据汇总于表12，效果对比如图23：</p> 
<p style="margin-left:.0001pt;text-align:center;">表12 num_hiddens对RNN的影响（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:410.65pt;"><tbody><tr><td style="width:79.55pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">num_hiddens</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">32</span></p> </td><td style="width:60.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">64</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">128</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">256</span></p> </td><td style="width:51.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">512</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1024</span></p> </td></tr><tr><td style="width:79.55pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">5.21</span></p> </td><td style="width:60.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">3.69</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.98</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.28</span></p> </td><td style="width:51.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.03</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.02</span></span></p> </td></tr><tr><td style="width:79.55pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">289066.5</span></span></p> </td><td style="width:60.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">182857.1</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">160003.4</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">84529.1</span></p> </td><td style="width:51.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">28992.3</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">9542.2</span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="349" src="https://images2.imgbox.com/0f/5c/hkc5xGzU_o.png" width="1108"></p> 
<p style="margin-left:.0001pt;text-align:center;">图23 num_hiddens对RNN的影响（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如表12与图23，我们可以发现随着<strong><strong>困惑度随num_hiddens增大不断减少至较稳定（效果变好），而词元/秒则逐渐减小（效率降低）</strong></strong>，故如何做好速度和性能的平衡或取舍可通过num_hiddens的选择来决定，而表8中512的num_hiddens是一个不错的选择。</p> 
<h4 id="1.4.2%20num_steps" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.4.2 num_steps</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">num_hiddens即小批量数据时间步，我们保持表8中其他参数不变，仅改变num_steps，每个取值进行20组实验取平均值，探究困惑度与词元/秒的变化趋势，数据汇总于表13，效果对比如图24：</p> 
<p style="margin-left:.0001pt;text-align:center;">表13 num_steps对RNN的影响（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.55pt;"><tbody><tr><td style="width:75.05pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">num_steps</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">15</span></p> </td><td style="width:57pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">20</span></p> </td><td style="width:55.5pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">25</span></p> </td><td style="width:52.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">30</span></p> </td><td style="width:53.9pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">35</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">40</span></p> </td></tr><tr><td style="width:75.05pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.5</span></p> </td><td style="width:57pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.42</span></p> </td><td style="width:55.5pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.3</span></p> </td><td style="width:52.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.21</span></p> </td><td style="width:53.9pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.03</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.02</span></span></p> </td></tr><tr><td style="width:75.05pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">28070.8</span></p> </td><td style="width:57pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">21622.1</span></p> </td><td style="width:55.5pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">24836.2</span></p> </td><td style="width:52.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">23272.4</span></p> </td><td style="width:53.9pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">28992.3</span></span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">26157.7</span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="347" src="https://images2.imgbox.com/41/1f/ySMItjVp_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图24 num_steps对RNN的影响（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如表13与图24，我们可以发现随着<strong><strong>困惑度随num_steps增大不断减少至较稳定（效果变好），而词元/秒则比较波动，没有明显规律</strong></strong>，故选择一个较高的num_steps可以取得比较好的性能，而表8中25的num_steps是一个不错的选择。</p> 
<h4 id="1.4.3%20batch_size" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.4.3 batch_size</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">batch_size与num_steps类似，即同时处理数据的RNN数，我们保持表8中其他参数不变，仅改变batch_size，每个取值进行20组实验取平均值，探究困惑度与词元/秒的变化趋势，数据汇总于表14，效果对比如图25：</p> 
<p style="margin-left:.0001pt;text-align:center;">表14 batch_size对RNN的影响（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:419.25pt;"><tbody><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">batch_size</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">16</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">32</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">64</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">128</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">256</span></p> </td></tr><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">13.7</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.03</span></span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.1</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.38</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">6.11</span></p> </td></tr><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">14027.9</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">28992.3</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">31438.9</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">53808.4</span></span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">47510.8</span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="348" src="https://images2.imgbox.com/e5/c4/dYnyCDvm_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图25 batch_size对RNN的影响（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如表14与图25，我们发现随着<strong><strong>困惑度随batch_size增大不断减少再增加（效果变好再变差），而词元/秒则是上升后再下降</strong></strong>，故batch_size的选择对速度和性能都有很大影响，实际情况下一般要经过多轮测试选择，而表8中32的batch_size是一个性能最优选。</p> 
<h4 id="1.4.4%20lr" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.4.4 lr</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">lr即学习率，决定模型的收敛/迭代速度，我们保持表8中其他参数不变，仅改变lr，每个取值进行20组实验取平均值，探究困惑度与词元/秒的变化趋势，数据汇总于表15，效果对比如图26：</p> 
<p style="margin-left:.0001pt;text-align:center;">表15 lr对RNN的影响（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:419.25pt;"><tbody><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">lr</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">0.01</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">0.1</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">0.25</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">0.5</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1</span></p> </td></tr><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">13.3</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">3.52</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.19</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.11</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.03</span></span></p> </td></tr><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">27826.5</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">27234.4</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">28626.6</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">27655.1</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">28992.3</span></span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="347" src="https://images2.imgbox.com/63/11/ke3l23BE_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图26 lr对RNN的影响（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如表5与图26，我们可以发现随着<strong><strong>困惑度随lr增大不断减少至稳定（效果变好，但小lr很有可能是因为未收敛），而词元/秒则是较为波动，无明显规律</strong></strong>，但lr的选择是一门“玄学”，本消融实验也仅作思路的参考，而表8中1的lr是一个较优选。</p> 
<h4 id="1.4.5%20epoch" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.4.5 epoch</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">epoch即整个训练过程要遍历多少次训练集，在以上四个消融实验中大部分模型已收敛（困惑度/loss无明显变化），困惑度随epoch的变化如图27所示，而对于lr中较大的困惑度取值去测试原因，选取lr=0.01，将epoch改为1500，效果如图28所示：</p> 
<p class="img-center"><img alt="" height="268" src="https://images2.imgbox.com/5a/74/Uh5kxIJr_o.png" width="410"></p> 
<p style="margin-left:.0001pt;text-align:center;">图27 epoch对困惑度的影响</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如图27所示，困惑度随着epoch增加而不断降低至稳定（收敛），与其他深度模型保持一致。</p> 
<p class="img-center"><img alt="" height="276" src="https://images2.imgbox.com/db/70/Lj8quMVq_o.png" width="387"></p> 
<p style="margin-left:.0001pt;text-align:center;">图28 lr=0.01，epoch=1500测试样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如图28，在测试样例中，epoch=1500困惑度仍维持在9.1，可见lr对收敛速度的影响，也侧面证实了lr的选择对模型效果的影响。</p> 
<h4 id="1.4.6%20%E7%BB%BC%E8%BF%B0" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.4.6 综述</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">前5部分我们分别对不同的五个超参做了消融实验，除此之外我们还可以改变vacab、激活函数（如变成ReLU）等去探究该参数对RNN的影响，方法类似就不展开了。根据分析结果再回顾表8中的参数组合，总体来说还是<strong><strong>兼顾了速度与性能的一组参数</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">调参的理由与本身对模型的影响有关，如学习率/Batch_size决定了迭代求解的速度与步幅，需要多次测试取较优值，而num_hiddens与模型原理息息相关，需要结合对应架构选择。但总的来说，<strong><strong>没有永恒合适的最优参数组合</strong></strong>，需根据数据集、任务、模型动态测试与调节。</p> 
<h3 id="2.LSTM" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.LSTM</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">在本章第一节我们从原理、代码实现（自购建与API）、参数调节与优化三方面深度剖析了RNN的经典网络，但<strong><strong>正如LeNet基于CNN，只了解最经典的架构意义有限，创新性高但存在较大局限性</strong></strong>（在各种如今的现实应用场景下），故接下来我们要进行现代循环神经网络的解析与代码实现，首先是LSTM（长短期记忆网络）。</p> 
<h4 id="2.1%20%E5%8E%9F%E7%90%86" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.1 原理</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">参考论文：<a href="https://papers.baulab.info/Hochreiter-1997.pdf" rel="nofollow" title="LST-1997(baulab.info)">LST-1997(baulab.info)</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">参考论文：<a href="https://arxiv.org/pdf/1506.04214.pdf" rel="nofollow" title="LSTM.pdf (arxiv.org)">LSTM.pdf (arxiv.org)</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">参考博客：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow" title="Understanding LSTM Networks -- colah's blog">Understanding LSTM Networks -- colah's blog</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">我们在前一节一直在谈“<strong><strong>梯度爆炸与梯度消失</strong></strong>”的问题，同时长期以来隐变量模型存在着<strong><strong>长期信息保存和短期输入缺失</strong></strong>的问题，最早的解决方案就是<strong><strong>长短期存储器LSTM</strong></strong>。</p> 
<p style="margin-left:.0001pt;text-align:justify;">首先让我们从LSTM的角度回顾RNN，它是一种<strong><strong>短期记忆</strong></strong>的模型，一个例子如图29，即：</p> 
<p style="margin-left:.0001pt;text-align:justify;">-  RNN中梯度更新小的layer停止学习</p> 
<p style="margin-left:.0001pt;text-align:justify;">－ 比如较早的层</p> 
<p style="margin-left:.0001pt;text-align:justify;">－ 序列越长，丢失的记忆越多</p> 
<p class="img-center"><img alt="" height="359" src="https://images2.imgbox.com/d6/e1/tYF3O8q5_o.png" width="581"></p> 
<p style="margin-left:.0001pt;text-align:center;">图29 RNN局限（短期记忆）</p> 
<p style="margin-left:.0001pt;text-align:justify;">故LSTM顾名思义，引入了长期记忆，在架构方面添加了<strong><strong>记忆元（单元）、几种用于控制状态的门（输入、忘记、输出）</strong></strong>，设计灵感来源于计算机的<strong><strong>逻辑门</strong></strong>，RNN与LSTM的架构对比可见图30：</p> 
<p class="img-center"><img alt="" height="386" src="https://images2.imgbox.com/63/18/kOdYgnJF_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图30 RNN vs LSTM（架构对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图30，我们可以看出：</p> 
<ol><li style="text-align:justify;">传统 RNN 神经元默认接受上一时刻的隐藏状态 ht-1 和当前输入 xt。</li><li style="text-align:justify;">LSTM的神经元在此基础上还输入了一个 cell 状态 ct-1，cell 状态 c 和RNN中的隐藏状态 h 类似，都<strong><strong>保存了历史的信息</strong></strong>，从ct-2 ~ ct-1 ~ ct。LSTM 中的 h 更多地是<strong><strong>保存上一时刻的输出信息</strong></strong>。</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;">让我们聚焦LSTM的模型，首先其<strong><strong>核心思想是记忆元（单元），也称细胞状态</strong></strong>，类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。同时如上文所述，通过精心设计的称作为<strong><strong>“门”结构</strong></strong>来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作，如图31所示：</p> 
<p class="img-center"><img alt="" height="493" src="https://images2.imgbox.com/aa/55/8TA86WNR_o.png" width="1108"></p> 
<p style="margin-left:.0001pt;text-align:center;">图31 LSTM核心思想与结构</p> 
<p style="margin-left:.0001pt;text-align:justify;">而LSTM的总体流程与门设计简介总结于图表1：</p> 
<p style="margin-left:.0001pt;text-align:center;">图表1 LSTM总体流程及门设计简介</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>输入</strong></strong>：将数据集导入模型</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="text-align:justify;"><strong><strong>1、Sigmiod层</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">    输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”。通过三个门来保护和控制细胞状态</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">2、<strong><strong>遗忘门（LSTM-1）</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">决定我们从“细胞”中丢弃什么信息。该层读取当前输入x和前神经元信息h，由ft来决定丢弃的信息。输出结果1表示“完全保留”，0 表示“完全舍弃”。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="328" src="https://images2.imgbox.com/c8/2e/Kb2FNzRD_o.png" width="1061"></p> <p></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">3、<strong><strong>输入门（LSTM-2）</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">确定细胞状态所存放的新信息，这一步由两层组成。sigmoid层作为“输入门层”，决定我们将要更新的值i；tanh层来创建一个新的候选值向量ct~加入到状态中。在语言模型的例子中，我们希望增加新的主语到细胞状态中，来替代旧的需要忘记的主语。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="312" src="https://images2.imgbox.com/fa/ff/vkThBZdW_o.png" width="1010"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">4、<strong><strong>输出门（LSTM-3）</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">更新旧细胞的状态，将ct-1更新为ct。我们把旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息。接着加上it*ct~。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的信息并添加新的信息的地方。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="327" src="https://images2.imgbox.com/ef/80/pnvAXpRd_o.png" width="1057"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">5、<strong><strong>输出确定/候选记忆元</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:left;">最后一步要确定输出，这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。在语言模型的例子中，因为语境中有一个代词，可能需要输出与之相关的信息。例如，输出判断是一个动词，那么我们需要根据代词是单数还是负数，进行动词的词形变化。</p> <p style="margin-left:.0001pt;text-align:left;"><img alt="" height="337" src="https://images2.imgbox.com/2d/85/cmoDvi0Z_o.png" width="1091"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>输出</strong></strong>：处理后的数据/预测数据</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">至此，我们对LSTM的原理/结构实现就比较清楚了，当然LSTM有很多变体，常见的几个总结于图32（GRU后续详解，其他不展开）：</p> 
<p class="img-center"><img alt="" height="514" src="https://images2.imgbox.com/ad/0d/dpcDYMCe_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图32 LSTM常见变体架构</p> 
<p style="margin-left:.0001pt;text-align:justify;">通过图表1我们很容易知道LSTM的长记忆引入解决了RNN的短期局限。而对于梯度消失或爆炸的缓解原因在这里做一定补充，通过对RNN的数理推导我们知道梯度消失的原因主要是<strong><strong>梯度函数中包含一个连乘项</strong></strong>，LSTM去除的方法是<strong><strong>通过门的作用使其约等于0或1</strong></strong>，如式11，详细来说即：</p> 
<p style="text-align:justify;">门的梯度接近1时，连乘项能够保证梯度很好地在 LSTM 中传递，避免梯度消失。</p> 
<ol><li style="margin-left:.0001pt;text-align:justify;">门的梯度接近0时，即上一时刻的信息对当前时刻并没有作用，此时没必要梯度回传。</li></ol> 
<p style="text-align:center;"><img alt="\begin{array}{l} \text { remove }: \prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}} \\ \text { todo: } \frac{\partial h_{j}}{\partial h_{j-1}} \approx 0 \text { or } \frac{\partial h_{j}}{\partial h_{j-1}} \approx 1 \end{array}" class="mathcode" src="https://images2.imgbox.com/4a/5f/3HilSCG6_o.png"></p> 
<p style="margin-left:.0001pt;text-align:center;">式11 LSTM梯度问题缓解策略</p> 
<p style="margin-left:.0001pt;text-align:justify;">而关于LSTM数理部分的推导，详细可见论文，在这里给出<strong><strong>用误差信号的FULL BPTT推导</strong></strong>，网络结构总览如图33，推导集合可见式集1与式集2：</p> 
<p class="img-center"><img alt="" height="820" src="https://images2.imgbox.com/3f/2e/kXSgbPU5_o.png" width="892"></p> 
<p style="text-align:center;">图33 LSTM网络结构总览</p> 
<p style="text-align:center;"><img alt="" height="959" src="https://images2.imgbox.com/df/8c/C5rry17D_o.png" width="838"><img alt="" height="991" src="https://images2.imgbox.com/13/e9/1GheEpkn_o.png" width="913"> </p> 
<p style="margin-left:.0001pt;text-align:justify;">至此，LSTM的架构与数理逻辑部分均解析完成。</p> 
<h4 id="2.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.2 代码实现</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">本部分我们进入LSTM的代码实现，根据2.1中对LSTM原理/架构的解析，我们编写代码文件为<strong><strong>LSTM.py</strong></strong>，在此仅作核心代码的解析。其中，LSTM实现的核心流程即<strong><strong>数据集导入-&gt;参数初始化-&gt;模型定义-&gt;训练和预测</strong></strong>，同样是有自购建与API两种方法，自购建模型定义部分代码可见Code6（但实验使用API构建）：</p> 
<pre><code class="language-python"># 模型状态初始化
def init_lstm_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
#模型定义
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
     W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * torch.tanh(C)
        Y = (H @ W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H, C)</code></pre> 
<p>代码编写完成后我们进入实验/LSTM的测试，用到的参数组合与baseline（表8）基本保持一致，结果样例如图35所示： </p> 
<p style="text-align:center;"><img alt="" height="93" src="https://images2.imgbox.com/1e/1b/dvH4s0sd_o.png" width="683"><img alt="" height="271" src="https://images2.imgbox.com/91/48/6d8x0H8Y_o.png" width="384"> </p> 
<p style="margin-left:.0001pt;text-align:center;">图35 LSTM结果样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图34与图35，处理后的数据能正常进入LSTM模型，<strong><strong>经过500次epoch，最终困惑度为1.0，在个人cpu上速度为2401.5词元/秒</strong></strong>，验证了LSTM模型设计与代码实现的合理性与正确性。</p> 
<h4 id="2.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.3 消融实验</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">    类似RNN，我们也可以对LSTM的参数进行消融实验以探究不同选择对模型速度与性能的影响，本部分以num_hiddens的消融实验为例，其他探究与1.4的逻辑和步骤保持一致，在此不赘述。</p> 
<p style="margin-left:.0001pt;text-align:justify;">我们保持其他参数不变，仅改变num_hiddens，每个取值进行20组实验取平均值，探究困惑度与词元/秒的变化趋势，数据汇总于表16，效果对比如图36：</p> 
<p style="margin-left:.0001pt;text-align:center;">表16 num_hiddens对LSTM的影响（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:410.65pt;"><tbody><tr><td style="width:79.55pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">num_hiddens</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">32</span></p> </td><td style="width:60.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">64</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">128</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">256</span></p> </td><td style="width:51.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">512</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1024</span></p> </td></tr><tr><td style="width:79.55pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">4.2</span></p> </td><td style="width:60.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">2.2</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.17</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.11</span></p> </td><td style="width:51.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.02</span></span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.02</span></span></p> </td></tr><tr><td style="width:79.55pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td><td style="width:66pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">99672.4</span></span></p> </td><td style="width:60.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">47770</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">43921.7</span></p> </td><td style="width:50.6pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">14154.9</span></p> </td><td style="width:51.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">2408.6</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1989.9</span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="350" src="https://images2.imgbox.com/41/e0/HynOHliB_o.png" width="1106"></p> 
<p style="margin-left:.0001pt;text-align:center;">图36 num_hiddens对LSTM的影响（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如表16与图36，我们可以发现随着<strong><strong>困惑度随num_hiddens增大不断减少至较稳定（效果变好），而词元/秒则逐渐减小（效率降低）</strong></strong>，故如何做好速度和性能的平衡或取舍可通过num_hiddens的选择来决定，而表8中512的num_hiddens是一个不错的选择。</p> 
<p style="margin-left:.0001pt;text-align:justify;">    至此，LSTM的理论与实验部分均已解析完成。</p> 
<h3 id="3.GRU" style="margin-left:.0001pt;text-align:justify;"><strong><strong>3.GRU</strong></strong></h3> 
<h4 id="3.1%20%E5%8E%9F%E7%90%86" style="margin-left:.0001pt;text-align:justify;"><strong><strong>3.1 原理</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">参考论文：<a href="https://arxiv.org/pdf/1406.1078.pdf" rel="nofollow" title="RNN Encoder–Decoder.pdf (arxiv.org)">RNN Encoder–Decoder.pdf (arxiv.org)</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">参考论文：<a href="https://arxiv.org/pdf/1412.3555.pdf" rel="nofollow" title="GRU.pdf (arxiv.org)">GRU.pdf (arxiv.org)</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">LSTM是对RNN的经典优化，较好地缓解了梯度爆炸或消失问题，且解决了隐模型长期信息保存和短期输入缺失的问题，但LSTM也存在<strong><strong>结构复杂、效率低下</strong></strong>的问题，故一个经典变体GRU（门控循环单元）出现了，架构对比如图37所示：</p> 
<p class="img-center"><img alt="" height="638" src="https://images2.imgbox.com/79/aa/w2vZtUR5_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图37 LSTM vs GRU（架构对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">简单来说，它<strong><strong>组合了遗忘门和输入门到一个单独的“更新门”中，也合并了cell state和hidden state</strong></strong>，并且做了一些其他的改变，形成了一个<strong><strong>更加简化</strong></strong>的模型，核心流程即<strong><strong>重置门-&gt;更新门-&gt;候选隐状态-&gt;隐状态</strong></strong>，详细的计算可见图表1和图32，在此不赘述。</p> 
<p style="margin-left:.0001pt;text-align:justify;">总的来说，GRU有以下两个显著特征：</p> 
<ol><li style="text-align:justify;">重置门有助于捕获序列中的短期依赖关系</li><li style="text-align:justify;">更新门有助于捕获序列中的长期依赖关系</li></ol> 
<h4 id="3.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" style="margin-left:.0001pt;text-align:justify;"><strong><strong>3.2 代码实现</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">本部分我们进入GRU的代码实现，根据3.1中对GRU原理/架构的解析，我们编写代码文件为<strong><strong>GRU.py</strong></strong>，在此仅作核心代码的解析。其中，GRU实现的核心流程即<strong><strong>数据集导入-&gt;参数初始化-&gt;模型定义-&gt;训练和预测</strong></strong>，同样是有自购建与API两种方法，自购建模型定义部分代码可见Code7（但实验使用API构建）：</p> 
<pre><code class="language-python"># 模型状态初始化
def init_gru_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )
#模型定义
def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)
        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)
        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)
        H = Z * H + (1 - Z) * H_tilda
        Y = H @ W_hq + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)</code></pre> 
<p>代码编写完成后我们进入实验/GRU的测试，用到的参数组合与baseline（表8）基本保持一致，结果样例如图39所示： </p> 
<p style="text-align:center;"><img alt="" height="93" src="https://images2.imgbox.com/ed/b4/dKN9AFh5_o.png" width="696"><img alt="" height="262" src="https://images2.imgbox.com/f8/75/sbrJhGiZ_o.png" width="390"> </p> 
<p style="margin-left:.0001pt;text-align:center;">图39 GRU结果样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图38与图39，处理后的数据能正常进入GRU模型，<strong><strong>经过500次epoch，最终困惑度为1.0，在个人cpu上速度为4538.1词元/秒</strong></strong>，验证了GRU模型设计与代码实现的合理性与正确性。</p> 
<h4 id="3.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" style="margin-left:.0001pt;text-align:justify;"><strong><strong>3.3 消融实验</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">类似RNN与LSTM，我们也可以对GRU的参数进行消融实验以探究不同选择对模型速度与性能的影响，本部分以num_hiddens的消融实验为例，其他探究与1.4的逻辑和步骤保持一致，在此不赘述。</p> 
<p style="margin-left:.0001pt;text-align:justify;">我们保持其他参数不变，仅改变num_hiddens，每个取值进行20组实验取平均值，探究困惑度与词元/秒的变化趋势，数据汇总于表17，效果对比如图40：</p> 
<p style="margin-left:.0001pt;text-align:center;">表17 num_hiddens对GRU的影响（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:410.65pt;"><tbody><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">num_hiddens</span></p> </td><td style="width:60.45pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">32</span></p> </td><td style="width:53.9pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">64</span></p> </td><td style="width:54.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">128</span></p> </td><td style="width:54.8pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">256</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">512</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1024</span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:60.45pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">3.7</span></p> </td><td style="width:53.9pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.7</span></p> </td><td style="width:54.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.1</span></p> </td><td style="width:54.8pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.08</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1</span></span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1</span></span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td><td style="width:60.45pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">82965.5</span></span></p> </td><td style="width:53.9pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">57806.2</span></p> </td><td style="width:54.4pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">47659.9</span></p> </td><td style="width:54.8pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">24216.3</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">4692.1</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1764.5</span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="348" src="https://images2.imgbox.com/55/3d/tNmQk7Fs_o.png" width="1108"></p> 
<p style="margin-left:.0001pt;text-align:center;">图40 num_hiddens对GRU的影响（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如表17与图40，我们可以发现随着<strong><strong>困惑度随num_hiddens增大不断减少至较稳定（效果变好），而词元/秒则逐渐减小（效率降低）</strong></strong>，故如何做好速度和性能的平衡或取舍可通过num_hiddens的选择来决定，而表8中512的num_hiddens是一个不错的选择。</p> 
<p style="margin-left:.0001pt;text-align:justify;">    至此，GRU的理论与实验部分均已解析完成。</p> 
<h3 id="4.%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90" style="margin-left:.0001pt;text-align:justify;"><strong><strong>4.对比分析</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">在前三节我们使用了不同方案（自购建、API）与不同参数组合（消融实验测试）构建了RNN、LSTM、GRU三种模型，在本节我们通过<strong><strong>对前三节消融实验的数据抽取，去对比分析三种模型的困惑度（性能）与词元/秒（速度）</strong></strong>，以<strong><strong>num_hiddens作为聚合维度</strong></strong>（样例，可换其他，逻辑一致这里不展开），数据汇总于表18与表19（分别对应困惑度对比与词元/秒对比），效果对比如图41与图42所示（逻辑同理）：</p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：对比模型均由API构建，数据集为时光机器书籍，其他参数与表8基本一致。</p> 
<p style="margin-left:.0001pt;text-align:center;">表18 模型困惑度对比分析（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:473.25pt;"><tbody><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">num_hiddens</span></p> </td><td style="width:60.45pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">32</span></p> </td><td style="width:59.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">64</span></p> </td><td style="width:49.1pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">128</span></p> </td><td style="width:54.8pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">256</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">512</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1024</span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">RNN</span></p> </td><td style="width:60.45pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">5.21</span></p> </td><td style="width:59.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">3.69</span></p> </td><td style="width:49.1pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.98</span></p> </td><td style="width:54.8pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.28</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.03</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.02</span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">LSTM</span></p> </td><td style="width:60.45pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">4.2</span></p> </td><td style="width:59.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">2.2</span></p> </td><td style="width:49.1pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.17</span></p> </td><td style="width:54.8pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.11</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.02</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1.02</span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">GRU</span></p> </td><td style="width:60.45pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">3.7</span></span></p> </td><td style="width:59.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.7</span></span></p> </td><td style="width:49.1pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.1</span></span></p> </td><td style="width:54.8pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1.08</span></span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1</span></span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">1</span></span></p> </td></tr></tbody></table> 
<p><img alt="" height="578" src="https://images2.imgbox.com/7e/db/cLQnLQKM_o.png" width="962"></p> 
<p style="margin-left:.0001pt;text-align:center;">图41 模型困惑度效果对比</p> 
<p style="margin-left:.0001pt;text-align:center;">表19 模型词元/秒对比分析（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:410.65pt;"><tbody><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">num_hiddens</span></p> </td><td style="width:57.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">32</span></p> </td><td style="width:56.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">64</span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">128</span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">256</span></p> </td><td style="width:50.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">512</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1024</span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">RNN</span></p> </td><td style="width:57.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">289066.5</span></span></p> </td><td style="width:56.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">182857.1</span></span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">160003</span></span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">84529.1</span></span></p> </td><td style="width:50.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">28992.3</span></span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">9542.2</span></span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">LSTM</span></p> </td><td style="width:57.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">99672.4</span></p> </td><td style="width:56.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">47770</span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">43921.7</span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">14154.9</span></p> </td><td style="width:50.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">2408.6</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1989.9</span></p> </td></tr><tr><td style="width:92.7pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">GRU</span></p> </td><td style="width:57.35pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">82965.5</span></p> </td><td style="width:56.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">57806.2</span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">47659.9</span></p> </td><td style="width:53.25pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">24216.3</span></p> </td><td style="width:50.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">4692.1</span></p> </td><td style="width:47.2pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1764.5</span></p> </td></tr></tbody></table> 
<p><img alt="" height="578" src="https://images2.imgbox.com/f3/81/DCgmDqs0_o.png" width="962"></p> 
<p style="margin-left:.0001pt;text-align:center;">图42 模型词元/秒效果对比</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图41、42与表18、19，我们可以从速度与性能两方面得出对比结论如下：</p> 
<ol><li style="text-align:justify;">性能：在本实验条件下，每个num_hiddens下性能均是<strong><strong>GRU &gt; LSTM &gt; RNN</strong></strong>（困惑度相反），而随着num_hiddens增大，三个模型的表现均越来越好，且性能差距越来越小。</li><li style="text-align:justify;">速度：在本实验条件下，每个num_hiddens下速度均是<strong><strong>GRU 与 LSTM &lt; RNN</strong></strong>（大部分情况下GRU速度优于LSTM），而随着num_hiddens增大，三个模型的速度均越来越低，且差距越来越小。</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;">    综述：故并不是说GRU在任何情况下都是优于传统RNN的选择（且本实验只以num_hiddens作为了聚合维度），<strong><strong>真实情况下要结合任务、数据集、算力资源等实际情况去择优选择合适的模型</strong></strong>。</p> 
<h2 id="%E4%B8%89%E3%80%81%E9%AB%98%E7%BA%A7%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%80%89%E6%8B%A9%E5%AE%9E%E7%8E%B0" style="margin-left:.0001pt;text-align:justify;"><strong><span style="color:#000000;"><strong>四、高级循环神经网络架构介绍与选择实现</strong></span></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">上一章中我们对几种经典的循环神经网络（RNN、LSTM、GRU）做了架构设计与数理推导的详解，且用了不同方式实现了几种循环神经网络，并通过消融实验的方式探究了几种超参数对模型速度与性能的影响，另外还对比分析了三种模型的优劣。</p> 
<p style="margin-left:.0001pt;text-align:justify;">而本章我们来介绍一些高级循环神经网络（本实验命名，非官方），以<strong><strong>深度循环神经网络/双向循环神经网络/编码器-解码器结构/序列到序列学习</strong></strong>为例，并选择一种实现。</p> 
<h3 id="1.%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.深度循环神经网络</strong></strong></h3> 
<h4 style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.1 原理</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">前面主要讨论的都是只有一个单向隐藏层的循环神经网络，而实际上<strong><strong>循环神经网络是可堆叠的，这就是深度循环神经网络的本质/核心</strong></strong>，一个样例如图43所示，这对层的添加、非线性的补充都是有指导意义的。而将函数依赖关系形式化，如式12，最后，输出层的计算仅基于第l个隐藏层最终的隐状态，如式13：</p> 
<p class="img-center"><img alt="" height="243" src="https://images2.imgbox.com/84/d0/cwg1N8zv_o.png" width="488"></p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：式子含义与网络迭代推理时保持一致，在此不赘述。</p> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="" height="519" src="https://images2.imgbox.com/7f/0b/bkskZlIR_o.png" width="458"></p> 
<p style="margin-left:.0001pt;text-align:center;">图43 深度循环神经网络样例</p> 
<h4 id="1.2%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.2 代码实现</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。以LSTM为例，与第二章2.2类似，唯一的区别是<strong><strong>我们指定了层的数量</strong></strong>，而不是使用单一层这个默认值，核心代码可见Code8，完整代码可见<strong><strong>DeepLSTM.Py</strong></strong>：</p> 
<pre><code class="language-python">vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_sizedevice = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)</code></pre> 
<p>代码编写完我们进入实验/多层LSTM的测试，用到的参数组合与baseline（表8）基本保持一致（num_hiddens由<strong><strong>512改为32</strong></strong>），结果样例如图45所示： </p> 
<p style="text-align:center;"><img alt="" height="73" src="https://images2.imgbox.com/0a/68/7Chb6Grl_o.png" width="584"></p> 
<p style="text-align:center;"><img alt="" height="236" src="https://images2.imgbox.com/c0/84/ozPQjBvE_o.png" width="322"></p> 
<p style="margin-left:.0001pt;text-align:center;">图45 双层LSTM结果样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图44与图45，处理后的数据能正常进入双层LSTM模型，<strong><strong>经过500次epoch，最终困惑度为2.1，在个人cpu上速度为56000.6词元/秒</strong></strong>，验证了GRU模型设计与代码实现的合理性与正确性。</p> 
<h4 id="1.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" style="margin-left:.0001pt;text-align:justify;"><strong><strong>1.3 消融实验</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">类似第二章的模型，我们也可以对深度循环神经网络的参数进行消融实验以探究不同选择对模型速度与性能的影响，而最重要的即num_layers的影响，故本部分以其消融实验为例，其他探究与第二章1.4的逻辑和步骤保持一致，在此不赘述。</p> 
<p style="margin-left:.0001pt;text-align:justify;">我们保持其他参数不变，仅改变num_layers，每个取值进行20组实验取平均值，探究困惑度与词元/秒的变化趋势，数据汇总于表20，效果对比如图46：</p> 
<p style="margin-left:.0001pt;text-align:center;">表20 num_layers对LSTM的影响（数据汇总）</p> 
<table cellspacing="0" style="margin-left:4.65pt;width:419.25pt;"><tbody><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">num_layers</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">1</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">2</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">3</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">4</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">5</span></p> </td></tr><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">困惑度</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">4.2</span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">2.09</span></span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">2.54</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">17.4</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">17.5</span></p> </td></tr><tr><td style="width:105.65pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">词元/秒</span></p> </td><td style="width:68.3pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#00ffff;"><span style="color:#000000;">99672.4</span></span></p> </td><td style="width:66.85pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">55921.8</span></p> </td><td style="width:55.15pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">38788.8</span></p> </td><td style="width:61.75pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">28000.5</span></p> </td><td style="width:52.95pt;"> <p style="margin-left:.0001pt;text-align:center;"><span style="color:#000000;">23579.3</span></p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="348" src="https://images2.imgbox.com/74/de/uvipuWED_o.png" width="1107"></p> 
<p style="margin-left:.0001pt;text-align:center;">图46 num_layers对LSTM的影响（效果对比）</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：如表20与图46，可以发现随着<strong><strong>困惑度随num_layers先减小再增大再稳定，而词元/秒则逐渐减小（效率降低）</strong></strong>，故num_layers的选择并不是越大越好，与其他超参数的选择也息息相关，需多次测试选出最优值，本实验中中2的num_layers是一个不错的选择。</p> 
<p style="margin-left:.0001pt;text-align:justify;">    至此，深度循环神经网络的理论与实验部分均已解析完成。</p> 
<h3 id="2.%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" style="margin-left:.0001pt;text-align:justify;"><strong><strong>2.双向循环神经网络</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">参考论文：<a href="https://deeplearning.cs.cmu.edu/S23/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf" rel="nofollow" title="Bidirectional Recurrent Neural Networks - (cmu.edu)">Bidirectional Recurrent Neural Networks - (cmu.edu)</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">首先来看一个例子感受一下“未来”的重要性，如表21所示：</p> 
<p style="margin-left:.0001pt;text-align:center;">表21 文本序列填空样例</p> 
<table border="1" cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">我 <span style="color:#ff0000;">饿</span> 。</p> <p style="margin-left:.0001pt;text-align:justify;">我 <span style="color:#ff0000;">不是</span> 非常饿。</p> <p style="margin-left:.0001pt;text-align:justify;">我 <span style="color:#ff0000;">非常</span> 非常饿，我可以吃下一只猪。</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">根据可获得的信息量，我们可以用不同的词填空，在本样例中，下文（“未来”）传达了重要信息/做了限制，而<strong><strong>RNN是只关注上文的</strong></strong>，在本部分存在局限，故BRNN（双向循环神经网络）出现了，架构样例如图47所示，前/反向传播更新如式14，输出如式15：</p> 
<p class="img-center"><img alt="" height="240" src="https://images2.imgbox.com/cb/5b/7ANkvEGr_o.png" width="450"></p> 
<p>Tips：式子含义与网络迭代推理时保持一致，在此不赘述。 </p> 
<p class="img-center"><img alt="" height="352" src="https://images2.imgbox.com/cf/f9/dFmxNw8w_o.png" width="600"></p> 
<p style="margin-left:.0001pt;text-align:center;">图47 BRNN架构样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">故BRNN的关键特征是<strong><strong>使用来自序列两端的信息来估计输出</strong></strong>，但<strong><strong>在预测下一个词元时这步的意义有限，且会大大降低计算速度</strong></strong>，故双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如，填充缺失的单词、词元注释（例如，用于命名实体识别） 以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。</p> 
<p style="margin-left:.0001pt;text-align:justify;">BRNN的代码实现如Code9所示，结果样例如图48：</p> 
<pre><code class="language-python">import torch
from torch import nn
from d2l import torch as d2l
# 加载数据
batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
# 通过设置“bidirective=True”来定义双向LSTM模型
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
# 训练模型
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</code></pre> 
<p>Tips：本样例为双向循环神经网络的错误应用，即用其进行序列预测。 </p> 
<p class="img-center"><img alt="" height="120" src="https://images2.imgbox.com/23/d8/AaszSGMx_o.png" width="871"></p> 
<p style="margin-left:.0001pt;text-align:center;">图48 双向LSTM结果样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">分析：根据图48，我们可以看到<strong><strong>最终预测输出的不合理性</strong></strong>，验证了双向循环神经网络在序列预测等任务上的局限性。</p> 
<h3 id="3.%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C" style="margin-left:.0001pt;text-align:justify;"><strong><strong>3.稠密连接网络</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">我们在第一节使用深度循环神经网络的时候难免会有一个问题：添加层是否可以提高准确性？如图49所示，也通过消融实验做了简单测试。在实验2中我们研究CNN的时候解析了ResNet（在此不赘述），实际上<strong><strong>深度循环神经网络是可以与残差网络组合的，即稠密连接网络</strong></strong>，架构样例如图50所示：</p> 
<p class="img-center"><img alt="" height="308" src="https://images2.imgbox.com/10/f1/DeFgZYqk_o.png" width="754"></p> 
<p style="text-align:center;"> 图49 准确性与层数关系</p> 
<p style="text-align:center;"><img alt="" height="529" src="https://images2.imgbox.com/e8/94/UhEUfSnz_o.png" width="551"></p> 
<p style="margin-left:.0001pt;text-align:center;">图50 稠密连接网络架构样例</p> 
<p style="margin-left:.0001pt;text-align:justify;">根据图50与两种相关网络定义，我们可以总结稠密连接网络主要特点如下：</p> 
<ol><li style="text-align:justify;">将前一层的输出连接为下一层的输入</li><li style="text-align:justify;">偶尔添加过渡层以减少维度</li></ol> 
<h3 id="4.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86" style="margin-left:.0001pt;text-align:justify;"><strong><strong>4.机器翻译与数据集</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">    前面的模型我们一直在以序列预测为基准任务，实际上是有点局限且乏味的，NLP作为计算机领域高速发展的领域，实际上还有许多经典的任务，比如<strong><strong>机器翻译任务，其为语言模型最成功的基准测试</strong></strong>。 因为机器翻译正是将输入序列转换成输出序列的序列转换模型的核心问题，故我们引入一下相关概念与数据集，后面的几个架构介绍都会基于机器翻译任务。</p> 
<p style="margin-left:.0001pt;text-align:justify;">机器翻译，顾名思义即指的是<strong><strong>将序列从一种语言自动翻译成另一种语言</strong></strong>，一般分为<strong><strong>统计机器翻译</strong></strong>（基于统计学方法）与<strong><strong>神经机器翻译</strong></strong>（基于神经网络）。</p> 
<p style="margin-left:.0001pt;text-align:justify;">机器翻译的数据集是<strong><strong>由源语言和目标语言的文本序列对组成</strong></strong>的。因此，我们需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。一个样例流程总结于表22（如何将预处理后的数据加载到小批量中用于训练）：</p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：本实验以Tatoeba项目的双语句子对 组成的“英－法”数据集为例，详情可见：<a href="http://www.manythings.org/anki/" rel="nofollow" title="Tab-delimited Bilingual Sentence Pairs from the Tatoeba Project(manythings.org)">Tab-delimited Bilingual Sentence Pairs from the Tatoeba Project(manythings.org)</a></p> 
<p style="margin-left:.0001pt;text-align:center;">表22 机器翻译数据集预处理</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>输入</strong></strong>：数据集</p> <p style="margin-left:.0001pt;text-align:justify;">数据集中的每一行都是制表符分隔的文本序列对， 序列对由英文文本序列和翻译后的法语文本序列组成。 请注意，每个文本序列可以是一个句子， 也可以是包含多个句子的一个段落，例如：</p> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#fafafa;"><span style="background-color:#fafafa;">Go. Va !</span></span></p> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#fafafa;"><span style="background-color:#fafafa;">Hi. Salut !</span></span></p> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#fafafa;"><span style="background-color:#fafafa;">Run!        Cours !</span></span></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">1、<strong><strong>预处理</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">    下载导入数据集后，需要经过几个预处理步骤，例如用空格代替不间断空格， 使用小写字母替换大写字母，并在单词和标点符号之间插入空格等。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">2、<strong><strong>词元化</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">与前面的词元化不同，在机器翻译中，一般更喜欢单词级词元化（最先进的模型可能使用更高级的词元化技术），例如：</p> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#fafafa;"><span style="background-color:#fafafa;">([[</span><span style="background-color:#fafafa;"><span style="color:#4070a0;">'go'</span></span><span style="background-color:#fafafa;">,</span> <span style="background-color:#fafafa;"><span style="color:#4070a0;">'.'</span></span><span style="background-color:#fafafa;">],</span></span></p> <p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#fafafa;"><span style="background-color:#fafafa;">[[</span><span style="background-color:#fafafa;"><span style="color:#4070a0;">'ça'</span></span><span style="background-color:#fafafa;">,</span> <span style="background-color:#fafafa;"><span style="color:#4070a0;">'alors'</span></span><span style="background-color:#fafafa;">,</span> <span style="background-color:#fafafa;"><span style="color:#4070a0;">'!'</span></span><span style="background-color:#fafafa;">]])</span></span></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;">3、<strong><strong>词表构建与加载数据集</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">分别为源语言和目标语言构建两个词表，并通过截断和填充方式实现一次只处理一个小批量的文本序列。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>输出</strong></strong>：将处理好的数据输出到模型，进行训练。</p> </td></tr></tbody></table> 
<h3 id="5.%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84" style="margin-left:.0001pt;text-align:justify;"><strong><strong>5.编码器-解码器架构</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">    Encoder-Decoder（编码器-解码器）是<strong><strong>深度学习模型的抽象概念</strong></strong>，一般认为<strong><strong>很多模型均起源/共同表征于这个架构</strong></strong>，包括但不限于CNN、RNN、Transformer，广义架构如图51：</p> 
<p class="img-center"><img alt="" height="218" src="https://images2.imgbox.com/68/be/C0Vcgqjj_o.png" width="961"></p> 
<p style="margin-left:.0001pt;text-align:center;">图51 编码器-解码器广义架构</p> 
<p style="margin-left:.0001pt;text-align:justify;">根据图51，很容易归纳出其架构的两个核心：</p> 
<ol><li style="text-align:justify;">编码器（Encoder）：负责将输入（Input）转化为特征（Feature）</li><li style="text-align:justify;">解码器（Decoder）：负责将特征（Feature）转化为目标（Target）</li></ol> 
<p style="margin-left:.0001pt;text-align:justify;">而我们提到很多模型可以在这个架构下共同表征，以CNN和RNN为例，如图52所示，它们的简单理解如下：</p> 
<ol><li style="text-align:justify;">CNN可以认为是解码器可以不接受输入的情况</li><li style="text-align:justify;">RNN可以认为是解码器同时接受输入的情况</li></ol> 
<p class="img-center"><img alt="" height="796" src="https://images2.imgbox.com/72/a8/COLEc0rl_o.png" width="822"></p> 
<p style="margin-left:.0001pt;text-align:center;">图52 CNN vs RNN（Encoder-Decoder）</p> 
<p style="margin-left:.0001pt;text-align:justify;">让我们的视角聚焦回RNN，第四节我们说到，机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列，故编码器-解码器架构是一个不错的选择，编码器<strong><strong>接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态</strong></strong>。解码器<strong><strong>将固定形状的编码状态映射到长度可变的序列</strong></strong>。代码实现分别如Code10与Code11所示：</p> 
<pre><code class="language-python">class Encoder(nn.Module):
    def __init__(self, **kwargs):
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args):
        raise NotImplementedError</code></pre> 
<p></p> 
<pre><code class="language-python">class Decoder(nn.Module):
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)

    def init_state(self, enc_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError
model = model.to(device)</code></pre> 
<h3 id="6.%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0" style="margin-left:.0001pt;text-align:justify;"><strong><strong>6.序列到序列学习</strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">参考论文：<a href="https://arxiv.org/pdf/1409.3215.pdf" rel="nofollow" title="Sequence to Sequence Learning with Neural Networks 14 Dec 2014">Sequence to Sequence Learning with Neural Networks 14 Dec 2014</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">上一节我们解析了编码器-解码器架构，其会启发人们使用具有状态的神经网络。本节我们来讲讲一个<strong><strong>使用循环神经网络设计基于“编码器－解码器”架构的序列转换模型——seq2seq（序列到序列学习）</strong></strong>。样例架构如图53所示，其中的层如图54所示：</p> 
<p class="img-center"><img alt="" height="280" src="https://images2.imgbox.com/1c/fa/ReuBBVZf_o.png" width="888"></p> 
<p style="margin-left:.0001pt;text-align:center;">图53 RNN编码器-解码器的序列到序列学习架构样例</p> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="" height="346" src="https://images2.imgbox.com/33/c5/6p4vMp42_o.png" width="610"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:center;">图54 循环神经网络编码器-解码器模型中的层</p> 
<p>如图54，序列到序列学习的核心是<strong><strong>一旦输出序列生成此词元，模型就会停止预测</strong></strong>。对于训练效果的度量，可以引入<strong><strong>常规的loss</strong></strong>（softmax来获得分布，并通过计算交叉熵损失函数来进行优化）而对于预测序列的评估，我们可以通过<strong><strong>与真实的标签序列进行比较来评估预测序列，即使用BLEU测量许多应用的输出序列的质量</strong></strong>。原则上说，对于预测序列中的任意n元语法， BLEU的评估都是这个n元语法是否出现在标签序列中，如式16： </p> 
<p style="text-align:center;"><img alt="\exp \left(\min \left(0,1-\frac{\text { len }_{\text {label }}}{\text { len }_{\text {pred }}}\right) \prod_{n=1}^{k} p_{n}^{1 / 2^{n}}\right." class="mathcode" src="https://images2.imgbox.com/46/d8/ZFbG6MrC_o.png"></p> 
<p style="margin-left:.0001pt;text-align:center;">式16 BLEU定义</p> 
<p style="margin-left:.0001pt;text-align:justify;">Tips：其中lenlabel表示标签序列中的词元数和lenpred表示预测序列中的词元数，k是用于匹配的最长的n元语法。 另外，用pn表示n元语法的精确度。</p> 
<p style="margin-left:.0001pt;text-align:justify;">在代码实现方面，详情可见<strong><strong>seq2seq.py</strong></strong>，本部分仅作核心代码解析。首先是编码器与解码器的设计，核心与Code10、Code11保持一致（需扩展），而在训练与模型初始化部分，代码如Code12，两个结果样例分别如图55与图56（训练与预测）：</p> 
<pre><code class="language-python">#@save 训练
def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
    """训练序列到序列模型"""
    def xavier_init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight)
        if type(m) == nn.GRU:
            for param in m._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(m._parameters[param])

    net.apply(xavier_init_weights)
    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = MaskedSoftmaxCELoss()
    net.train()
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                     xlim=[10, num_epochs])
    for epoch in range(num_epochs):
        timer = d2l.Timer()
        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量
        for batch in data_iter:
            optimizer.zero_grad()
            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]
            bos = torch.tensor([tgt_vocab['&lt;bos&gt;']] * Y.shape[0],
                          device=device).reshape(-1, 1)
            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学
            Y_hat, _ = net(X, dec_input, X_valid_len)
            l = loss(Y_hat, Y, Y_valid_len)
            l.sum().backward()      # 损失函数的标量进行“反向传播”
            d2l.grad_clipping(net, 1)
            num_tokens = Y_valid_len.sum()
            optimizer.step()
            with torch.no_grad():
                metric.add(l.sum(), num_tokens)
        if (epoch + 1) % 10 == 0:
            animator.add(epoch + 1, (metric[0] / metric[1],))
    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
        f'tokens/sec on {str(device)}')
#模型初始化
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()
train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)net = d2l.EncoderDecoder(encoder, decoder)
train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</code></pre> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="" height="576" src="https://images2.imgbox.com/65/44/8HHCO0ax_o.png" width="832"></p> 
<p style="text-align:center;">图55 seq2seq用于机器翻译的结果样例（训练） </p> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="" height="224" src="https://images2.imgbox.com/20/29/DyRCH9uV_o.png" width="884"></p> 
<p style="margin-left:.0001pt;text-align:center;">图56 seq2seq用于机器翻译的结果样例（预测）</p> 
<p style="margin-left:.0001pt;text-align:justify;">类似前面所有RNN模型，seq2seq同样可以进行消融实验探究不同参数组合对模型效果的影响，逻辑与步骤在上文已详细分析，在此不赘述。</p> 
<p style="margin-left:.0001pt;text-align:justify;">至此，大部分主流RNN模型架构均已解析完成。</p> 
<h2 id="%E4%BA%94%E3%80%81%E6%80%BB%E7%BB%93" style="margin-left:.0001pt;text-align:justify;">五、总结</h2> 
<h3 id="1.%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA" style="margin-left:.0001pt;text-align:justify;">1.实验结论</h3> 
<p style="margin-left:.0001pt;text-align:justify;">本次实验完成任务梳理如表23，不同RNN简介与对比如图表2所示：</p> 
<p style="margin-left:.0001pt;text-align:center;">表23 实验3完成任务梳理</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>1、理论梳理：</strong></strong></p> <p style="margin-left:.0001pt;text-align:justify;">    第一章进行了循环神经网络的综述（背景、概念、原理、发展历程）与RNN训练的基本原理与流程介绍，在第二章中按时间线从架构与数理两部分对RNN（tanh）、LSTM、GRU进行了解析。在第三章总结介绍了一些其他的循环神经网络及其优化。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>2、多种RNN实践与优化</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">    从自构建与API两种方式对比实现了RNN，并对RNN、LSTM、GRU均进行了不同参数的消融实验，定量探究对应参数与架构设计对模型速度与性能的影响，并对比分析了三种模型效果。在高级循环神经网络中，提出了多种优化策略，并分别选择了深度循环神经网络、双向循环神经网络、序列到序列学习进行了实践探究。</p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>3、方案补充</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">   对于RNN的高级架构实现与优化，除了给定的要求，均作了相关的拓展，比如在原理侧，详细解析了语言模型的核心前置知识（序列模型/预测、文本预处理、机器翻译等）。</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:center;">图表2 不同RNN简介与对比</p> 
<table cellspacing="0" style="margin-left:5.4pt;"><tbody><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>1、经典RNN（tanh）：</strong></strong></p> <p style="margin-left:.0001pt;text-align:justify;">广义上RNN的开山之作，通过循环将训练“学”到的东西蕴藏在权值W中，本质上是循环/递推函数。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="440" src="https://images2.imgbox.com/91/90/Rectqjyo_o.png" width="1012"></p> <p></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>2、LSTM</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">在RNN（tanh）的基础上引入了记忆元，并通过遗忘门、输入门、输出门进行状态控制，实现了长短期记忆共用，也缓解了梯度爆炸/梯度消失的问题。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="538" src="https://images2.imgbox.com/c9/ce/KfUYEOs3_o.png" width="992"></p> <p></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>3、GRU</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">GRU主要是对LSTM的简化，组合了遗忘门和输入门到一个单独的“更新门”中，也合并了cell state和hidden state，并且做了一些其他的改变。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="496" src="https://images2.imgbox.com/40/9a/xeUikFTB_o.png" width="924"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>4、深度循环神经网络</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">深度循环神经网络的核心即堆叠RNN（改变隐藏层的数量）。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="503" src="https://images2.imgbox.com/cd/a8/Sw0HdYDT_o.png" width="444"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>5、稠密连接网络</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">稠密连接网络即深度循环神经网络与残差网络的组合。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="529" src="https://images2.imgbox.com/62/18/ZZaxRKGI_o.png" width="551"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>6、双向循环神经网络</strong></strong>：</p> <p></p> <p style="margin-left:.0001pt;text-align:justify;">双向循环神经网络即同时关注上下文的RNN（使用来自序列两端的信息来估计输出），但在预测下一个词元时这步的意义有限，且会大大降低计算速度，故双向层的使用在实践中非常少，并且仅仅应用于部分场合。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="529" src="https://images2.imgbox.com/5e/ce/Q8d5YEyW_o.png" width="551"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>7、编码器-解码器结构</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">编码器-解码器是深度学习模型的抽象概念，一般认为很多模型均起源/共同表征于这个架构，对RNN即编码器接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。解码器将固定形状的编码状态映射到长度可变的序列。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="180" src="https://images2.imgbox.com/c9/db/bnCipjjy_o.png" width="760"></p> <p></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr><tr><td style="vertical-align:top;width:415.3pt;"> <p style="margin-left:.0001pt;text-align:justify;"><strong><strong>8、序列到序列学习</strong></strong>：</p> <p style="margin-left:.0001pt;text-align:justify;">序列到序列学习即用循环神经网络设计基于“编码器－解码器”架构的序列转换模型。</p> <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="280" src="https://images2.imgbox.com/10/df/pUKRYp6F_o.png" width="888"></p> <p style="margin-left:.0001pt;text-align:center;"></p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">补充：RNN的选择需要和实际需求紧密结合，并不存在某种模型/算法适用于各种数据集、任务、算力资源中。</p> 
<h3 id="2.%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><strong>2. 参考资料</strong></h3> 
<p>1.<a href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/index.html" rel="nofollow" title="8. 循环神经网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)">8. 循环神经网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p> 
<p>2.<a href="https://zhuanlan.zhihu.com/p/123211148" rel="nofollow" title="史上最详细循环神经网络讲解（RNN/LSTM/GRU） - 知乎 (zhihu.com)">史上最详细循环神经网络讲解（RNN/LSTM/GRU） - 知乎 (zhihu.com)</a></p> 
<p>3.<a href="https://www.jianshu.com/p/d35a2fd593eb" rel="nofollow" title="RNN的研究发展过程 - 简书 (jianshu.com)">RNN的研究发展过程 - 简书 (jianshu.com)</a></p> 
<p>4.<a href="https://zhuanlan.zhihu.com/p/32668465" rel="nofollow" title="从90年代的SRNN开始，纵览循环神经网络27年的研究进展 - 知乎 (zhihu.com)">从90年代的SRNN开始，纵览循环神经网络27年的研究进展 - 知乎 (zhihu.com)</a></p> 
<p>5.<a href="https://www.cnblogs.com/zhengzhicong/p/12890660.html" rel="nofollow" title="深度学习中的序列模型演变及学习笔记（含RNN/LSTM/GRU/Seq2Seq/Attention机制）">深度学习中的序列模型演变及学习笔记（含RNN/LSTM/GRU/Seq2Seq/Attention机制）</a></p> 
<p>6.<a href="https://blog.csdn.net/qq_32172681/article/details/100060263" title="循环神经网络RNN论文解读_循环神经网络论文_纸上得来终觉浅～的博客-CSDN博客">循环神经网络RNN论文解读_循环神经网络论文_纸上得来终觉浅～的博客-CSDN博客</a></p> 
<p>7.<a href="https://blog.csdn.net/bestrivern/article/details/90723524" title="RNN详解(Recurrent Neural Network)_bestrivern的博客-CSDN博客">RNN详解(Recurrent Neural Network)_bestrivern的博客-CSDN博客</a></p> 
<p>8.<a href="https://blog.csdn.net/a635661820/article/details/45390671" title="LSTM简介以及数学推导(FULL BPTT)_lstm的数学表达_a635661820的博客-CSDN博客">LSTM简介以及数学推导(FULL BPTT)_lstm的数学表达_a635661820的博客-CSDN博客</a></p> 
<p>9.<a href="https://www.jianshu.com/p/247a72812aff" rel="nofollow" title="循环神经网络 RNN、LSTM、GRU - 简书 (jianshu.com)">循环神经网络 RNN、LSTM、GRU - 简书 (jianshu.com)</a></p> 
<p>10.<a href="https://zhuanlan.zhihu.com/p/45649187" rel="nofollow" title="一份详细的LSTM和GRU图解 - 知乎 (zhihu.com)">一份详细的LSTM和GRU图解 - 知乎 (zhihu.com)</a></p> 
<p>11.<a href="https://zhuanlan.zhihu.com/p/572922549" rel="nofollow" title="深度学习：编码器-解码器架构 - 知乎 (zhihu.com)">深度学习：编码器-解码器架构 - 知乎 (zhihu.com)</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9f004b098705a5a88c32d000da7b43e0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">记一次使用replaceAll的问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/eab72da26829e81a01ee977ddd131d2a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">GoJS Beginner Tutorial #1</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>