<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>强化学习——基本概念&#43;Q表格 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="强化学习——基本概念&#43;Q表格" />
<meta property="og:description" content="基本概念 马尔可夫过程（MP）: 一个马尔科夫过程可以由一个元组组成 〈S,P〉
S 为（有限）的状态（state）集；P 为状态转移矩阵， 。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率，所以矩阵每一行元素之和为1。 马尔可夫决策过程（MDP）： 相对于MP，MDP加入了瞬时奖励 R(Immediate reward）、动作集合 A 和折扣因子 γ （Discount factor）。MDP由元组 〈S,A,P,R,γ〉 定义。其中
S 为（有限）的状态（state）集；A 为有限的动作集；P 为状态转移矩阵。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率。系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往的状态 R 为回报函数（reward function), 系统下一时刻的奖励仅由当前时刻的状态决定，不依赖于以往的状态 γ 为折扣因子，范围在[0,1]之间， 越大，说明agent看得越“远”。 回报 回报描述了从时间 t 起的总折扣奖励（区分奖励），用于衡量一个agent转移到一个state对于我们达到目的有“多有用”（how good）。即：
我们的最终的目标就是让回报的期望（expected return）最大。
状态值函数 值函数是在一个（一组）action下定义的，我们称之为策略（policy）。MDP中一个状态s在策略 π 下的值函数记为 ，它代表从s开始的回报的期望。
动作值函数（action-value function） 从状态s开始，并执行动作a，然后遵循策略 π 所获得的回报的期望
状态值函数和动作值函数的区别 这是两个概念，分别是状态价值V(s)和动作价值Q(s,a)，前者是对环境中某一个状态的价值大小做的评估，后者是对在某状态下的动作的价值大小的评估。概念类似，但主要区别应该是体现在用途以及算法上吧，比如对于离散型的动作空间，可以单纯基于动作Q值去寻优（DQN算法），如果是动作空间巨大或者动作是连续型的，那么可以判断状态价值并结合策略梯度来迭代优化（AC算法）......简单来说就是不同场景下我们会选择动作值函数或者状态值函数来寻找最优策略
贝尔曼方程 状态值函数可以分解为瞬时回报加上后续状态的折扣值，即
动作值函数也可以如此分解：
即：贝尔曼方程描述了当前状态值函数和其后续状态值函数之间的关系，即状态值函数（动作值函数）等于瞬时回报的期望加上下一状态的（折扣）状态值函数（动作值函数）的期望。
贝尔曼最优方程（Bellman Optimality Equation） 强化学习的目标是找到一个最优策略，使得回报最大。准确的说是使值函数最大，包括状态值函数和动作值函数，分别记为 和。对于任意一个MDPs，总是存在一个最优的策略 ，在使用这个策略时就能取得最优值函数，即 ， 策略分类 有模型策略：环境的状态转移概率和奖励函数已知
无模型策略：在未知的环境里，任何决策的概率函数和奖励函数是未知的。
即：在状态s下采取a行动的奖励以及转移到状态S&#39;的概率是已知的
在状态s下采取a行动的奖励以及转移到状态S&#39;的概率是无法获得的，在这种情况下，我们可以选择让主体不断与环境交互，采集大量的轨迹数据，从轨迹中获取信息来改进策略，从而获得更多的奖励。
Q表格 主体不断与环境交互的结果是，可以得到一张Q表格，它像是一本生活手册。里面的每一个Q函数（动作值函数）记录了在某个状态下选择某个动作，后续能够获得多少总奖励（回报期望）。最开始的时候表格会全部初始化为0。智能体会不断和环境交互得到不同的轨迹，当交互的次数足够多的时候，我们就可以估算出每一个状态下，每个动作的平均总奖励，进而更Q表格，表格的更新就是强化的过程。 强化是指我们可以用下一个状态的价值来更新当前状态的价值，其实就是强化学习里面自举的概念。
例子：悬崖行走问题 游戏规则：智能体从出发点S 出发，到达目的地G，同时避免掉进悬崖（cliff），每走一步就有−1 分的惩罚，掉进悬崖会有−100 分的惩罚。掉进悬崖游戏不会结束，智能体会回到出发点，游戏继续，直到到达目的地结束游戏。智能体需要尽快地到达目的地，为了到达目的地，智能体可以沿着例如蓝线和红线的路线行走。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/28ee22eb9bfc705e29d45a2419feeb42/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-22T22:30:13+08:00" />
<meta property="article:modified_time" content="2022-11-22T22:30:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">强化学习——基本概念&#43;Q表格</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="margin-left:0in;"> 
 <div style="margin-left:0in;"> 
  <h2 style="margin-left:0px;">基本概念</h2> 
  <h4 style="margin-left:0px;">马尔可夫过程（MP）:</h4> 
  <p>一个马尔科夫过程可以由一个元组组成 〈S,P〉</p> 
  <ul><li>S 为（有限）的状态（state）集；</li><li>P 为状态转移矩阵， <img alt="Ps{s}'=P[S_{t+1}={s}'|S_{t}=s]" class="mathcode" src="https://images2.imgbox.com/1e/3d/7QHQPDGq_o.png">。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率，所以矩阵每一行元素之和为1。</li></ul> 
  <h4 style="margin-left:0px;">马尔可夫决策过程（MDP）：</h4> 
  <p style="margin-left:0;">      相对于MP，MDP加入了瞬时奖励 R(Immediate reward）、动作集合 A 和折扣因子 γ （Discount factor）。MDP由元组 〈S,A,P,R,γ〉 定义。其中</p> 
  <ul><li>S 为（有限）的状态（state）集；</li><li>A 为有限的动作集；</li><li>P 为状态转移矩阵。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率。系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往的状态</li></ul> 
  <p>                <img alt="Ps{s}'=P[S_{t+1}={s}'|S_{t}=s,A_{t}=a]" class="mathcode" src="https://images2.imgbox.com/e5/8e/SqiKJuBv_o.png"></p> 
  <ul><li>R 为回报函数（reward function), 系统下一时刻的奖励仅由当前时刻的状态决定，不依赖于以往的状态</li></ul> 
  <p>                <img alt="R_{s}^{a} = E\left [ R_{t}|S_{t}=s, A_{t}=a \right ]" class="mathcode" src="https://images2.imgbox.com/87/db/Tw5egmXA_o.png"></p> 
  <ul><li>γ 为折扣因子，范围在[0,1]之间， 越大，说明agent看得越“远”。</li></ul> 
  <h4 style="margin-left:0px;">回报</h4> 
  <p>      回报描述了从时间 t 起的总折扣奖励<span style="color:#1a439c;"><span style="background-color:#ffd900;">（区分奖励），</span></span>用于衡量一个agent转移到一个state对于我们达到目的有“多有用”（how good）。即：</p> 
  <p>        <img alt="G _{t}=r _{t+1} +\gamma r_{t+2} + ... +\gamma^{n} r_{t+n}=r_{t+1}+\gamma G_{t+1}" class="mathcode" src="https://images2.imgbox.com/4c/6f/6iymDtB0_o.png"></p> 
  <p>        我们的最终的目标就是让回报的<span style="background-color:#ffd900;">期望</span>（expected return）最大。</p> 
  <h4 style="margin-left:0px;">状态值函数</h4> 
  <p>     值函数是在一个<span style="background-color:#ffd900;">（一组）action</span>下定义的，我们称之为<span style="background-color:#ffd900;">策略</span>（policy）。MDP中一个状态s在策略 π 下的值函数记为<img alt="V_{\pi }(s)" class="mathcode" src="https://images2.imgbox.com/4c/60/qAjfEYVm_o.png"> ，它代表从s开始的<span style="color:#1a439c;"><strong>回报的期望</strong></span>。</p> 
  <p>        <img alt="v_{\pi }(s)=E_{\pi }[G_{t}|S_{t}=s]" class="mathcode" src="https://images2.imgbox.com/8b/ad/21YrzTdm_o.png"></p> 
  <h4>动作值函数（action-value function）</h4> 
  <p> 从状态s开始，<span style="color:#1a439c;"><strong>并执行动作a</strong></span>，然后遵循策略 π 所获得的回报的<span style="color:#1a439c;"><strong>期望</strong></span></p> 
  <p>        <img alt="q_{\pi }(s,a)=E_{\pi } [G_{t}|S_{t}=s, A_{t}=a]" class="mathcode" src="https://images2.imgbox.com/1b/85/LNp44hzN_o.png"></p> 
  <h4>状态值函数和动作值函数的区别</h4> 
  <p><em>      </em>这是两个概念，分别是<span style="background-color:#ffd900;">状态价值</span>V(s)和<span style="background-color:#ffd900;">动作价值</span>Q(s,a)，前者是对环境中某一个状态的价值大小做的评估，后者是对在某状态下的动作的价值大小的评估。概念类似，但主要区别应该是体现在用途以及算法上吧，比如对于离散型的动作空间，可以单纯基于动作Q值去寻优（DQN算法），如果是动作空间巨大或者动作是连续型的，那么可以判断状态价值并结合策略梯度来迭代优化（AC算法）<em>......</em>简单来说就是<strong><span style="color:#1a439c;">不同场景下我们会选择动作值函数或者状态值函数来寻找最优策略</span></strong></p> 
  <h4 style="margin-left:0px;">贝尔曼方程</h4> 
  <p>状态值函数可以分解为<strong>瞬时回报</strong>加上后续状态的折扣值，即</p> 
  <p>        <img alt="v_{\pi }(s)=E_{\pi }[R_{t+1}+\gamma v_{\pi }(S_{t+1})|S_{t}=s]" class="mathcode" src="https://images2.imgbox.com/7d/5a/rqPjLgl9_o.png"></p> 
  <p>动作值函数也可以如此分解：</p> 
  <p>        <img alt="q{_\pi}(s,a)=E_\pi[R_{t+1}+\gamma q{_\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a]" class="mathcode" src="https://images2.imgbox.com/20/c0/8gG3g3Fj_o.png"></p> 
  <p>即：贝尔曼方程描述了当前状态值函数和其后续状态值函数之间的关系，即状态值函数（动作值函数）等于瞬时回报的期望加上下一状态的（折扣）状态值函数（动作值函数）的期望。</p> 
  <h4><strong>贝尔曼最优方程（Bellman Optimality Equation）</strong></h4> 
  <p>强化学习的目标是找到一个最优策略，使得回报最大。准确的说是使值函数最大，包括状态值函数和动作值函数，分别记为 <img alt="v_{\ast }(s)" class="mathcode" src="https://images2.imgbox.com/ad/97/dghWBcC6_o.png">和<img alt="q_{\ast }(s,a)" class="mathcode" src="https://images2.imgbox.com/f9/31/2XmD6gwu_o.png">。对于任意一个MDPs，总是存在一个<span style="color:#1a439c;"><strong>最优的策略 <img alt="\pi _{\ast }" class="mathcode" src="https://images2.imgbox.com/87/93/VDs2d05W_o.png"></strong></span>，在使用这个策略时就能取得最优值函数，即 <img alt="v_{\pi_{ \ast} }(s)=v_{\ast }(s)" class="mathcode" src="https://images2.imgbox.com/c9/f5/DlGIuMUX_o.png"> ， <img alt="q_{\pi_{ \ast} }(s,a)=q_{\ast }(s,a)" class="mathcode" src="https://images2.imgbox.com/1f/69/MsibsYgg_o.png"></p> 
  <p></p> 
  <h4 style="margin-left:0px;">策略分类</h4> 
  <p style="margin-left:0;">        有模型策略：环境的状态转移概率和奖励函数已知</p> 
  <p style="margin-left:0;">        无模型策略：在未知的环境里，任何决策的概率函数和奖励函数是未知的。</p> 
  <p class="img-center"><img alt="" height="154" src="https://images2.imgbox.com/01/6f/gWe9ZAgq_o.png" width="300"></p> 
  <p style="margin-left:0;">        即：在状态s下采取a行动的奖励以及转移到状态S'的概率是已知的</p> 
  <p class="img-center"><img alt="" height="156" src="https://images2.imgbox.com/78/85/0TkdQqKS_o.png" width="300"></p> 
  <p style="margin-left:0;">        在状态s下采取a行动的奖励以及转移到状态S'的概率是无法获得的，在这种情况下，我们可以选择让主体不断与环境交互，采集大量的轨迹数据，从轨迹中获取信息来改进策略，从而获得更多的奖励。</p> 
  <h2 style="margin-left:0px;"> Q表格       </h2> 
  <p style="margin-left:0;">      主体不断与环境交互的结果是，可以得到一张<span style="background-color:#ffff00;">Q</span><span style="background-color:#ffff00;">表格</span>，它像是一本生活手册。里面的每一个Q函数（动作值函数）记录了在某个状态下选择某个动作，后续能够获得多少总奖励（<span style="background-color:#ffff00;">回报期望</span>）。最开始的时候表格会全部初始化为0。智能体会不断和环境交互得到不同的轨迹，当交互的次数足够多的时候，我们就可以估算出每一个状态下，每个动作的平均总奖励，进而更Q表格，表格的更新就是强化的过程。 强化是指我们可以用<span style="color:#e84c22;">下一个状态的价值来更新当前状态的价值</span>，其实就是强化学习里面<span style="background-color:#ffff00;">自举</span>的概念。</p> 
  <h4 style="margin-left:0px;">例子：悬崖行走问题</h4> 
  <p class="img-center"><img alt="" height="211" src="https://images2.imgbox.com/9c/c8/p9KS4Nut_o.png" width="350"></p> 
  <p style="margin-left:0;">        游戏规则：智能体从出发点S 出发，到达目的地G，同时避免掉进悬崖（cliff），每走一步就有−1 分的惩罚，掉进悬崖会有−100 分的惩罚。掉进悬崖游戏不会结束，智能体会回到出发点，游戏继续，直到到达目的地结束游戏。智能体需要尽快地到达目的地，为了到达目的地，智能体可以沿着例如蓝线和红线的路线行走。</p> 
  <p style="margin-left:0;">        在这个例子中：“每走一步就有−1 分的惩罚，掉进悬崖会有−100 分的惩罚”属于<span style="background-color:#ffff00;">瞬时回报</span>，那如何计算在某个状态下采用某个动作所获得的<span style="background-color:#ffff00;">未来的总奖励</span>（Q函数）呢?</p> 
  <p style="margin-left:0;">        未来的总奖励：</p> 
  <p style="margin-left:0;">         <img alt="Q(s,a)=r_{t+1}+r_{t+2}+ ...+r_{t+n}" class="mathcode" src="https://images2.imgbox.com/63/7d/ns9CjGYP_o.png"></p> 
  <p style="margin-left:0;">         若任务是一个持续的没有尽头的任务，将采取动作a后，后续的所有奖励加起来作为当前的状态价值就很不合适，所以可以引入折扣因子,即：</p> 
  <p style="margin-left:0;">        <img alt="Q(s,a)\leftarrow G _{t}=r _{t} +\gamma r_{t+1} + ... +\gamma^{n} r_{t+n}=r_{t}+G_{t+1}" class="mathcode" src="https://images2.imgbox.com/16/32/ny0ytwfF_o.png"></p> 
  <p style="margin-left:0;">参考文献：</p> 
  <p style="margin-left:0;">强化学习——马尔科夫决策过程和贝尔曼方程 - 野风的文章 - 知乎 https://zhuanlan.zhihu.com/p/34021617</p> 
  <p style="margin-left:0;">《Easy RL：强化学习教程》</p> 
  <p style="margin-left:0;"></p> 
 </div> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/34e9fb41fe5dba5618e9233b763356bf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">文件绝对路径泄露</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d0e772d5ce442fd014f5739e9022dc78/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">游戏DrawCall分析工具-SpectorJS入门</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>