<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LeNet详解 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LeNet详解" />
<meta property="og:description" content="LeNet-5是一个较简单的卷积神经网络。下图显示了其结构：输入的二维图像，先经过两次卷积层到池化层，再经过全连接层，最后使用softmax分类作为输出层。关于CNN参见：https://blog.csdn.net/qq_42570457/article/details/81458077
LeNet-5 这个网络虽然很小，但是它包含了深度学习的基本模块：卷积层，池化层，全连接层。是其他深度学习模型的基础， 这里我们对LeNet-5进行深入分析。同时，通过实例分析，加深对与卷积层和池化层的理解。
LeNet-5共有7层，不包含输入，每层都包含可训练参数；每个层有多个Feature Map，每个FeatureMap通过一种卷积滤波器提取输入的一种特征，然后每个FeatureMap有多个神经元。
各层参数详解：
1、INPUT层-输入层 首先是数据 INPUT 层，输入图像的尺寸统一归一化为32*32。
注意：本层不算LeNet-5的网络结构，传统上，不将输入层视为网络层次结构之一。
2、C1层-卷积层 输入图片：32*32
卷积核大小：5*5
卷积核种类：6
输出featuremap大小：28*28 （32-5&#43;1）=28
神经元数量：28*28*6
可训练参数：（5*5&#43;1) * 6（每个滤波器5*5=25个unit参数和一个bias参数，一共6个滤波器）
连接数：（5*5&#43;1）*6*28*28=122304
详细说明：对输入图像进行第一次卷积运算（使用 6 个大小为 5*5 的卷积核），得到6个C1特征图（6个大小为28*28的 feature maps, 32-5&#43;1=28）。我们再来看看需要多少个参数，卷积核的大小为5*5，总共就有6*（5*5&#43;1）=156个参数，其中&#43;1是表示一个核有一个bias。对于卷积层C1，C1内的每个像素都与输入图像中的5*5个像素和1个bias有连接，所以总共有156*28*28=122304个连接（connection）。有122304个连接，但是我们只需要学习156个参数，主要是通过权值共享实现的。
3、S2层-池化层（下采样层） 输入：28*28
采样区域：2*2
采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid
采样种类：6
输出featureMap大小：14*14（28/2）
神经元数量：14*14*6
可训练参数：2*6（和的权&#43;偏置）
连接数：（2*2&#43;1）*6*14*14
S2中每个特征图的大小是C1中特征图大小的1/4。
详细说明：第一次卷积之后紧接着就是池化运算，使用 2*2核 进行池化，于是得到了S2，6个14*14的 特征图（28/2=14）。S2这个pooling层是对C1中的2*2区域内的像素求和乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。于是每个池化核有两个训练参数，所以共有2x6=12个训练参数，但是有5x14x14x6=5880个连接。
4、C3层-卷积层 输入：S2中所有6个或者几个特征map组合
卷积核大小：5*5
卷积核种类：16
输出featureMap大小：10*10 (14-5&#43;1)=10
C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合。
存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。则：可训练参数：6*(3*5*5&#43;1)&#43;6*(4*5*5&#43;1)&#43;3*(4*5*5&#43;1)&#43;1*(6*5*5&#43;1)=1516
连接数：10*10*1516=151600
详细说明：第一次池化之后是第二次卷积，第二次卷积的输出是C3，16个10x10的特征图，卷积核大小是 5*5. 我们知道S2 有6个 14*14 的特征图，怎么从6 个特征图得到 16个特征图了？ 这里是通过对S2 的特征图特殊组合计算得到的16个特征图。具体如下：
C3的前6个feature map（对应上图第一个红框的6列）与S2层相连的3个feature map相连接（上图第一个红框），后面6个feature map与S2层相连的4个feature map相连接（上图第二个红框），后面3个feature map与S2层部分不相连的4个feature map相连接，最后一个与S2层的所有feature map相连。卷积核大小依然为5*5，所以总共有6*(3*5*5&#43;1)&#43;6*(4*5*5&#43;1)&#43;3*(4*5*5&#43;1)&#43;1*(6*5*5&#43;1)=1516个参数。而图像大小为10*10，所以共有151600个连接。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/ef3cd11a4fa6cb869736e5b3c5252120/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-08-06T20:44:06+08:00" />
<meta property="article:modified_time" content="2018-08-06T20:44:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LeNet详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>       LeNet-5是一个较简单的卷积神经网络。下图显示了其结构：输入的二维图像，先经过两次卷积层到池化层，再经过全连接层，最后使用softmax分类作为输出层。关于CNN参见：<a href="https://blog.csdn.net/qq_42570457/article/details/81458077">https://blog.csdn.net/qq_42570457/article/details/81458077</a></p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_1.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="217" src="https://images2.imgbox.com/98/25/lbG8e0sA_o.png" width="517"></a></p> 
<p>       LeNet-5 这个网络虽然很小，但是它包含了<a href="http://cuijiahua.com/blog/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="nofollow">深度学习</a>的基本模块：卷积层，池化层，全连接层。是其他深度学习模型的基础， 这里我们对LeNet-5进行深入分析。同时，通过实例分析，加深对与卷积层和池化层的理解。</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_4.jpg" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="193" src="https://images2.imgbox.com/20/4e/cnNuZoLH_o.jpg" width="700"></a></p> 
<p>       LeNet-5共<strong>有7层</strong>，不包含输入，每层都包含可训练参数；每个层有<strong>多个Feature Map</strong>，每个FeatureMap通过一种卷积滤波器提取输入的一种特征，然后每个FeatureMap有<strong>多个神经元。</strong></p> 
<p><strong>各层参数详解：</strong></p> 
<h4>1、INPUT层-输入层</h4> 
<p>       首先是数据 INPUT 层，输入图像的尺寸统一归一化为32*32。</p> 
<p><strong>       注意：本层不算LeNet-5的网络结构，传统上，不将输入层视为网络层次结构之一。</strong></p> 
<h4>2、C1层-卷积层</h4> 
<p>      输入图片：32*32</p> 
<p>      卷积核大小：5*5</p> 
<p>      卷积核种类：6</p> 
<p>      输出featuremap大小：28*28 （32-5+1）=28</p> 
<p>      神经元数量：28*28*6</p> 
<p>      可训练参数：（5*5+1) * 6（每个滤波器5*5=25个unit参数和一个bias参数，一共6个滤波器）</p> 
<p>      连接数：（5*5+1）*6*28*28=122304</p> 
<p><strong>     详细说明：</strong>对输入图像进行第一次卷积运算（使用 6 个大小为 5*5 的卷积核），得到6个C1特征图（6个大小为28*28的 feature maps, 32-5+1=28）。我们再来看看需要多少个参数，卷积核的大小为5*5，总共就有6*（5*5+1）=156个参数，其中+1是表示一个核有一个bias。对于卷积层C1，C1内的每个像素都与输入图像中的5*5个像素和1个bias有连接，所以总共有156*28*28=122304个连接（connection）。有122304个连接，但是我们只需要学习156个参数，主要是通过权值共享实现的。</p> 
<h4>3、S2层-池化层（下采样层）</h4> 
<p>      输入：28*28</p> 
<p>      采样区域：2*2</p> 
<p>      采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid</p> 
<p>      采样种类：6</p> 
<p>      输出featureMap大小：14*14（28/2）</p> 
<p>      神经元数量：14*14*6</p> 
<p>      可训练参数：2*6（和的权+偏置）</p> 
<p>      连接数：（2*2+1）*6*14*14</p> 
<p>      S2中每个特征图的大小是C1中特征图大小的1/4。</p> 
<p><strong>       详细说明：</strong>第一次卷积之后紧接着就是池化运算，使用 2*2核 进行池化，于是得到了S2，6个14*14的 特征图（28/2=14）。S2这个pooling层是对C1中的2*2区域内的像素求和乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。于是每个池化核有两个训练参数，所以共有2x6=12个训练参数，但是有5x14x14x6=5880个连接。</p> 
<h4>4、C3层-卷积层</h4> 
<p>       输入：S2中所有6个或者几个特征map组合</p> 
<p>      卷积核大小：5*5</p> 
<p>      卷积核种类：16</p> 
<p>      输出featureMap大小：10*10 (14-5+1)=10</p> 
<p>      C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合。</p> 
<p>       存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。则：可训练参数：6*(3*5*5+1)+6*(4*5*5+1)+3*(4*5*5+1)+1*(6*5*5+1)=1516</p> 
<p>       连接数：10*10*1516=151600</p> 
<p><strong>       详细说明：</strong>第一次池化之后是第二次卷积，第二次卷积的输出是C3，16个10x10的特征图，卷积核大小是 5*5. 我们知道S2 有6个 14*14 的特征图，怎么从6 个特征图得到 16个特征图了？ 这里是通过对S2 的特征图特殊组合计算得到的16个特征图。具体如下：</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_5.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="175" src="https://images2.imgbox.com/50/61/p8nQxnra_o.png" width="430"></a></p> 
<p>       C3的前6个feature map（对应上图第一个红框的6列）与S2层相连的3个feature map相连接（上图第一个红框），后面6个feature map与S2层相连的4个feature map相连接（上图第二个红框），后面3个feature map与S2层部分不相连的4个feature map相连接，最后一个与S2层的所有feature map相连。卷积核大小依然为5*5，所以总共有6*(3*5*5+1)+6*(4*5*5+1)+3*(4*5*5+1)+1*(6*5*5+1)=1516个参数。而图像大小为10*10，所以共有151600个连接。</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_9.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="193" src="https://images2.imgbox.com/f2/5d/0tFkFnKa_o.png" width="298"></a></p> 
<p>        C3与S2中前3个图相连的卷积结构如下图所示：</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_6.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="189" src="https://images2.imgbox.com/2d/10/4vvWVvN9_o.png" width="313"></a></p> 
<p>       上图对应的参数为 3*5*5+1，一共进行6次卷积得到6个特征图，所以有6*（3*5*5+1）参数。 为什么采用上述这样的组合了？论文中说有两个原因：1）减少参数，2）这种不对称的组合连接的方式有利于提取多种组合特征。</p> 
<h4>5、S4层-池化层（下采样层）</h4> 
<p>       输入：10*10</p> 
<p>       采样区域：2*2</p> 
<p>      采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid</p> 
<p>      采样种类：16</p> 
<p>      输出featureMap大小：5*5（10/2）</p> 
<p>      神经元数量：5*5*16=400</p> 
<p>      可训练参数：2*16=32（和的权+偏置）</p> 
<p>      连接数：16*（2*2+1）*5*5=2000</p> 
<p>      S4中每个特征图的大小是C3中特征图大小的1/4</p> 
<p><strong>      详细说明：</strong>S4是pooling层，窗口大小仍然是2*2，共计16个feature map，C3层的16个10x10的图分别进行以2x2为单位的池化得到16个5x5的特征图。这一层有2x16共32个训练参数，5x5x5x16=2000个连接。连接的方式与S2层类似。</p> 
<h4>6、C5层-卷积层</h4> 
<p>     输入：S4层的全部16个单元特征map（与s4全相连）</p> 
<p>     卷积核大小：5*5</p> 
<p>     卷积核种类：120</p> 
<p>     输出featureMap大小：1*1（5-5+1）</p> 
<p>     可训练参数/连接：120*（16*5*5+1）=48120</p> 
<p><strong>     详细说明：</strong>C5层是一个卷积层。由于S4层的16个图的大小为5x5，与卷积核的大小相同，所以卷积后形成的图的大小为1x1。这里形成120个卷积结果。每个都与上一层的16个图相连。所以共有(5x5x16+1)x120 = 48120个参数，同样有48120个连接。C5层的网络结构如下：</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_7.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="279" src="https://images2.imgbox.com/3d/3e/wWXx4JI4_o.png" width="294"></a></p> 
<h4>7、F6层-全连接层</h4> 
<p>      输入：c5 120维向量</p> 
<p>      计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过sigmoid函数输出。</p> 
<p>      可训练参数:84*(120+1)=10164</p> 
<p><strong>      详细说明：</strong>6层是全连接层。F6层有84个节点，对应于一个7x12的比特图，-1表示白色，1表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84=10164。ASCII编码图如下：</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_8.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="193" src="https://images2.imgbox.com/98/f1/9XFqsvuW_o.png" width="371"></a></p> 
<p>     F6层的连接方式如下：</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_9.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="193" src="https://images2.imgbox.com/79/b2/Q6BtrEgd_o.png" width="298"></a></p> 
<h4>8、Output层-全连接层</h4> 
<p>       Output层也是全连接层，共有10个节点，分别代表数字0到9，且如果节点i的值为0，则网络识别的结果是数字i。采用的是径向基函数（RBF）的网络连接方式。假设x是上一层的输入，y是RBF的输出，则RBF输出的计算方式是：</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_10.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="43" src="https://images2.imgbox.com/55/48/7B5L5Jad_o.png" width="121"></a></p> 
<p>     上式w_ij 的值由i的比特图编码确定，i从0到9，j取值从0到7*12-1。RBF输出的值越接近于0，则越接近于i，即越接近于i的ASCII编码图，表示当前网络输入的识别结果是字符i。该层有84x10=840个参数和连接。</p> 
<p style="text-align:center;"><a href="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_11.png" rel="nofollow"><img alt="网络解析（一）：LeNet-5详解" class="has" height="271" src="https://images2.imgbox.com/71/38/EhJVW6ec_o.png" width="354"></a></p> 
<p>上图是LeNet-5识别数字3的过程。</p> 
<h4>总结</h4> 
<ul><li>LeNet-5是一种用于手写体字符识别的非常高效的卷积神经网络。</li><li>卷积神经网络能够很好的利用图像的结构信息。</li><li>卷积层的参数较少，这也是由卷积层的主要特性即局部连接和共享权重所决定。</li></ul> 
<p> 参考：<a href="http://cuijiahua.com/blog/2018/01/dl_3.html" rel="nofollow">http://cuijiahua.com/blog/2018/01/dl_3.html</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fd4ac8c906be6012fdaaf4fa5534140f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ubuntu 虚拟机设置root密码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ff1ac8081abbb334bf1621aa85416360/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">PyQt5基本控件详解之QLineEdit（四）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>