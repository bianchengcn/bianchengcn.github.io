<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hive函数 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hive函数" />
<meta property="og:description" content="文章目录 一、Hive建表SQL二、Hive函数三、函数1、查看内置函数2、空字段赋值(nvl)3、CASE WHEN THEN ELSE END4、行转列5、列转行6、开窗函数6.1 简介6.2 语法6.3 案例6.4 LAG函数6.5 Ntile函数6.6 Rank 7、自定义函数 四、压缩和存储1、简介2、压缩简介3、Map输出阶段压缩4、开启Reduce输出阶段压缩5、文件存储格式5.1 列式存储和行式存储5.2 TextFile_行存储5.3 Orc_列存储5.4 Parquet_列存储5.5 数据存储大小对比 6、存储和压缩结合 四、企业级调优1、查看执行计划2、Hive建表优化3、HQL语法优化3.1 列裁剪和分区裁剪3.2 Group By3.3 CBO优化 4、数据倾斜4.1 现象4.2 单表数据倾斜优化4.3 Join数据倾斜优化 5、Hive Job优化 一、Hive建表SQL Hive建表SQL
二、Hive函数 Hive函数
三、函数 1、查看内置函数 查看系统自带函数
show functions; 查看自带函数用法
#	显示简单用法 desc function !=; # 显示详细用法 desc function extended !=; 2、空字段赋值(nvl) NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。功能：如果value为NULL，则NVL函数返回default_value的值，否则返回value的值如果两个参数都为NULL ，则返回NULL。 #	当common为空时，返回age SELECT nvl(name,&#39;age&#39;) FROM user; #	当name为空时候，用full_name替代 SELECT nvl(name,full_name) FROM user; 3、CASE WHEN THEN ELSE END 根据不同数据，返回不同逻辑" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/138ed0959fcf311f0bd75c89919ee66c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-18T20:46:02+08:00" />
<meta property="article:modified_time" content="2023-11-18T20:46:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hive函数</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#HiveSQL_4" rel="nofollow">一、Hive建表SQL</a></li><li><a href="#Hive_7" rel="nofollow">二、Hive函数</a></li><li><a href="#_9" rel="nofollow">三、函数</a></li><li><ul><li><a href="#1_11" rel="nofollow">1、查看内置函数</a></li><li><a href="#2nvl_28" rel="nofollow">2、空字段赋值(nvl)</a></li><li><a href="#3CASE_WHEN_THEN_ELSE_END_41" rel="nofollow">3、CASE WHEN THEN ELSE END</a></li><li><a href="#4_85" rel="nofollow">4、行转列</a></li><li><a href="#5_192" rel="nofollow">5、列转行</a></li><li><a href="#6_231" rel="nofollow">6、开窗函数</a></li><li><ul><li><a href="#61__233" rel="nofollow">6.1 简介</a></li><li><a href="#62__240" rel="nofollow">6.2 语法</a></li><li><a href="#63__309" rel="nofollow">6.3 案例</a></li><li><a href="#64_LAG_486" rel="nofollow">6.4 LAG函数</a></li><li><a href="#65_Ntile_521" rel="nofollow">6.5 Ntile函数</a></li><li><a href="#66_Rank_579" rel="nofollow">6.6 Rank</a></li></ul> 
    </li><li><a href="#7_591" rel="nofollow">7、自定义函数</a></li></ul> 
   </li><li><a href="#_605" rel="nofollow">四、压缩和存储</a></li><li><ul><li><a href="#1_607" rel="nofollow">1、简介</a></li><li><a href="#2_612" rel="nofollow">2、压缩简介</a></li><li><a href="#3Map_645" rel="nofollow">3、Map输出阶段压缩</a></li><li><a href="#4Reduce_674" rel="nofollow">4、开启Reduce输出阶段压缩</a></li><li><a href="#5_710" rel="nofollow">5、文件存储格式</a></li><li><ul><li><a href="#51__712" rel="nofollow">5.1 列式存储和行式存储</a></li><li><a href="#52_TextFile__743" rel="nofollow">5.2 TextFile_行存储</a></li><li><a href="#53_Orc__748" rel="nofollow">5.3 Orc_列存储</a></li><li><a href="#54_Parquet__769" rel="nofollow">5.4 Parquet_列存储</a></li><li><a href="#55__785" rel="nofollow">5.5 数据存储大小对比</a></li></ul> 
    </li><li><a href="#6_848" rel="nofollow">6、存储和压缩结合</a></li></ul> 
   </li><li><a href="#_919" rel="nofollow">四、企业级调优</a></li><li><ul><li><a href="#1_921" rel="nofollow">1、查看执行计划</a></li><li><a href="#2Hive_951" rel="nofollow">2、Hive建表优化</a></li><li><a href="#3HQL_957" rel="nofollow">3、HQL语法优化</a></li><li><ul><li><a href="#31__959" rel="nofollow">3.1 列裁剪和分区裁剪</a></li><li><a href="#32_Group_By_966" rel="nofollow">3.2 Group By</a></li><li><a href="#33_CBO_992" rel="nofollow">3.3 CBO优化</a></li></ul> 
    </li><li><a href="#4_998" rel="nofollow">4、数据倾斜</a></li><li><ul><li><a href="#41__1000" rel="nofollow">4.1 现象</a></li><li><a href="#42__1010" rel="nofollow">4.2 单表数据倾斜优化</a></li><li><a href="#43_Join_1047" rel="nofollow">4.3 Join数据倾斜优化</a></li></ul> 
    </li><li><a href="#5Hive_Job_1098" rel="nofollow">5、Hive Job优化</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="HiveSQL_4"></a>一、Hive建表SQL</h3> 
<p><a href="https://download.csdn.net/download/weixin_44624117/88550252">Hive建表SQL</a></p> 
<h3><a id="Hive_7"></a>二、Hive函数</h3> 
<p><a href="https://download.csdn.net/download/weixin_44624117/88550255">Hive函数</a></p> 
<h3><a id="_9"></a>三、函数</h3> 
<h4><a id="1_11"></a>1、查看内置函数</h4> 
<p>查看系统自带函数</p> 
<pre><code class="prism language-hive">show functions;
</code></pre> 
<p>查看自带函数用法</p> 
<pre><code class="prism language-hive">#	显示简单用法
desc function !=;
# 显示详细用法
desc function extended !=;
</code></pre> 
<h4><a id="2nvl_28"></a>2、空字段赋值(nvl)</h4> 
<table><thead><tr><th>NVL：</th><th>给值为NULL的数据赋值，它的格式是NVL( value，default_value)。</th></tr></thead><tbody><tr><td>功能：</td><td>如果value为NULL，则NVL函数返回default_value的值，否则返回value的值如果两个参数都为NULL ，则返回NULL。</td></tr></tbody></table> 
<pre><code class="prism language-hive">#	当common为空时，返回age
SELECT nvl(name,'age') FROM user;
#	当name为空时候，用full_name替代
SELECT nvl(name,full_name) FROM user;
</code></pre> 
<h4><a id="3CASE_WHEN_THEN_ELSE_END_41"></a>3、CASE WHEN THEN ELSE END</h4> 
<p><font color="#dd0000">根据不同数据，返回不同逻辑</font></p> 
<pre><code class="prism language-hive">SELECT
 CASE deptno
   WHEN 1 THEN Engineering
   WHEN 2 THEN Finance
   ELSE admin
 END,
 CASE zone
   WHEN 7 THEN Americas
   ELSE Asia-Pac
 END
 FROM emp_details

/** 当a=b时，返回c；当a=d时，返回d；当a=e时，放回e；其他情况返回f。*/
CASE a 
	WHEN b THEN c
	WHEN d THEN e
	ELSE f
END
</code></pre> 
<p>案例：</p> 
<pre><code class="prism language-hive">SELECT 
	name,
	CASE sex 
		WHEN '男' THEN 1 
		ELSE 0 
	END
FROM default.user01 u

SELECT 
	name,
	SUM(CASE sex WHEN '男'THEN  1 ELSE 0 END )
FROM default.user01 u 
GROUP BY name ;
</code></pre> 
<h4><a id="4_85"></a>4、行转列</h4> 
<p><font color="#dd0000">将多行数据进行汇总成1条数据、多行转为单行</font></p> 
<ul><li><code>CONCAT(string A/col, string B/col…)</code>：多个字段拼接</li><li><code>CONCAT_WS(separator, str1, str2,...)</code>：多字段拼接(指定拼接符号)</li><li><code>COLLECT_SET(col)</code>：接受分组数据，汇总为Array（去重）</li><li><code>COLLECT_LIST(col)</code>：接受分组数据，汇总为Array（不去重）</li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">SELECT</span> 
	deptno <span class="token keyword">AS</span> Deptno<span class="token punctuation">,</span>
	CONCAT_WS<span class="token punctuation">(</span><span class="token string">','</span> <span class="token punctuation">,</span> COLLECT_LIST<span class="token punctuation">(</span>Ename<span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token keyword">AS</span> enames
<span class="token keyword">FROM</span> <span class="token keyword">user</span>
<span class="token keyword">GROUP</span> <span class="token keyword">BY</span> deptno
</code></pre> 
<p><img src="https://images2.imgbox.com/ce/82/l3Y6iHJH_o.png" alt="在这里插入图片描述"></p> 
<p><strong><code>CONCAT</code></strong></p> 
<pre><code class="prism language-hive"># 字符串拼接
#	返回输入字符串连接后的结果，支持任意个输入字符串;
CONCAT(string A/col, string B/col…)：
</code></pre> 
<p>案例：</p> 
<pre><code class="prism language-hive">SELECT CONCAT(name,age)  FROM  default.user01 u ;
#	结果：
悟空12
大海14
</code></pre> 
<p>**<code>CONCAT_WS(separator, str1, str2,...)：</code>**多字符串拼接</p> 
<ul><li> <p>它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。</p> </li><li> <p>分隔符可以是与剩余参数一样的字符串。</p> </li><li> <p>如果分隔符是 NULL，返回值也将为 NULL。</p> </li><li> <p>这个函数会跳过分隔符参数后的任何 NULL 和空字符串。</p> </li><li> <p>分隔符将被加到被连接的字符串之间;</p> </li><li> <p>注意: <code>CONCAT_WS must be "string or array&lt;string&gt;</code></p> </li></ul> 
<pre><code class="prism language-hive">#	`separator`：连接符
#	`str1`：参数1
#	`str2`：参数2
#	：字符串拼接(CONCAT_WS('+','a','b')=&gt;a+b)
CONCAT_WS(separator, str1, str2,...)
</code></pre> 
<p>案例：</p> 
<pre><code class="prism language-hive">SELECT concat_ws('.', 'www', 'facebook', 'com') FROM default.user01;
www.facebook.com

SELECT concat_ws('.', 'www', array('facebook', 'com')) FROM default.user01;
www.facebook.com
www.facebook.com
</code></pre> 
<p><strong><code>COLLECT_SET(col)</code></strong></p> 
<ul><li>函数只接受基本数据类型</li><li>它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</li><li>去重</li></ul> 
<pre><code class="prism language-hive">#	将分组后数据汇总为Array
collect_list(name)
</code></pre> 
<p>案例：</p> 
<pre><code class="prism language-hive">#	["宋宋","凤姐","婷婷"]
select collect_set(name)from default.user01 group by age;
#	宋宋+凤姐+婷婷
select  concat_ws('+',collect_set(name))from default.user01 group by age;
</code></pre> 
<p><strong><code>COLLECT_LIST(col):</code></strong></p> 
<ul><li>不去重</li><li>函数指接收基本数据类型</li><li>它的主要作用是将某字段的值进行不去重汇总，产生array类型字段。</li></ul> 
<pre><code class="prism language-hive">#	将分组后数据汇总为Array
collect_list(name)
</code></pre> 
<p>案例</p> 
<pre><code class="prism language-hive">#	["宋宋","宋宋","凤姐","婷婷"]
select collect_list(name)from default.user01 group by age;
#	宋宋+宋宋+凤姐+婷婷
select  concat_ws('+',collect_list(name))from default.user01 group by age;
</code></pre> 
<h4><a id="5_192"></a>5、列转行</h4> 
<ul><li><code>EXPLODE(col)</code>：将hive表的一列中复杂的array或者map结构拆分成多行。</li><li><code>SPLIT(string str, string regex)</code>: 按照regex字符串分割str，会返回分割后的字符串数组。</li><li><code>LATERAL VIEW</code>：侧视图 
  <ul><li><code>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</code></li><li>用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UDTF会报一行拆分成一行或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/c9/51/pvIDL2uz_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/04/d5/oeytFAJY_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-hive">SELECT split(category,",")  FROM movie_info ;
</code></pre> 
<p><img src="https://images2.imgbox.com/17/c1/YtRHLHAb_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-hive">SELECT explode(split(category,","))  FROM movie_info mi;
</code></pre> 
<p><img src="https://images2.imgbox.com/41/8c/uXAg1GJB_o.png" alt="在这里插入图片描述"></p> 
<ul><li><code>split</code>：将<code>category</code>列中数据，拆分为多条，以Array类型。</li><li><code>explode</code>：将单列中Array存储的转为多行数据。</li><li><code>lateral VIEW</code>：将Array中数据整合为可被查询的列。</li></ul> 
<pre><code class="prism language-hive">SELECT movie,category_name 
	FROM movie_info 
lateral VIEW
explode(split(category,",")) movie_info_tmp  AS category_name ;
</code></pre> 
<h4><a id="6_231"></a>6、开窗函数</h4> 
<h5><a id="61__233"></a>6.1 简介</h5> 
<ul><li>窗口函数不同于我们熟悉的常规函数及聚合函数，它输入多行数据（一个窗口），为每行数据进行一次计算，返回一个值。</li><li>灵活运用窗口函数可以解决很多复杂的问题，如去重、排名、同比及和环比、连续登录等。</li></ul> 
<p><img src="https://images2.imgbox.com/af/a0/8KWYHli3_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="62__240"></a>6.2 语法</h5> 
<p><strong>语法：</strong></p> 
<p><img src="https://images2.imgbox.com/43/ac/q9s4oUCO_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-hive">Function（arg1 ……） over（[partition by arg1 ……] [order by arg1 ……] [&lt;window_expression&gt;]）
</code></pre> 
<ul><li> <p>rows必须跟在Order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量</p> </li><li> <p>如果不指定partition by，则不对数据进行分区，换句话说，所有数据看作同一个分区。</p> </li><li> <p>如果不指定order by， 则不对各分区进行排序，通常用于那些顺序无关的窗口函数，如sum（）。</p> </li><li> <p>如果不指定窗口子句：</p> 
  <ul><li>不指定order by，默认使用分区内所有行，等同于</li></ul> <pre><code class="prism language-hive">Function() over(rows between unbounded precedeing and unbounded following)
</code></pre> 
  <ul><li>如果指定order by，默认使用分区内第起点到当前行，等同于</li></ul> <pre><code class="prism language-hive">Function() over(rows between unbounded preceding and current row)
</code></pre> </li><li> <p><code>Over（）</code>语法</p> </li></ul> 
<table><thead><tr><th>语法</th><th>解释(添加)</th><th>解释(不添加)</th></tr></thead><tbody><tr><td>Partition by：</td><td>表示将数据先按字段进行分区</td><td>不对数据进行分区，换句话说，所有数据看作同一个分区</td></tr><tr><td>Order by：</td><td>表示将各个分区内的数据按字段进行排序。<br>不指定默认所有行。</td><td>则不对各分区进行排序，通常用于那些顺序无关的窗口函数。<br>指定后：从开头行至当前行。</td></tr></tbody></table> 
<pre><code>partition by：
不指定：则不对数据进行分区，换句话说，所有数据看作同一个分区。

order by：
不指定order by：默认使用分区内所有行，等同于
指定order by：  默认使用分区内第起点到当前行，等同于
</code></pre> 
<ul><li><code>window_expression</code>语法</li></ul> 
<table><thead><tr><th>语法</th><th>解释</th></tr></thead><tbody><tr><td>n preceding</td><td>往前n行</td></tr><tr><td>n following</td><td>往后n行</td></tr><tr><td>current row</td><td>当前行</td></tr><tr><td>unbounded preceding</td><td>从前面的起点开始</td></tr><tr><td>unbounded following</td><td>到后面的终点结束</td></tr></tbody></table> 
<pre><code>#   从开头到当前行
ROWS BETWEEN unbounded preceding AND current row

#   昨天至今天
rows between 1 preceding and current row

#   当天到最后一天
rows between current row and unbounded following
</code></pre> 
<h5><a id="63__309"></a>6.3 案例</h5> 
<p>数据表</p> 
<ul><li>查询在2017年4月份，到店购买的人，及到店次数</li></ul> 
<pre><code class="prism language-hive">select
	name ,
	count(name) over()
from
	business
where
	substring(orderdate, 1, 7)= '2017-04'
group by name;
</code></pre> 
<ul><li>查询顾客的购买明细，及到当前日期为止的购买总金额</li></ul> 
<pre><code class="prism language-hive">select
	name,
	orderdate,
	cost,
	sum(cost)over(partition by name
order by
	orderdate)
from business;
</code></pre> 
<p>执行结果</p> 
<pre><code>jack	2017-01-01	10	10
jack	2017-01-05	46	56
jack	2017-01-08	55	111
jack	2017-02-03	23	134
jack	2017-04-06	42	176
mart	2017-04-08	62	62
</code></pre> 
<ul><li>每一行数据都新增一列，即消费总额</li></ul> 
<pre><code class="prism language-hive">select
	name,
	orderdate,
	cost,
	sum(cost) over() sample1
from business;
</code></pre> 
<p>执行结果：</p> 
<pre><code>mart	2017-04-13	94	661
neil	2017-06-12	80	661
mart	2017-04-11	75	661
neil	2017-05-10	12	661
mart	2017-04-09	68	661
mart	2017-04-08	62	661
jack	2017-01-08	55	661
</code></pre> 
<ul><li>计算每个人的销售总额 
  <ul><li>以name分区计算，每行数据新增一列，即每人消费总额</li></ul> </li></ul> 
<pre><code class="prism language-hive">select
	name,
	orderdate,
	cost ,
	sum(cost)over(partition by name)
from business;
</code></pre> 
<p>执行结果：</p> 
<pre><code class="prism language-hive">jack	2017-01-05	46	176
jack	2017-01-08	55	176
jack	2017-01-01	10	176
jack	2017-04-06	42	176
jack	2017-02-03	23	176
mart	2017-04-13	94	299
mart	2017-04-11	75	299
mart	2017-04-09	68	299
</code></pre> 
<ul><li>计算每个人截止到当天的销售总额 
  <ul><li>以name分区、日期排序计算，每行数据增一列，即截止到当前的消费总额，也就是从起点到当前行做聚合。</li><li>如果指定order by，默认使用分区内第起点到当前行。</li></ul> </li></ul> 
<pre><code class="prism language-hive">select
	name,
	orderdate,
	cost,
	sum(cost)over(partition by name order by orderdate)as sum_now
from
	business;
</code></pre> 
<pre><code class="prism language-hive">select
	name,
	orderdate,
	cost,
	sum(cost)over(partition by name order by orderdate 
# 	固定写法，表明从开头到当前行
                rows between unbounded preceding and current row )as sum_now
from
	business;
</code></pre> 
<p>执行结果：</p> 
<pre><code class="prism language-hive">jack	2017-01-01	10	10
jack	2017-01-05	46	56
jack	2017-01-08	55	111
jack	2017-02-03	23	134
jack	2017-04-06	42	176
mart	2017-04-08	62	62
mart	2017-04-09	68	130
</code></pre> 
<ul><li>每个人连续两天的消费总额 
  <ul><li>以name分区、日期排序计算，每行数据增一列，即连续两天的消费总额也就是前一行和当前行聚合。</li><li>当天的值，为当天值与昨天值相加。</li></ul> </li></ul> 
<pre><code class="prism language-hive">select 
	name,
	orderdate,
	cost, 
	sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row ) as sample5
from business;
</code></pre> 
<p>执行结果</p> 
<pre><code class="prism language-hive">jack	2017-01-01	10	10
jack	2017-01-05	46	56   (10+46=56)
jack	2017-01-08	55	101  (46+55=101)
jack	2017-02-03	23	78		(55+23=78)
jack	2017-04-06	42	65
mart	2017-04-08	62	62
mart	2017-04-09	68	130
</code></pre> 
<ul><li>每个人从当天到最后一天的消费总额 
  <ul><li>以name分区、日期排序计算，每行数据增一列，即当前天到最后一天的消费总额，也就是当前行聚合最后一行。</li><li>行，从当前行至最后一行总和。</li></ul> </li></ul> 
<pre><code class="prism language-hive">select 
	name,
	orderdate,
	cost,
	sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING )
from business;
</code></pre> 
<p>计算结果：</p> 
<pre><code class="prism language-hive">jack	2017-01-01	10	176
jack	2017-01-05	46	166
jack	2017-01-08	55	120	(120=42+23+55)
jack	2017-02-03	23	65	(65=42+23)
jack	2017-04-06	42	42	(42=42)
mart	2017-04-08	62	299
mart	2017-04-09	68	237
</code></pre> 
<h5><a id="64_LAG_486"></a>6.4 LAG函数</h5> 
<ul><li>Lag函数用于统计窗口内往上第n行值。</li><li><code>scalar_pexpression</code>：列名。</li><li><code>offset</code> ：为往上几行。</li><li><code>default</code> ：是设置的默认值（当往上第n行为NULL时，取默认值，否则就为NULL）</li></ul> 
<pre><code>LAG  (scalar_expression [,offset] [,default]) OVER ([query_partition_clause] order_by_clause);
</code></pre> 
<ul><li>查看顾客上次的购买时间</li></ul> 
<pre><code class="prism language-hive">select
	name,
	orderdate,
	cost,
	lag(orderdate,1,'1900-01-01') over(partition by name order by	orderdate ) as last_time
from	business ;
</code></pre> 
<p>执行结果：</p> 
<pre><code class="prism language-hive">jack	2017-01-01	10	1900-01-01
jack	2017-01-05	46	2017-01-01
jack	2017-01-08	55	2017-01-05
jack	2017-02-03	23	2017-01-08
jack	2017-04-06	42	2017-02-03
mart	2017-04-08	62	1900-01-01
mart	2017-04-09	68	2017-04-08
mart	2017-04-11	75	2017-04-09
</code></pre> 
<h5><a id="65_Ntile_521"></a>6.5 Ntile函数</h5> 
<ul><li>为已排序的行，均分为指定数量的组，组号按顺序排列，返回组号，不支持<code>rows between</code></li></ul> 
<pre><code class="prism language-hive">select
	name,
	orderdate,
	cost,
	ntile(5) over( order by orderdate) sorted
from business;
</code></pre> 
<p>执行结果：(分为了5组，下面取第1组数据即可)</p> 
<pre><code>jack	2017-01-01	10	1
tony	2017-01-02	15	1
tony	2017-01-04	29	1
jack	2017-01-05	46	2
tony	2017-01-07	50	2
jack	2017-01-08	55	2
jack	2017-02-03	23	3
jack	2017-04-06	42	3
mart	2017-04-08	62	3
mart	2017-04-09	68	4
mart	2017-04-11	75	4
mart	2017-04-13	94	4
neil	2017-05-10	12	5
neil	2017-06-12	80	5
</code></pre> 
<p>取第一组数据</p> 
<pre><code class="prism language-hive">select
	t1.name,
	t1.orderdate,
	t1.cost
from
	(select
		name,
		orderdate,
		cost,
		ntile(5) over( order by orderdate) sorted
	from business ) t1
where
	t1.sorted = 1;
</code></pre> 
<p>执行结果：</p> 
<pre><code class="prism language-hive">OK
jack	2017-01-01	10
tony	2017-01-02	15
tony	2017-01-04	29
</code></pre> 
<h5><a id="66_Rank_579"></a>6.6 Rank</h5> 
<ul><li><code>Rank()</code>：排序，值相同时会重复，总数不会变(1、1、3、4)。</li><li><code>DENSE_RANK()</code>：排序，值相同时会重复，总数会较少(1、1、2、3)。</li><li><code>ROW_NUMBER()</code>：根据顺序计算。字段相同就按排头字段继续排(1、2、3、4)。</li></ul> 
<p>源数据:</p> 
<p><img src="https://images2.imgbox.com/15/51/geZ5F7Ec_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/f4/01/Ayr1t84D_o.png" alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传"></p> 
<h4><a id="7_591"></a>7、自定义函数</h4> 
<ul><li> <p>**内置函数：**Hive自带的函数。</p> </li><li> <p>**自定义函数：**当Hive提供的内置函数无法满足你的业务处理需要时。可以自己定义一些函数。</p> 
  <ul><li> <p><code>UDF（User-Defined-Function）</code>： 一进一出。</p> </li><li> <p><code>UDAF（User-Defined Aggregation Function）</code>：聚合函数，多进一出，类似：count/max/min</p> </li><li> <p><code>UDTF（User-Defined Table-Generating Functions）</code>：炸裂函数，一进多出，如：explode()</p> </li></ul> </li></ul> 
<h3><a id="_605"></a>四、压缩和存储</h3> 
<h4><a id="1_607"></a>1、简介</h4> 
<ul><li>Hive不会强制要求将数据转换成特定的格式才能使用。利用Hadoop的InputFormat API可以从不同数据源读取数据，使用OutputFormat API可以将数据写成不同的格式输出。</li><li>对数据进行压缩虽然会增加额外的CPU开销，但是会节约客观的磁盘空间，并且通过减少载入内存的数据量而提高I/O吞吐量会更加提高网络传输性能。</li><li>原则上Hadoop的job时I/O密集型的话就可以采用压缩可以提高性能，如果job是CPU密集型的话，那么使用压缩可能会降低执行性能。</li></ul> 
<h4><a id="2_612"></a>2、压缩简介</h4> 
<p>常用压缩算法</p> 
<p><img src="https://images2.imgbox.com/d4/8b/6IpBCjif_o.png" alt="在这里插入图片描述"></p> 
<table><thead><tr><th>压缩格式</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>Deflate</td><td>Deflate</td><td>.deflate</td><td>否</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>Gzip</td><td>Deflate</td><td>.gz</td><td>否</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>Bzip2</td><td>Bzip2</td><td>.bz2</td><td>是</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>Lzo</td><td>Lzo</td><td>.lzo</td><td>是</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>Snappy</td><td>.snappy</td><td>否</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table> 
<p>压缩效率对比</p> 
<table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table> 
<p>要在Hadoop中启用压缩，可以配置如下参数（<code>mapred-site.xml</code>文件中）：</p> 
<table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,<br>org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table> 
<h4><a id="3Map_645"></a>3、Map输出阶段压缩</h4> 
<p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。</p> 
<p>（1）开启hive中间传输数据压缩功能</p> 
<pre><code class="prism language-hive">set hive.exec.compress.intermediate =true;
</code></pre> 
<p>（2）开启mapreduce中map输出压缩功能</p> 
<pre><code class="prism language-hive">set mapreduce.map.output.compress=true;
</code></pre> 
<p>（3）设置mapreduce中map输出数据的压缩方式</p> 
<pre><code class="prism language-hive">set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;
</code></pre> 
<p>（4）执行查询语句</p> 
<pre><code class="prism language-hive">select count(ename) name from emp;
</code></pre> 
<p>查看历史日志中，有压缩方式<code>.snappy</code></p> 
<p><img src="https://images2.imgbox.com/da/41/hxLH7fxZ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="4Reduce_674"></a>4、开启Reduce输出阶段压缩</h4> 
<p>当Hive将输出写入到表中时可以通过属性hive.exec.compress.output，对输出内容进行压缩。</p> 
<p>将<code>hive.exec.compress.output = false</code>，这样输出就是非压缩的纯文本文件了。</p> 
<p>将<code>hive.exec.compress.output = true</code>，来开启输出结果压缩功能。</p> 
<p>（1）开启hive最终输出数据压缩功能</p> 
<pre><code class="prism language-hive">set hive.exec.compress.output=true;
</code></pre> 
<p>（2）开启mapreduce最终输出数据压缩</p> 
<pre><code class="prism language-hive">set mapreduce.output.fileoutputformat.compress=true;
</code></pre> 
<p>（3）设置mapreduce最终数据输出压缩方式</p> 
<pre><code class="prism language-hive">set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;
</code></pre> 
<p>（4）设置mapreduce最终数据输出压缩为块压缩</p> 
<pre><code class="prism language-hive">set mapreduce.output.fileoutputformat.compress.type=BLOCK;
</code></pre> 
<p>（5）测试一下输出结果是否是压缩文件</p> 
<pre><code class="prism language-hive">insert overwrite local directory '/opt/module/hive/datas/distribute-result' select * from score distribute by name sort by score desc;
</code></pre> 
<p><img src="https://images2.imgbox.com/15/11/pvhDyRz2_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="5_710"></a>5、文件存储格式</h4> 
<h5><a id="51__712"></a>5.1 列式存储和行式存储</h5> 
<p>Hive支持的存储数据的格式主要有：</p> 
<ul><li>TEXTFILE：行存储</li><li>SEQUENCEFILE：行存储</li><li>ORC：列存储</li><li>PARQUET：列存储</li></ul> 
<p>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p> 
<p><img src="https://images2.imgbox.com/0a/23/3FbiG4sQ_o.png" alt="在这里插入图片描述"></p> 
<ul><li>行存储的特点</li></ul> 
<p>查询满足条件的一整行数据的时候</p> 
<p>列存储则需要去每个聚集的字段找到对应的每个列的值</p> 
<p>行存储只需要找到其中一个值，其余的值都在相邻地方</p> 
<p>所以此时行存储查询的速度更快。</p> 
<ul><li>列存储的特点</li></ul> 
<p>每个字段的数据聚集存储，查询只需要少数几个字段的时候，能大大减少读取的数据量；</p> 
<p>每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p> 
<h5><a id="52_TextFile__743"></a>5.2 TextFile_行存储</h5> 
<ul><li>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。</li><li>可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</li></ul> 
<h5><a id="53_Orc__748"></a>5.3 Orc_列存储</h5> 
<p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p> 
<p><img src="https://images2.imgbox.com/40/a9/neHyFSgo_o.png" alt="在这里插入图片描述"></p> 
<p>如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。</p> 
<ul><li> <p>每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p> 
  <ul><li> <p><code>Index Data</code>：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p> </li><li> <p><code>Row Data</code>：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p> </li><li> <p><code>Stripe Footer</code>：存的是各个Stream的类型，长度等信息。每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p> </li></ul> </li></ul> 
<p>ORC中压缩：</p> 
<ul><li><code>Zlib</code>：压缩比高，效率低。压缩ORC的默认压缩格式。</li><li><code>Snappy</code>：压缩比低，效率高。</li></ul> 
<h5><a id="54_Parquet__769"></a>5.4 Parquet_列存储</h5> 
<p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p> 
<ul><li>（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</li><li>（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</li><li>（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</li></ul> 
<p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。</p> 
<p><img src="https://images2.imgbox.com/81/e7/8mbWaOBv_o.png" alt="在这里插入图片描述"></p> 
<p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。</p> 
<p>除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p> 
<h5><a id="55__785"></a>5.5 数据存储大小对比</h5> 
<ul><li>TextFile</li></ul> 
<pre><code class="prism language-hive">create table log_text (
	track_time string,
	url string,
	session_id string,
	referer string,
	ip string,
	end_user_id string,
	city_id string
)
row format delimited fields terminated by '\t'
stored as textfile;
</code></pre> 
<ul><li>ORC</li></ul> 
<pre><code class="prism language-hive">create table log_orc(
	track_time string,
	url string,
	session_id string,
	referer string,
	ip string,
	end_user_id string,
	city_id string
)
row format delimited fields terminated by '\t'
stored as orc
tblproperties("orc.compress"="NONE"); // 由于ORC格式时自带压缩的，这设置orc存储不使用压缩
</code></pre> 
<ul><li>Parquet</li></ul> 
<pre><code class="prism language-hive">create table log_parquet(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as parquet ;
</code></pre> 
<p>同样数据文件存储后大小：</p> 
<ul><li>TextFile：18.1M</li><li>ORC：7.7 M</li><li>Parquet：13.1M</li></ul> 
<p>存储的大小对比：ORC <strong>&gt;</strong> Parquet <strong>&gt;</strong> textFile</p> 
<p>查询速度：速度接近</p> 
<h4><a id="6_848"></a>6、存储和压缩结合</h4> 
<table><thead><tr><th>Key</th><th>Default</th><th>Notes</th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>high level compression (one of NONE, ZLIB, SNAPPY)</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>number of bytes in each compression chunk</td></tr><tr><td>orc.stripe.size</td><td>268,435,456</td><td>number of bytes in each stripe</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>number of rows between index entries (must be &gt;= 1000)</td></tr><tr><td>orc.create.index</td><td>true</td><td>whether to create row indexes</td></tr><tr><td>orc.bloom.filter.columns</td><td>“”</td><td>comma separated list of column names for which bloom filter should be created</td></tr><tr><td>orc.bloom.filter.fpp</td><td>0.05</td><td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td></tr></tbody></table> 
<p>注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现。</p> 
<ul><li>ZLIB压缩的ORC存储方式（2.8 M）</li></ul> 
<pre><code class="prism language-hive">create table log_orc_zlib(
	track_time string,
	url string,
	session_id string,
	referer string,
	ip string,
	end_user_id string,
	city_id string
)
row format delimited fields terminated by '\t'
stored as orc
tblproperties("orc.compress"="ZLIB");
</code></pre> 
<ul><li>SNAPPY压缩的ORC存储方式（3.7 M）</li></ul> 
<pre><code class="prism language-hive">create table log_orc_snappy(
	track_time string,
	url string,
	session_id string,
	referer string,
	ip string,
	end_user_id string,
	city_id string
)
row format delimited fields terminated by '\t'
stored as orc
tblproperties("orc.compress"="SNAPPY");
</code></pre> 
<ul><li>SNAPPY压缩的parquet存储方式（6.4 MB）</li></ul> 
<pre><code class="prism language-hive">create table log_parquet_snappy(
	track_time string,
	url string,
	session_id string,
	referer string,
	ip string,
	end_user_id string,
	city_id string
)
row format delimited fields terminated by '\t'
stored as parquet
tblproperties("parquet.compression"="SNAPPY");
</code></pre> 
<ul><li>hive表的数据存储格式一般选择：orc或parquet。</li><li>压缩方式一般选择snappy，lzo。</li><li>ORC默认压缩：<code>Zlib</code>。</li></ul> 
<h3><a id="_919"></a>四、企业级调优</h3> 
<h4><a id="1_921"></a>1、查看执行计划</h4> 
<pre><code class="prism language-hive">EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query
</code></pre> 
<ul><li>没有生成MR任务的</li></ul> 
<pre><code class="prism language-hive">explain select * from emp;
</code></pre> 
<p><img src="https://images2.imgbox.com/97/fe/TlV0cwNk_o.png" alt="在这里插入图片描述"></p> 
<ul><li>生成MR任务的</li></ul> 
<pre><code class="prism language-hive">explain select deptno, avg(sal) avg_sal from emp group by deptno;
</code></pre> 
<p><img src="https://images2.imgbox.com/62/81/hxQZEMd5_o.png" alt="在这里插入图片描述"></p> 
<p>查看详细执行计划</p> 
<pre><code class="prism language-hive">explain extended select * from emp;
</code></pre> 
<h4><a id="2Hive_951"></a>2、Hive建表优化</h4> 
<ul><li>分区表</li><li>分桶表</li><li>合适的文件格式</li></ul> 
<h4><a id="3HQL_957"></a>3、HQL语法优化</h4> 
<h5><a id="31__959"></a>3.1 列裁剪和分区裁剪</h5> 
<p>在生产环境中，会面临列很多或者数据量很大时，如果使用<code>select * </code>或者不指定分区进行全列或者全表扫描时效率很低。Hive在读取数据时，可以只读取查询中所需要的列，忽视其他的列，这样做可以节省读取开销（中间表存储开销和数据整合开销）</p> 
<ul><li>列裁剪：在查询时只读取需要的列。</li><li>分区裁剪：在查询中只读取需要的分区。</li></ul> 
<h5><a id="32_Group_By_966"></a>3.2 Group By</h5> 
<ul><li>默认情况下，Map阶段同一Key数据分给一个Reduce，当一个Key数据过大时就倾斜了。</li><li>并不是所有的聚合操作都需要再Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</li></ul> 
<p><img src="https://images2.imgbox.com/b3/29/wZWWjjAN_o.png" alt="在这里插入图片描述"></p> 
<p><strong>参数配置</strong></p> 
<ul><li>开启Map端聚合设置</li></ul> 
<pre><code class="prism language-hive">#	开启在Map端聚合
set hive.map.aggr = true
#	Map端聚合条目数目
set hive.groupby.mapaggr.checkinterval = 100000
#	开启数据倾斜时，进行负载均衡
set hive.groupby.skewindata = true
</code></pre> 
<ul><li> <p>当开启数据负载均衡时，生成的查询计划会有2个MRJob。</p> <p>第一个MRJob中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；</p> <p>第二个MRJob再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p> </li></ul> 
<h5><a id="33_CBO_992"></a>3.3 CBO优化</h5> 
<h4><a id="4_998"></a>4、数据倾斜</h4> 
<h5><a id="41__1000"></a>4.1 现象</h5> 
<p>数据倾斜：绝大多数任务很快完成，只有1个或者几个任务执行的很忙甚至最终执行失败。</p> 
<p>数据过量：所有的任务执行都很慢。这时候只有提高执行资源才能优化HQL的执行效率。</p> 
<p><strong>原因：</strong></p> 
<p>按照Key分组后，少量的任务负载着绝大部分的数据的计算，也就是说。产生数据倾斜的HQL中一定存在着分组的操作，所以从HQL的角度，我们可以将数据倾斜分为单表携带了Group By字段的查询和2表(多表)Join的查询。</p> 
<h5><a id="42__1010"></a>4.2 单表数据倾斜优化</h5> 
<p><strong>1）使用参数优化</strong></p> 
<p>当任务中存在Group By操作同时聚合函数为count或者sum。可以设置参数来处理数据倾斜的问题。</p> 
<pre><code class="prism language-shell"><span class="token comment">#	是否在Map端进行聚合，默认为True</span>
<span class="token builtin class-name">set</span> hive.map.aggr <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#	在Map端进行聚合操作的条目数目</span>
<span class="token builtin class-name">set</span> hive.groupby.mapaggr.checkinterval <span class="token operator">=</span> <span class="token number">100000</span>
<span class="token comment">#	有数据倾斜的时候进行负载均衡（默认是false）</span>
<span class="token builtin class-name">set</span> hive.groupby.skewindata <span class="token operator">=</span> <span class="token boolean">true</span>
</code></pre> 
<p><strong>2）增加Reduce数量</strong></p> 
<p>当数据中多个key同时导致数据倾斜，可以通过增加Reduce的数量解决数据倾斜问题。</p> 
<ul><li>方式一（动态调整）：</li></ul> 
<pre><code class="prism language-shell"><span class="token comment">#	每个Reduce处理的数据量默认为 256M（参数1）</span>
<span class="token builtin class-name">set</span> <span class="token assign-left variable">hive.exec.reducers.bytes.per.reducer</span><span class="token operator">=</span><span class="token number">256000000</span>
<span class="token comment">#	每个任务最大的Reduce数，默认为1009（参数2）</span>
<span class="token builtin class-name">set</span> <span class="token assign-left variable">hive.exec.reducers.max</span><span class="token operator">=</span><span class="token number">1009</span>
<span class="token comment">#	计算Reducer数的公式</span>
N <span class="token operator">=</span> min<span class="token punctuation">(</span>参数2 ，总数入数据量/参数1<span class="token punctuation">)</span>
</code></pre> 
<ul><li>方式二（直接指定）：</li></ul> 
<pre><code class="prism language-shell"><span class="token comment">#	直接指定Reduce个数</span>
<span class="token builtin class-name">set</span> mapreduce.job.reduces <span class="token operator">=</span> <span class="token number">15</span><span class="token punctuation">;</span>
</code></pre> 
<h5><a id="43_Join_1047"></a>4.3 Join数据倾斜优化</h5> 
<p><strong>1）使用参数</strong></p> 
<p>在编写Join查询语句时，如果确定是由于join出现的数据倾斜：</p> 
<pre><code class="prism language-shell"><span class="token comment"># join的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置</span>
<span class="token builtin class-name">set</span> <span class="token assign-left variable">hive.skewjoin.key</span><span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">;</span>
<span class="token comment"># 如果是join过程出现倾斜应该设置为true</span>
<span class="token builtin class-name">set</span> <span class="token assign-left variable">hive.optimize.skewjoin</span><span class="token operator">=</span>false<span class="token punctuation">;</span>
</code></pre> 
<p>如果开启了，在Join过程中Hive会将计数超过阈值hive.skewjoin.key（默认100000）的倾斜key对应的行临时写进文件中，然后再启动另一个job做map join生成结果。通过 hive.skewjoin.mapjoin.map.tasks参数还可以控制第二个job的mapper数量，默认10000。</p> 
<pre><code class="prism language-shell"><span class="token builtin class-name">set</span> <span class="token assign-left variable">hive.skewjoin.mapjoin.map.tasks</span><span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">;</span>
</code></pre> 
<p><strong>2）大小表join</strong></p> 
<p>可以使用MapJoin，没有Reduce阶段就不会出现数据倾斜。</p> 
<p><strong>3）大表大表Join</strong></p> 
<p>使用打散加扩容的方式解决数据倾斜问题，选择其中较大的表做打算操作</p> 
<pre><code class="prism language-hive">SELECT * ,concat(id,'_' , '0 or 1 or 2')FROM A; t1
</code></pre> 
<p>选择其中较小的表做扩容处理</p> 
<pre><code class="prism language-hive">SELECT *,concat(id,'-','0') from B
UNION ALL
SELECT *,concat(id,'-','1') from B
UNION ALL
SELECT *,concat(id,'-','2') from B;t2
</code></pre> 
<h4><a id="5Hive_Job_1098"></a>5、Hive Job优化</h4> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre> 
<p>1</p> 
<pre><code class="prism language-hive"></code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e18f2b5722d8ee81f838cfcdc53b9568/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux学习命令之source</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e297b8605ac596cf2b9722b0e7e43cdb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">dropout层加在哪里</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>