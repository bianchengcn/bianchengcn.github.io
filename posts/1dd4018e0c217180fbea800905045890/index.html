<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python实现简单的爬虫功能 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python实现简单的爬虫功能" />
<meta property="og:description" content="前言 Python是一种广泛应用于爬虫的高级编程语言，它提供了许多强大的库和框架，可以轻松地创建自己的爬虫程序。在本文中，我们将介绍如何使用Python实现简单的爬虫功能，并提供相关的代码实例。
如何实现简单的爬虫 1. 导入必要的库和模块 在编写Python爬虫时，我们需要使用许多库和模块，其中最重要的是requests和BeautifulSoup。Requests库可以帮助我们发送HTTP请求，并从网站上获取数据，而BeautifulSoup可以帮助我们从HTML文件中提取所需的信息。因此，我们需要首先导入这两个库。
import requests from bs4 import BeautifulSoup 2. 发送HTTP请求 在爬虫程序中，我们需要向网站发送HTTP请求，通常使用GET方法。Requests库提供了一个get()函数，我们可以使用它来获取网站的HTML文件。这个函数需要一个网站的URL作为参数，并返回一个包含HTML文件的响应对象。我们可以使用text属性来访问HTML文件的文本内容。
url = &#34;https://www.example.com&#34; response = requests.get(url) html = response.text 在发送HTTP请求时，我们需要注意是否需要添加用户代理和头信息。有些网站会检查用户代理和头信息，如果没有正确的值，它们就会拒绝我们的请求。为了避免这种情况，我们可以在HTTP请求中添加用户代理和头信息。我们可以使用requests库的headers选项来添加头信息。
headers = { &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3&#34;} response = requests.get(url, headers=headers) 3. 解析HTML文件 在获取了网站的HTML文件之后，我们需要从中提取我们想要的信息。为此，我们需要使用BeautifulSoup库，它提供了许多强大的函数和方法，可以轻松地解析HTML文件。
我们可以使用BeautifulSoup函数将HTML文件转换为BeautifulSoup对象。然后，我们可以使用find()、find_all()等方法来查找HTML文件中的元素。这些方法需要一个标签名称作为参数，并返回一个包含所选元素的列表或单个元素。
soup = BeautifulSoup(html, &#34;html.parser&#34;) title = soup.find(&#34;title&#34;).text 为了从HTML文件中提取更多的信息，我们需要了解CSS选择器。CSS选择器是一种用于选择HTML元素的语法，类似于CSS中的样式选择器。我们可以使用CSS选择器来获取HTML文件中特定元素的信息。例如，我们可以使用select()方法和一个CSS选择器来选择一个类别的所有元素。
items = soup.select(&#34;.item&#34;) for item in items: title = item.select(&#34;.title&#34;)[0].text price = item." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/1dd4018e0c217180fbea800905045890/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-08T14:03:38+08:00" />
<meta property="article:modified_time" content="2023-08-08T14:03:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python实现简单的爬虫功能</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>前言</h4> 
<p>Python是一种广泛应用于爬虫的高级编程语言，它提供了许多强大的库和框架，可以轻松地创建自己的爬虫程序。在本文中，我们将介绍如何使用Python实现简单的爬虫功能，并提供相关的代码实例。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e2/11/WoFMs4z9_o.png"></p> 
<h4>如何实现简单的爬虫</h4> 
<h5>1. 导入必要的库和模块</h5> 
<p>在编写Python爬虫时，我们需要使用许多库和模块，其中最重要的是requests和BeautifulSoup。Requests库可以帮助我们发送HTTP请求，并从网站上获取数据，而BeautifulSoup可以帮助我们从HTML文件中提取所需的信息。因此，我们需要首先导入这两个库。</p> 
<pre><code class="language-python">import requests
from bs4 import BeautifulSoup
</code></pre> 
<h5>2. 发送HTTP请求</h5> 
<p>在爬虫程序中，我们需要向网站发送HTTP请求，通常使用GET方法。Requests库提供了一个get()函数，我们可以使用它来获取网站的HTML文件。这个函数需要一个网站的URL作为参数，并返回一个包含HTML文件的响应对象。我们可以使用text属性来访问HTML文件的文本内容。</p> 
<pre><code class="language-python">url = "https://www.example.com"
response = requests.get(url)
html = response.text</code></pre> 
<p>在发送HTTP请求时，我们需要注意是否需要添加用户代理和头信息。有些网站会检查用户代理和头信息，如果没有正确的值，它们就会拒绝我们的请求。为了避免这种情况，我们可以在HTTP请求中添加用户代理和头信息。我们可以使用requests库的headers选项来添加头信息。</p> 
<pre><code class="language-python">headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}

response = requests.get(url, headers=headers)</code></pre> 
<h5>3. 解析HTML文件</h5> 
<p>在获取了网站的HTML文件之后，我们需要从中提取我们想要的信息。为此，我们需要使用BeautifulSoup库，它提供了许多强大的函数和方法，可以轻松地解析HTML文件。</p> 
<p>我们可以使用BeautifulSoup函数将HTML文件转换为BeautifulSoup对象。然后，我们可以使用find()、find_all()等方法来查找HTML文件中的元素。这些方法需要一个标签名称作为参数，并返回一个包含所选元素的列表或单个元素。</p> 
<pre><code class="language-python">soup = BeautifulSoup(html, "html.parser")
title = soup.find("title").text
</code></pre> 
<p>为了从HTML文件中提取更多的信息，我们需要了解CSS选择器。CSS选择器是一种用于选择HTML元素的语法，类似于CSS中的样式选择器。我们可以使用CSS选择器来获取HTML文件中特定元素的信息。例如，我们可以使用select()方法和一个CSS选择器来选择一个类别的所有元素。</p> 
<pre><code class="language-python">items = soup.select(".item")
for item in items:
    title = item.select(".title")[0].text
    price = item.select(".price")[0].text
</code></pre> 
<h5>4. 存储数据</h5> 
<p>在爬取数据后，我们可能需要将数据存储到本地文件或数据库中。Python提供了许多方式来实现这一点，例如使用CSV、JSON或SQLite等格式来存储数据。</p> 
<p>如果我们要将数据保存到CSV文件中，我们可以使用csv库。这个库提供了一个writer()函数，我们可以使用它来创建一个CSV写入器。然后，我们可以使用writerow()方法向CSV文件中写入数据。</p> 
<pre><code class="language-python">import csv

with open("data.csv", "w", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(["Title", "Price"])
    for item in items:
        title = item.select(".title")[0].text
        price = item.select(".price")[0].text
        writer.writerow([title, price])</code></pre> 
<p>如果我们要将数据保存到SQLite数据库中，我们可以使用sqlite3库。这个库提供了一个链接到数据库的函数connect()和一个游标对象，我们可以使用它来执行SQL查询。</p> 
<pre><code class="language-python">import sqlite3

conn = sqlite3.connect("data.db")
cursor = conn.cursor()
cursor.execute("CREATE TABLE items (title TEXT, price TEXT)")

for item in items:
    title = item.select(".title")[0].text
    price = item.select(".price")[0].text
    cursor.execute("INSERT INTO items VALUES (?, ?)", (title, price))

conn.commit()
conn.close()
</code></pre> 
<h4>完整的代码示例：</h4> 
<pre><code class="language-python">import requests
from bs4 import BeautifulSoup
import csv
import sqlite3


def get_data():
    url = "https://www.example.com"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}
    response = requests.get(url, headers=headers)
    html = response.text
    soup = BeautifulSoup(html, "html.parser")
    title = soup.find("title").text
    items = soup.select(".item")
    data = []
    for item in items:
        title = item.select(".title")[0].text
        price = item.select(".price")[0].text
        data.append((title, price))
    return title, data


def save_csv(title, data):
    with open("data.csv", "w", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(["Title", "Price"])
        for item in data:
            writer.writerow(item)


def save_sqlite(title, data):
    conn = sqlite3.connect("data.db")
    cursor = conn.cursor()
    cursor.execute("CREATE TABLE items (title TEXT, price TEXT)")
    for item in data:
        cursor.execute("INSERT INTO items VALUES (?, ?)", item)
    conn.commit()
    conn.close()


title, data = get_data()
save_csv(title, data)
save_sqlite(title, data)
</code></pre> 
<h4>总结</h4> 
<p>本文介绍了如何使用Python实现简单的爬虫功能，并提供了相关的代码示例。使用这些代码，您可以轻松地从网站上获取所需的数据，并将它们存储到本地文件或数据库中。在编写爬虫程序时，请务必尊重网站的使用规则，并避免过度频繁地发出HTTP请求，以避免对网站造成不必要的负担。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/11b7a36d498e9c3e7b6d82b03fdc2ca4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java中异步socket类的实现和源代码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e5536485ae9df629fe1a27f32dfd5243/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Netty自定义编码解码器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>