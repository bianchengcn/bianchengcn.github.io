<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>dropout层的添加，L2正则化 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="dropout层的添加，L2正则化" />
<meta property="og:description" content="dropout 一般添加在线性层，要给某一线性层添加dropout，dropout函数一般写在此线性层的前面。
注意：最后一个线性层前面一般不加dropout，一般线性层和激活函数搭配
import torch from torch import nn self.linears = nn.Sequential( nn.Linear(1, neural num), nn.ReLU(inplace=True), nn.Dropout(p=0.5), nn.Linear(neural_num, neural_num), nn.ReLU(inplace=True), #此三行为一组，Dropout写在对应线性层的前面 nn.Dropout(p=0.5), #有人说一般不加，但也可以加 nn.Linear(neural num, 1) #最后一个线性层一般不加Dropout ) L2正则化：
net_normal = Net(neural_num=n_hidden) net_weight_decay = Net(neural_num=n_hidden)
optimizer = torch.optim.SGD(net_normal.parameters(), lr=lr_init, momentum=0.9)
optimizer = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-2)
仅仅只是优化器中多加此参数代表使用了L2正则化。weight_decay=1e-2
同济自豪兄好像把dropout写在了此线性层的后面
原始的：
加上激活函数后的： 另一种加dropout：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/949ab995d4ca607bdbedd5d200388fa0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-26T15:44:01+08:00" />
<meta property="article:modified_time" content="2023-04-26T15:44:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">dropout层的添加，L2正则化</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>dropout 一般添加在线性层，要给某一线性层添加dropout，dropout函数一般写在此线性层的前面。</p> 
<p>注意：<strong>最后一个线性层前面一般不加dropout，一般线性层和激活函数搭配</strong></p> 
<pre><code class="language-python">import torch
from torch import nn
 
self.linears = nn.Sequential(
    nn.Linear(1, neural num), 
    nn.ReLU(inplace=True),

    nn.Dropout(p=0.5),
    nn.Linear(neural_num, neural_num), 
    nn.ReLU(inplace=True),
    #此三行为一组，Dropout写在对应线性层的前面


    nn.Dropout(p=0.5), #有人说一般不加，但也可以加
    nn.Linear(neural num, 1)
    #最后一个线性层一般不加Dropout    
        )
</code></pre> 
<hr> 
<p><strong>L2正则化：</strong></p> 
<p>net_normal = Net(neural_num=n_hidden)  <br> net_weight_decay = Net(neural_num=n_hidden)<br>  <br> optimizer = torch.optim.SGD(net_normal.parameters(), lr=lr_init, momentum=0.9)</p> 
<p>optimizer = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-2)</p> 
<p><strong>仅仅只是优化器中多加此参数代表使用了L2正则化。weight_decay=1e-2</strong></p> 
<hr> 
<p>同济自豪兄好像把dropout写在了此线性层的后面</p> 
<p>原始的：</p> 
<p><img alt="" height="525" src="https://images2.imgbox.com/4b/2f/adHLPjHK_o.png" width="581"></p> 
<p>加上激活函数后的： </p> 
<p> <img alt="" height="653" src="https://images2.imgbox.com/24/da/rLHjRE9U_o.png" width="555"></p> 
<p> 另一种加dropout：</p> 
<p><img alt="" height="779" src="https://images2.imgbox.com/00/14/DsDeJn3W_o.png" width="1060"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/90b8f45be5c4009b3fea5fa4bed4c55b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">concat合并的一个问题及解决办法pandas DataFrame</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/437ae014c1fdb0e861a9d1db9cc4470d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">软件测试技术（四）白盒测试</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>