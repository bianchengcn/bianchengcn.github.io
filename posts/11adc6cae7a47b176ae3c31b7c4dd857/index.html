<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>强化学习笔记：Q_learning （Q-table）示例举例 - 编程中国的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="强化学习笔记：Q_learning （Q-table）示例举例" />
<meta property="og:description" content="1 示例介绍 在右侧有宝藏，探险者到达宝藏所在的位置，就可以获得奖励
比如某一时刻的状态是这个样子的：“-o---T”
T 就是宝藏的位置, o 是探索者的位置
如果在某个地点 s1, 探索者计算了他能有的两个行为, Q(s1, a1) &gt; Q(s1, a2), 那么探索者就会选择 left 这个行为. 否则就是right
‘
参考内容：小例子 - 强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)
2 导入库&amp; 超参数设定 import numpy as np import pandas as pd import time np.random.seed(2) # 随机种子 N_STATES = 6 # 探宝者可能在的位置 ACTIONS = [&#39;left&#39;, &#39;right&#39;] # 探索者的可用动作 EPSILON = 0.9 # 贪婪度 greedy #0.9的概率选择最大的Q对应的action #0.1的概率随机选action ALPHA = 0.1 # 学习率 GAMMA = 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/11adc6cae7a47b176ae3c31b7c4dd857/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-06T11:38:55+08:00" />
<meta property="article:modified_time" content="2021-12-06T11:38:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程中国的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程中国的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">强化学习笔记：Q_learning （Q-table）示例举例</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1 示例介绍</h2> 
<p>在右侧有宝藏，探险者到达宝藏所在的位置，就可以获得奖励</p> 
<p>比如某一时刻的状态是这个样子的：“-o---T”</p> 
<p> T 就是宝藏的位置, o 是探索者的位置</p> 
<p></p> 
<p>如果在某个地点 <code>s1</code>, 探索者计算了他能有的两个行为, <code>Q(s1, a1) &gt; Q(s1, a2)</code>, 那么探索者就会选择 <code>left</code> 这个行为. 否则就是right</p> 
<p>‘</p> 
<p>参考内容：<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/general-rl/" rel="nofollow" title="小例子 - 强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)">小例子 - 强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p> 
<h2>2  导入库&amp; 超参数设定</h2> 
<pre><code class="language-python">import numpy as np
import pandas as pd
import time

np.random.seed(2)  
# 随机种子

N_STATES = 6   
# 探宝者可能在的位置

ACTIONS = ['left', 'right']     
# 探索者的可用动作

EPSILON = 0.9   
# 贪婪度 greedy 
#0.9的概率选择最大的Q对应的action
#0.1的概率随机选action

ALPHA = 0.1     
# 学习率

GAMMA = 0.9    
# 奖励递减值

MAX_EPISODES = 13   
# 最大回合数

FRESH_TIME = 0.3    
# 移动间隔时间</code></pre> 
<h2>3 创建Q-table</h2> 
<p>这是一个DataFrame</p> 
<p><a href="https://blog.csdn.net/qq_40206371/article/details/120389674" title="python 库整理：pandas_UQI-LIUWJ的博客-CSDN博客">python 库整理：pandas_UQI-LIUWJ的博客-CSDN博客</a></p> 
<p>index是目前探宝者的位置，columns是对应的动作<br>  </p> 
<pre><code class="language-python">def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     
        # q_table 初始状态全0
        columns=actions,    
        # columns 对应的是action名称
    )
    return table

#对于我们这个示例来说，出来的Q-table将会是
# q_table:
"""
   left  right
0   0.0    0.0
1   0.0    0.0
2   0.0    0.0
3   0.0    0.0
4   0.0    0.0
5   0.0    0.0
"""</code></pre> 
<h2>4 定义动作</h2> 
<p>定义探宝者如何挑选行为的</p> 
<p>这里我们使用ε-greedy</p> 
<p><a href="https://blog.csdn.net/qq_40206371/article/details/121012559?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163817482116780261972342%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=163817482116780261972342&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-2-121012559.pc_v2_rank_blog_default&amp;utm_term=greedy&amp;spm=1018.2226.3001.4450" title="强化学习笔记： generalized policy iteration with MC_UQI-LIUWJ的博客-CSDN博客">强化学习笔记： generalized policy iteration with MC_UQI-LIUWJ的博客-CSDN博客</a></p> 
<pre><code class="language-python"># 在某个 state 地点, 选择行为
def choose_action(state, q_table):
    state_actions = q_table.iloc[state, :]  
    # 选出这个 state 的所有 action 值（把这一行挑出来 pd.Series）
    if (np.random.uniform() &gt; EPSILON) or (state_actions.all() == 0): 
        # 非贪婪的10% ， 或者这个 state 还没有探索过
        action_name = np.random.choice(ACTIONS)
        #exploration 探索
    else:
        action_name = state_actions.idxmax()    
        # 贪婪模式的90% 
        # exploitation 利用
    return action_name</code></pre> 
<h2>5 设置状态转换和奖励reward</h2> 
<p>        做出行为后, 环境也要给我们的行为一个反馈, 反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward (R).</p> 
<p>        这里定义的规则就是, 只有 移动到了 <code>T</code>, 探宝者才会得到唯一的一个奖励, 奖励值 R=1, 其他情况都没有奖励.</p> 
<pre><code class="language-python">def get_env_feedback(S, A):
    # This is how agent will interact with the environment
    if A == 'right':    
        # move right
        if S == N_STATES - 2:   
            #现在在位置4，再往右1格是5，也就是宝藏所在的位置
            S_ = 'terminal'
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:   
        # move left
        R = 0
        if S == 0:
            S_ = S  
            # 在最右侧，碰壁了，所以不变状态
        else:
            S_ = S - 1
    return S_, R</code></pre> 
<h2>6 环境更新可视化</h2> 
<pre><code class="language-python">def update_env(S, episode, step_counter):
    env_list = ['-']*(N_STATES-1) + ['T']   
    # 没有探宝者的环境
    if S == 'terminal':
        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)
        print('\r{}'.format(interaction), end='')
        time.sleep(2)
        print('\r                                ', end='')
        #探宝者在终点
    else:
        env_list[S] = 'o'
        interaction = ''.join(env_list)
        print('\r{}'.format(interaction), end='')
        time.sleep(FRESH_TIME)
        #探宝者所在的位置替换为o
</code></pre> 
<h2>7 Q-learning</h2> 
<p>使用的是TD：</p> 
<p><img alt="" src="https://images2.imgbox.com/51/3c/54Zi3qlw_o.png"></p> 
<p> </p> 
<p style="text-align:center;"><img alt="2-1-1.png" src="https://images2.imgbox.com/a4/01/uyoKJ44U_o.png"></p> 
<p> </p> 
<pre><code class="language-python">def rl():
    q_table = build_q_table(N_STATES, ACTIONS)  
    # 创建初始 q table

    for episode in range(MAX_EPISODES):     
    #每一个回合
        step_counter = 0
        #从初始位置到达藏宝地需要多少步？我们设置一个计数器

        S = 0   
        # 回合初始位置

        is_terminated = False   
        # 是否回合结束

        update_env(S, episode, step_counter)    
        # 环境更新
        '''
        如果在终点，那么输出这一回合的一些信息
        如果非重点，那么输出当前探宝者所在的状态
        '''
        
        while not is_terminated:

            A = choose_action(S, q_table)   
            # 选行为（ε-greedy）

            S_, R = get_env_feedback(S, A)  
            # 实施行为并得到环境的反馈
            
            q_predict = q_table.loc[S, A]    # 估算的(状态-行为)值
            #Q(s,a)

            if S_ != 'terminal':
                q_target = R + GAMMA * q_table.iloc[S_, :].max()   
            #  实际的(状态-行为)值 (回合没结束)
            # q_table.iloc[S_, :].max()  ——&gt;  max Q(s',a') 
            else:
                q_target = R     
                #  实际的(状态-行为)值 (回合结束)
                #没有后续状态了，所以也不用加那一项
                #当然get_env_feedback 当遇到terminal 的时候，直接返回'terminal' ，也算不出它的max Q(s',a') 
                is_terminated = True    
                # 此时已经到达terminal了
            
            #if-else的作用就是计算q-target，也就是TD更新里面 减号之前的部分

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  
            #  q_table 更新（TD）
            S = S_  
            # 探索者移动到下一个状态

            update_env(S, episode, step_counter+1)  
            # 环境更新
            '''
            如果在终点，那么输出这一回合的一些信息
            如果非重点，那么输出当前探宝者所在的状态
            '''

            step_counter += 1

            #一致循环，直到拿到宝藏为止
          print(q_table)

        return q_table
</code></pre> 
<h2>8 训练</h2> 
<p>直接调用函数即可</p> 
<pre><code class="language-python">q_table = rl()
print(q_table)</code></pre> 
<p>在上面的实现中，命令行一次只会出现一行状态（这个是在update_env里面设置的('\r'+end='')）</p> 
<p><a href="https://blog.csdn.net/qq_40206371/article/details/121732959" title="​​​​​​python笔记 print+‘\r‘ （打印新内容时删除打印的旧内容）_UQI-LIUWJ的博客-CSDN博客">​​​​​​python笔记 print+‘\r‘ （打印新内容时删除打印的旧内容）_UQI-LIUWJ的博客-CSDN博客</a></p> 
<p></p> 
<p>如果不加这个限制，我们看一个episode：</p> 
<p><img alt="" height="490" src="https://images2.imgbox.com/32/a0/wese6XMi_o.png" width="267"></p> 
<p>然后我们综合考量下每个episode之后的Q-table</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>1</td><td><img alt="" height="148" src="https://images2.imgbox.com/07/96/oc0Srfvr_o.png" width="143"></td><td>38</td></tr><tr><td>2</td><td><img alt="" height="155" src="https://images2.imgbox.com/06/b1/Jbq8xt8w_o.png" width="139"></td><td>22</td></tr><tr><td>3</td><td><img alt="" height="158" src="https://images2.imgbox.com/44/bb/jE8MfKhs_o.png" width="169"></td><td>9</td></tr><tr><td>4</td><td><img alt="" height="160" src="https://images2.imgbox.com/a7/21/FYncV8b8_o.png" width="173"></td><td>5</td></tr><tr><td>5</td><td><img alt="" height="164" src="https://images2.imgbox.com/24/7f/uKM3gebD_o.png" width="215"></td><td>7</td></tr><tr><td>6</td><td><img alt="" height="158" src="https://images2.imgbox.com/71/72/HxfZl2FT_o.png" width="190"></td><td>5</td></tr><tr><td>7</td><td><img alt="" height="164" src="https://images2.imgbox.com/a4/22/6W8a9mdO_o.png" width="190"></td><td>5</td></tr><tr><td>8</td><td><img alt="" height="154" src="https://images2.imgbox.com/a3/90/mJdG4APo_o.png" width="196"></td><td>5</td></tr><tr><td>9</td><td><img alt="" height="162" src="https://images2.imgbox.com/b3/14/HGVMHCC9_o.png" width="208"></td><td>5</td></tr><tr><td>10</td><td><img alt="" height="159" src="https://images2.imgbox.com/91/a1/roYzFz8f_o.png" width="200"></td><td>5</td></tr></tbody></table> 
<p>可以发现 left  不会增长，但right的会一直增大 </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e9687a54fcc93e892bdc91fd9303cff9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">890行。设计最强最全学生成绩管理系统（C语言大作业）（文后附解析说明的博客）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6dad07683ddd7d530399f22029457bfd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">3种常见的渗透测试漏洞总结，快来收藏√</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程中国的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>